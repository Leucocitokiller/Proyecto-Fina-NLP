{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gN2xzVUq7vrI",
        "n4vykxSpiuW7",
        "ud0e5zbBiy_K",
        "sgmIsl-MrNIf",
        "hm-vdKNdup6v",
        "-V_8blME1WMF",
        "VWsyoCJM-2RP",
        "TUWA7XZS_sik",
        "UMhymHzYCwg7",
        "rFHzL5O5ZCMm",
        "SZHIHCJeaT-m",
        "3VSV1S7nbreo",
        "34SB2iIsbwcv",
        "YevtYLGld4D5",
        "El8ih8sT30jI",
        "vLnJpfTsQ3uf",
        "Y3jISJCHRpaQ",
        "XvHx1nbUxvpC",
        "Rnxw1uP75JPx",
        "MxuHGtiwk7OY",
        "jOi7smzXkgVv",
        "WXKbstDDl4Ei",
        "1t9nGg7NlxFp",
        "E0IoYi5ip5pu",
        "gW5hk2E6y-gd",
        "fe03QqRtziYW",
        "sngD9pOkzrlN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leucocitokiller/Proyecto-Fina-NLP/blob/main/Proyecto_final_MLIII_Libenson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto Final NLP y Redes Neuronales."
      ],
      "metadata": {
        "id": "k37euq467fIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducci√≥n"
      ],
      "metadata": {
        "id": "gN2xzVUq7vrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se realizar√° un analisis de sentimientos usando una lista de comentarios respecto a comidas de distintos restaurantes."
      ],
      "metadata": {
        "id": "J0oUsUt_jRgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuente de datos."
      ],
      "metadata": {
        "id": "9LOubDs570jQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importaci√≥n de librerias"
      ],
      "metadata": {
        "id": "n4vykxSpiuW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "#-----librerias para trabajar PLN\n",
        "!python -m spacy download es_core_news_md\n",
        "import spacy\n",
        "import es_core_news_md\n",
        "#es_core_news_md Medium (modelo mediano):\n",
        "#Es m√°s pesado y m√°s lento que el sm, pero mucho m√°s preciso. Tiene vectores de palabras, entiende mejor el significado de las palabras.\n",
        "\n",
        "#-----instalaci√≥n d librerias para an√°lisis de sentimientos.\n",
        "!pip install spacy spacy-transformers\n",
        "!pip install pysentimiento\n",
        "from pysentimiento import create_analyzer\n",
        "\n",
        "#----librerias para normalizaci√≥n de textos\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#----librerias para graficar y wordcloud.\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#----librer√≠as para trabajar con TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#----libreria para trabajar con BoW.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#----librerias para Machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n"
      ],
      "metadata": {
        "id": "WnH-JHG6YX7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura de los datos desde Github"
      ],
      "metadata": {
        "id": "ud0e5zbBiy_K"
      }
    },
    {
      "source": [
        "# Diccionario con las fuentes y sus URLs\n",
        "filepath_dict = {\n",
        "    'yelp': 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/main/yelp_comentarios.csv',\n",
        "    'amazon': 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/main/amazon_cells_comentarios.csv'\n",
        "\n",
        "}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['Comentario', 'Valor'], sep=';', encoding='latin-1')\n",
        "    df['Origen'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "df.head(1100)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IUoKBrJIfxo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizaci√≥n de la fuente de datos."
      ],
      "metadata": {
        "id": "6K0QGMcM8Y_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## En el proceso de Normalizaci√≥n de datos, se realizar√° lo siguiente:\n",
        "\n",
        "### 1. Eliminaci√≥n de signos de puntuaci√≥n.\n",
        "### 2. Reducir a min√∫sculas todo el texto.\n",
        "### 3. Convertir a n√∫mero la columna Valor para su postprocesamiento."
      ],
      "metadata": {
        "id": "bfNPZD-QrbuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Eliminaci√≥n de signos de puntuaci√≥n"
      ],
      "metadata": {
        "id": "sgmIsl-MrNIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definici√≥n de funci√≥n para eliminar los signos de puntuaci√≥n utilizando re, pero considerando no borrar las vocales con acento.\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Normaliza el texto a NFKD para separar letras y sus tildes\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    # Elimina los caracteres diacr√≠ticos (como las tildes)\n",
        "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
        "    # Elimina todo lo que no sea letras, n√∫meros o espacios\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Aplicar la funci√≥n a la columna 'review_lower'\n",
        "df['Comentarios'] = df['Comentario'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "IioeKtgqsMNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "M64iR-vGtejh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Se puede observar que la puntuaci√≥n se elimin√≥, y las palabras con acentos mantuvieron las vocales para mayor legibilidad."
      ],
      "metadata": {
        "id": "6yND37eCuaUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Reducir a min√∫sculas todo el texto."
      ],
      "metadata": {
        "id": "hm-vdKNdup6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'Comentarios_lower' with lowercase values from 'Comentario'\n",
        "df['Comentarios_lower'] = df['Comentarios'].str.lower()"
      ],
      "metadata": {
        "id": "8C_RLf5TuuJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "hkoPOq8K1M1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.  Convertir a n√∫mero la columna Valor para su postprocesamiento."
      ],
      "metadata": {
        "id": "-V_8blME1WMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos la columna rating a valor num√©rico\n",
        "df['Valor'] = pd.to_numeric(df['Valor'], errors='coerce')"
      ],
      "metadata": {
        "id": "pX8lMP2n1cX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Valor']"
      ],
      "metadata": {
        "id": "Uek7A0O31wX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento NLP"
      ],
      "metadata": {
        "id": "xCaWuodl3CY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generaci√≥n del objeto de SPacy para utilizar en el procesamiento del texto en espa√±ol."
      ],
      "metadata": {
        "id": "VWsyoCJM-2RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = es_core_news_md.load()"
      ],
      "metadata": {
        "id": "ZgYQ6Jru-_zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizaci√≥n."
      ],
      "metadata": {
        "id": "TUWA7XZS_sik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comentarios_tokenizados'] = df['Comentarios_lower'].apply(lambda text: nlp(text))"
      ],
      "metadata": {
        "id": "UWTXya9V_2WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Comentarios_tokenizados','Comentarios_lower']].head()"
      ],
      "metadata": {
        "id": "ovh-gLQ_BbEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remoci√≥n de Stop Words"
      ],
      "metadata": {
        "id": "UMhymHzYCwg7"
      }
    },
    {
      "source": [
        "def parse_and_remove_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Remueve las stopwords de un objeto spaCy Doc.\n",
        "    \"\"\"\n",
        "    # Filtrar stopwords y obtener los tokens como texto\n",
        "    tokens_filtrados = [token.text for token in doc if not token.is_stop]\n",
        "    return tokens_filtrados\n",
        "\n",
        "# Aplicar la funci√≥n al DataFrame\n",
        "df['Comentarios_sin_StopWords'] = df['Comentarios_tokenizados'].apply(parse_and_remove_stopwords)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NK7Z0iQOXvt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Comentarios_tokenizados','Comentarios_sin_StopWords']].head()"
      ],
      "metadata": {
        "id": "Ktbu3Xa3Xzbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lematizado"
      ],
      "metadata": {
        "id": "rFHzL5O5ZCMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizar_sin_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Devuelve una lista de lemas excluyendo las stopwords.\n",
        "\n",
        "    Par√°metro:\n",
        "    - doc: objeto spaCy Doc\n",
        "\n",
        "    Retorna:\n",
        "    - Lista de lemas (str) sin stopwords\n",
        "    \"\"\"\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "# Aplicar la funci√≥n y guardar el resultado en una nueva columna\n",
        "df['Comentarios_lema'] = df['Comentarios_tokenizados'].apply(lematizar_sin_stopwords)"
      ],
      "metadata": {
        "id": "s-a15Lu_ZDRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Comentarios_tokenizados','Comentarios_lema']].head(100)"
      ],
      "metadata": {
        "id": "qAbnf0-bZWAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78Nr-pJ54THw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordCloud (Nube de Palabras)\n"
      ],
      "metadata": {
        "id": "SZHIHCJeaT-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordCloud de Yelp"
      ],
      "metadata": {
        "id": "3VSV1S7nbreo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar el DataFrame\n",
        "df_yelp = df[df['Origen'] == 'yelp']\n",
        "\n",
        "# Unir todos los lemas en un solo string (comentarios lematizados ya est√°n en listas)\n",
        "texto_yelp = ' '.join([' '.join(lemas) for lemas in df_yelp['Comentarios_lema']])\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_yelp)\n",
        "\n",
        "# Mostrar la nube\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios de Yelp\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kX8lCYg_aEyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word cloud de Amazon"
      ],
      "metadata": {
        "id": "34SB2iIsbwcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar el DataFrame\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Unir todos los lemas en un solo string (comentarios lematizados ya est√°n en listas)\n",
        "texto_amazon = ' '.join([' '.join(lemas) for lemas in df_amazon['Comentarios_lema']])\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_amazon)\n",
        "\n",
        "# Mostrar la nube\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios de Yelp\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4JQKIXwdbyzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conteo de Palabras mas comunes"
      ],
      "metadata": {
        "id": "YevtYLGld4D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graficar_palabras_comunes(df, origen, top_n=10):\n",
        "    # Filtrar y aplanar los lemas\n",
        "    lemas = [lema for lemas in df[df['Origen'] == origen]['Comentarios_lema'] for lema in lemas]\n",
        "    conteo = Counter(lemas).most_common(top_n)\n",
        "\n",
        "    # Separar palabras y frecuencias\n",
        "    palabras, frecuencias = zip(*conteo)\n",
        "\n",
        "    # Crear gr√°fico\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(palabras, frecuencias, color='skyblue')\n",
        "    plt.xlabel('Frecuencia')\n",
        "    plt.title(f'Top {top_n} Palabras M√°s Comunes - {origen.capitalize()}')\n",
        "    plt.gca().invert_yaxis()  # Poner la palabra m√°s com√∫n arriba\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Graficar para Yelp\n",
        "graficar_palabras_comunes(df, 'yelp')\n",
        "\n",
        "# Graficar para Amazon\n",
        "graficar_palabras_comunes(df, 'amazon')\n"
      ],
      "metadata": {
        "id": "4Vt-Btc6d7lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigramas + WordCloud"
      ],
      "metadata": {
        "id": "CvpcHB4Vh_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_bigramas_spacy(df, origen, top_n=50):\n",
        "    \"\"\"\n",
        "    Genera bigramas usando spaCy a partir de la columna 'Comentarios_Lema', sin stopwords.\n",
        "    Luego genera una nube de palabras.\n",
        "    \"\"\"\n",
        "    # Filtrar los comentarios por 'origen' (por ejemplo, 'yelp' o 'amazon')\n",
        "    comentarios = df[df['Origen'] == origen]['Comentarios_lema']\n",
        "\n",
        "    # Generar bigramas\n",
        "    bigramas = []\n",
        "    for comentario in comentarios:\n",
        "        # Crear un Doc de spaCy a partir de la lista de lemas (de la columna 'Comentarios_Lema')\n",
        "        doc = nlp(' '.join(comentario))  # Unimos la lista de lemas y lo procesamos con spaCy\n",
        "        # Extraer bigramas\n",
        "        for i in range(len(doc) - 1):\n",
        "            if not doc[i].is_stop and not doc[i+1].is_stop:  # Asegurarse de que no sean stopwords\n",
        "                bigramas.append((doc[i].lemma_, doc[i+1].lemma_))\n",
        "\n",
        "    # Contar los bigramas m√°s comunes\n",
        "    conteo_bigramas = Counter(bigramas).most_common(top_n)\n",
        "\n",
        "    # Convertir los bigramas a formato texto \"palabra1 palabra2\"\n",
        "    bigramas_texto = {' '.join(bigrama): freq for bigrama, freq in conteo_bigramas}\n",
        "\n",
        "    # Generar la nube de palabras de los bigramas\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigramas_texto)\n",
        "\n",
        "    # Mostrar la nube\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud de Bigramas - {origen.capitalize()}\")\n",
        "    plt.show()\n",
        "\n",
        "# Generar la nube de bigramas para Yelp y Amazon\n",
        "generar_bigramas_spacy(df, 'yelp')\n",
        "generar_bigramas_spacy(df, 'amazon')\n"
      ],
      "metadata": {
        "id": "DruD_yJRgtMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ An√°lisis de sentimiento en espa√±ol con pysentimiento"
      ],
      "metadata": {
        "id": "GO6Gth5wGeZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Procesamiento del sentimiento con los comentarios sin procesar."
      ],
      "metadata": {
        "id": "yWdIRcQWLunE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pysentimiento import create_analyzer\n",
        "\n",
        "# Crear analizador de sentimientos\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "# Aplicar a una columna de texto\n",
        "df['Sentimiento'] = df['Comentarios'].apply(lambda x: analyzer.predict(x).output)\n",
        "# Sentimiento solo guarda lo predicho (POS, NEU o NEG)\n",
        "\n",
        "df['Probabilidad'] = df['Comentarios'].apply(lambda x: analyzer.predict(x).probas)\n",
        "#Ese diccionario contiene la probabilidad de cada clase: positivo, neutro, negativo Ejemplo: {'POS': 0.84, 'NEU': 0.10, 'NEG': 0.06}.\n",
        "\n"
      ],
      "metadata": {
        "id": "59rspiPIEWFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä  Gr√°fico de barras de frecuencia de sentimientos\n"
      ],
      "metadata": {
        "id": "hPjwZg3VQej1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An√°lisis de sentimientos con todos los datos de yelp y Amazon juntos."
      ],
      "metadata": {
        "id": "El8ih8sT30jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribuci√≥n de Sentimientos')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zN86u1J4E2xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentimiento'].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    labels=['Positivo', 'Neutro', 'Negativo'],\n",
        "    colors=['lightgreen', 'lightblue', 'salmon']\n",
        ")\n",
        "plt.title('Distribuci√≥n porcentual de Sentimientos')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JclTvB0JQWqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribuci√≥n de sentiminetos de los comentarios de Yelp"
      ],
      "metadata": {
        "id": "vLnJpfTsQ3uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar por origen \"yelp\"\n",
        "df_yelp = df[df['Origen'] == 'yelp']\n",
        "\n",
        "# Graficar los sentimientos de Yelp\n",
        "sns.countplot(data=df_yelp, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribuci√≥n de Sentimientos - Yelp')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8OmdpqN8Q0WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribuci√≥n de sentiminetos de los comentarios de Amazon"
      ],
      "metadata": {
        "id": "Y3jISJCHRpaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar por origen \"amazon\"\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Graficar los sentimientos de Yelp\n",
        "sns.countplot(data=df_amazon, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribuci√≥n de Sentimientos - Yelp')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Usa2F9ZwRtjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribuci√≥n de sentimientos pero con los datos lematizados."
      ],
      "metadata": {
        "id": "XvHx1nbUxvpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "resultados = [analyzer.predict(texto) for texto in df['Comentarios_lema'].apply(' '.join)]\n",
        "\n",
        "# Aplicar a una columna de texto\n",
        "df['Sentimiento_lema'] = [resultado.output for resultado in resultados]\n",
        "df['Probabilidad_lema'] = [resultado.probas for resultado in resultados]"
      ],
      "metadata": {
        "id": "TiO-sKHrS6sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar por origen \"yelp\"\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Graficar los sentimientos de Yelp\n",
        "sns.countplot(data=df_amazon, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribuci√≥n de Sentimientos - Yelp')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "coAP0uOSx3V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No se observan cambios respecto al an√°lisis realizado con las frases sin lematizar."
      ],
      "metadata": {
        "id": "FOePV31v3pYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An√°lisis de Confianza para filtrar comentarios con baja certeza"
      ],
      "metadata": {
        "id": "Rnxw1uP75JPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# M√°xima probabilidad (nivel de certeza del modelo)\n",
        "df['Confianza'] = df['Probabilidad'].apply(lambda x: max(x.values()))  # En este caso, de la lista {'POS': 0.84, 'NEU': 0.10, 'NEG': 0.06} s√≥lo guarda 0.84 que es el valor mayor"
      ],
      "metadata": {
        "id": "FTUYtdov50XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar comentarios cuya confianza sea menor a 0.6\n",
        "comentarios_baja_confianza = df[df['Confianza'] < 0.6]\n",
        "\n",
        "# Ver los primeros resultados\n",
        "comentarios_baja_confianza[['Comentarios', 'Sentimiento', 'Confianza']]"
      ],
      "metadata": {
        "id": "25tVeE8N7z-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x='Confianza', bins=20, kde=True, color='skyblue')\n",
        "plt.axvline(0.6, color='red', linestyle='--', label='Umbral 0.6')\n",
        "plt.title('Distribuci√≥n de Confianza del Sentimiento')\n",
        "plt.xlabel('Confianza')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nLb5WfkF6AUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(data=df, x='Sentimiento', y='Confianza', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribuci√≥n de Confianza por Sentimiento')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Confianza')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bYBos64W_GWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x='Confianza', hue='Origen', bins=20, kde=True, palette='Set2')\n",
        "plt.axvline(0.6, color='red', linestyle='--', label='Umbral 0.6')\n",
        "plt.title('Distribuci√≥n de Confianza por Origen')\n",
        "plt.xlabel('Confianza')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "93c3qmNh_V_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_baja_confianza = df[df['Confianza'] < 0.6]\n"
      ],
      "metadata": {
        "id": "Hu1Xi5-6BRMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Unir todos los lemas en un solo string (asumiendo que cada fila de 'Comentarios_lema' es una lista de palabras)\n",
        "texto_baja_confianza = ' '.join([' '.join(lemas) for lemas in df_baja_confianza['Comentarios_lema']])"
      ],
      "metadata": {
        "id": "LsaUcHuPBwfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='cool').generate(texto_baja_confianza)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios con Baja Confianza\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FDwXL5ztB3uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF_IDF"
      ],
      "metadata": {
        "id": "MmX_Ui9GjS38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåü Explicaci√≥n del TF-IDF"
      ],
      "metadata": {
        "id": "MxuHGtiwk7OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency - Inverse Document Frequency) es una t√©cnica de procesamiento de texto utilizada para evaluar la importancia de una palabra dentro de un conjunto de documentos. Se basa en dos conceptos:\n",
        "\n",
        "TF (Frecuencia de T√©rmino): Mide cu√°ntas veces aparece un t√©rmino en un documento espec√≠fico, comparado con el n√∫mero total de t√©rminos en ese documento.\n",
        "Esto ayuda a capturar cu√°n relevante es una palabra dentro de un documento en particular.\n",
        "\n",
        "IDF (Frecuencia Inversa de Documentos): Mide la importancia de una palabra dentro de un conjunto de documentos. Si una palabra aparece en muchos documentos, tiene menos valor. La f√≥rmula es:\n",
        "\n",
        "Esto ayuda a reducir el peso de las palabras que aparecen frecuentemente en todos los documentos (como \"el\", \"y\", \"de\"), ya que no agregan mucha informaci√≥n.\n",
        "\n",
        "As√≠, la importancia de un t√©rmino en un documento depende tanto de su frecuencia en ese documento como de cu√°n com√∫n es en todo el conjunto de documentos."
      ],
      "metadata": {
        "id": "__ox6x-IlBef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. TFIDF espera trabajar con strings y  no listas, por lo que se procesde a crear una nueva columna con los datos lematizados en formato str."
      ],
      "metadata": {
        "id": "yaI-9TITjbgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comentarios_lema_str'] = df['Comentarios_lema'].apply(lambda x: ' '.join(x))\n"
      ],
      "metadata": {
        "id": "TR3M-4mmjpg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. C√°lculo de TF-IDF con TfidVetorizer."
      ],
      "metadata": {
        "id": "bzD19US8jw10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el vectorizador\n",
        "tfidfvectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Ajustar y transformar\n",
        "tfidf_matrix = tfidfvectorizer.fit_transform(df['Comentarios_lema_str'])\n",
        "\n",
        "# Obtener los t√©rminos\n",
        "features = tfidfvectorizer.get_feature_names_out()\n",
        "\n"
      ],
      "metadata": {
        "id": "UyySUzAlj2A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Convertir a dataframe para posterior an√°lisis."
      ],
      "metadata": {
        "id": "jOi7smzXkgVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear DataFrame TF-IDF\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=features)\n",
        "\n",
        "# Sumamos por columna para ver los t√©rminos m√°s importantes globalmente\n",
        "tfidf_scores = df_tfidf.sum().sort_values(ascending=False)\n",
        "\n",
        "# Mostrar top 10\n",
        "print(tfidf_scores.head(10).round(3))\n",
        "\n",
        "# Add this code block after the first TF-IDF transformation and before the split\n",
        "print(f\"Number of features in tfidf_matrix after fitting: {tfidf_matrix.shape[1]}\")\n",
        "print(f\"Number of features in df_tfidf: {df_tfidf.shape[1]}\")"
      ],
      "metadata": {
        "id": "Qy0sAmK6km33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Aplicando TF-IDF y Gr√°ficos"
      ],
      "metadata": {
        "id": "4QnMIDTMlgAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gr√°fico de todos los comentarios (Yelp y Amanzon)"
      ],
      "metadata": {
        "id": "WXKbstDDl4Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tomar los 10 t√©rminos con mayor puntuaci√≥n TF-IDF\n",
        "top_tfidf = tfidf_scores.head(10)\n",
        "\n",
        "# Crear gr√°fico de barras\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_tfidf.plot(kind='barh', color='skyblue')\n",
        "plt.title(\"Top 10 T√©rminos con Mayor TF-IDF\")\n",
        "plt.xlabel(\"Puntaje TF-IDF\")\n",
        "plt.ylabel(\"T√©rmino\")\n",
        "plt.gca().invert_yaxis()  # Para que el mayor quede arriba\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XB-PFMLXktOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gr√°fico para comentarios de YELP"
      ],
      "metadata": {
        "id": "1t9nGg7NlxFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar comentarios de Yelp\n",
        "df_yelp = df[df['Origen'] == 'yelp']\n",
        "\n",
        "# Calcular TF-IDF solo para los comentarios de Yelp\n",
        "tfidf_yelp_matrix = tfidfvectorizer.transform(df_yelp['Comentarios_lema_str'])\n",
        "tfidf_yelp_scores = tfidf_yelp_matrix.sum(axis=0).A1  # Sumar por columna\n",
        "yelp_features = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_yelp_tfidf = pd.DataFrame(tfidf_yelp_scores, index=yelp_features, columns=['TF-IDF'])\n",
        "df_yelp_tfidf = df_yelp_tfidf.sort_values(by='TF-IDF', ascending=False)\n",
        "\n",
        "# Graficar los top 10 t√©rminos de Yelp\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_yelp_tfidf.head(10).plot(kind='barh', color='lightgreen')\n",
        "plt.title(\"Top 10 T√©rminos con Mayor TF-IDF - Yelp\")\n",
        "plt.xlabel(\"Puntaje TF-IDF\")\n",
        "plt.ylabel(\"T√©rmino\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MVx_NaJYl06b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gr√°fico para comentarios de Amazon"
      ],
      "metadata": {
        "id": "zqdrlUllmNaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar comentarios de Amazon\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Calcular TF-IDF solo para los comentarios de Amazon\n",
        "# Change fit_transform to transform here\n",
        "tfidf_amazon_matrix = tfidfvectorizer.transform(df_amazon['Comentarios_lema_str'])\n",
        "tfidf_amazon_scores = tfidf_amazon_matrix.sum(axis=0).A1  # Sumar por columna\n",
        "# Use the features learned from the full dataset\n",
        "amazon_features = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_amazon_tfidf = pd.DataFrame(tfidf_amazon_scores, index=amazon_features, columns=['TF-IDF'])\n",
        "df_amazon_tfidf = df_amazon_tfidf.sort_values(by='TF-IDF', ascending=False)\n",
        "\n",
        "# Graficar los top 10 t√©rminos de Amazon\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_amazon_tfidf.head(10).plot(kind='barh', color='yellow')\n",
        "plt.title(\"Top 10 T√©rminos con Mayor TF-IDF - amazon\")\n",
        "plt.xlabel(\"Puntaje TF-IDF\")\n",
        "plt.ylabel(\"T√©rmino\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Ensure the rest of your code, including the cell with the error, remains the same\n",
        "# as the `tfidfvectorizer` object will now retain the vocabulary from the full dataset."
      ],
      "metadata": {
        "id": "M09ICYRomOlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BoW (Bag of Words)."
      ],
      "metadata": {
        "id": "E0IoYi5ip5pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW convierte un conjunto de documentos en una matriz de ocurrencias de palabras. A diferencia de TF-IDF, que pondera las palabras seg√∫n su frecuencia e importancia en relaci√≥n con todo el corpus, BoW solo cuenta cu√°ntas veces aparece una palabra en un documento sin considerar la frecuencia global de la palabra."
      ],
      "metadata": {
        "id": "CYjaC_KyqTck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciamos el vectorizador BoW\n",
        "vectorizer_bow = CountVectorizer()\n",
        "\n",
        "# Aplicamos el vectorizador a los comentarios lematizados\n",
        "X_bow = vectorizer_bow.fit_transform(df['Comentarios_lema_str'])\n",
        "\n",
        "# Convertimos la matriz de caracter√≠sticas en un DataFrame para visualizar\n",
        "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
        "\n",
        "# Obtener los nombres de las caracter√≠sticas (palabras)\n",
        "features_bow = vectorizer_bow.get_feature_names_out()\n",
        "\n",
        "# Crear un DataFrame con las frecuencias de las palabras\n",
        "df_bow = pd.DataFrame(X_bow.toarray(), columns=features_bow)\n",
        "\n",
        "# Sumar las frecuencias de palabras por columna\n",
        "bow_word_frequencies = df_bow.sum().sort_values(ascending=False)\n",
        "\n",
        "# Mostrar las top 10 palabras m√°s frecuentes\n",
        "print(bow_word_frequencies.head(10))"
      ],
      "metadata": {
        "id": "SbBUwjR8p9rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gr√°fico de las 10 palabras m√°s frecuentes en BoW\n",
        "top_words = bow_word_frequencies.head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_words.index, y=top_words.values, palette='viridis')\n",
        "plt.title('Top 10 palabras m√°s frecuentes (BoW)')\n",
        "plt.xlabel('Palabras')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aK4Up7kLsCgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la nube de palabras con BoW\n",
        "wordcloud_bow = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bow_word_frequencies)\n",
        "\n",
        "# Mostrar la nube de palabras\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud_bow, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de palabras (BoW)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XogoJQjdsG5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruebas de Modelos de Machine Learning"
      ],
      "metadata": {
        "id": "s755wLyIyO4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separaci√≥n Train-Test"
      ],
      "metadata": {
        "id": "gW5hk2E6y-gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino las caracter√≠sticas (X) y las etiquetas (y)\n",
        "X = df_tfidf  # Las caracter√≠sticas son los valores del TF-IDF\n",
        "y = df['Valor']  # La variable objetivo es 'Valor' (0 o 1)\n",
        "\n",
        "# Paso 2: Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "WhtOCFZsyR4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi√≥n Logistica"
      ],
      "metadata": {
        "id": "BUm2hZ5g375q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento del modelo con TI-FID."
      ],
      "metadata": {
        "id": "fe03QqRtziYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear y entrenar el modelo de regresi√≥n log√≠stica\n",
        "logreg_model = LogisticRegression(max_iter=1000)\n",
        "logreg_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2Wf41pX0zi9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Realizar predicci√≥n con Test y evaluar el Accuarcy"
      ],
      "metadata": {
        "id": "sngD9pOkzrlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacer predicciones\n",
        "y_pred_logreg = logreg_model.predict(X_test)\n",
        "\n",
        "# Evaluar precisi√≥n\n",
        "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
        "print(f'Precisi√≥n de la regresi√≥n log√≠stica: {accuracy_logreg:.3f}')"
      ],
      "metadata": {
        "id": "SX2z04VNzxbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluacion del modelo."
      ],
      "metadata": {
        "id": "Vt98TGLTz8jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score,\n",
        "    accuracy_score, precision_score, recall_score, f1_score\n",
        ")\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test, y_pred_logreg)\n",
        "labels = ['Negativo', 'Positivo']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicci√≥n')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.show()\n",
        "\n",
        "# 2. Curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, logreg_model.decision_function(X_test))\n",
        "roc_auc = roc_auc_score(y_test, logreg_model.decision_function(X_test))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'√Årea bajo la curva = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# 3. M√©tricas\n",
        "accuracy = accuracy_score(y_test, y_pred_logreg)\n",
        "precision = precision_score(y_test, y_pred_logreg)\n",
        "recall = recall_score(y_test, y_pred_logreg)\n",
        "f1 = f1_score(y_test, y_pred_logreg)\n",
        "\n",
        "print(\"M√©tricas de desempe√±o del modelo:\")\n",
        "print(f\"Accuracy : {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall   : {recall:.2f}\")\n",
        "print(f\"F1 Score : {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "VYXEP3Ck1hqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä M√©tricas basadas en la matriz:\n",
        "\n",
        "True Positives (TP) = 136 ‚Üí positivos bien predichos\n",
        "\n",
        "True Negatives (TN) = 158 ‚Üí negativos bien predichos\n",
        "\n",
        "False Positives (FP) = 61 ‚Üí negativos clasificados como positivos\n",
        "\n",
        "False Negatives (FN) = 45 ‚Üí positivos clasificados como negativos\n",
        "\n",
        "üìä M√©tricas basadas en el desempe√±o:\n",
        "\n",
        "üîπ Accuracy (0.73): el modelo predice correctamente el 73% de los casos totales.\n",
        "\n",
        "üîπ Precision (0.69): de todos los comentarios que el modelo predijo como positivos, el 69% realmente lo eran. Podr√≠a estar dando algunos falsos positivos.\n",
        "\n",
        "üîπ Recall (0.75): de todos los comentarios positivos reales, el modelo logr√≥ detectar el 75%. Esto est√° bastante bien.\n",
        "\n",
        "üîπ F1 Score (0.72): el equilibrio entre precisi√≥n y recall es bueno."
      ],
      "metadata": {
        "id": "1VyOSFCN3nmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba de modelo"
      ],
      "metadata": {
        "id": "kGG3NVXJ_wb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nueva_rese√±a = \"Estoy disconforme con la atenci√≥n\"  # Reemplaza con la rese√±a que deseas probar\n",
        "nueva_rese√±a_tfidf = tfidfvectorizer.transform([nueva_rese√±a])\n",
        "prediccion = logreg_model.predict(nueva_rese√±a_tfidf)\n",
        "# Obtener la probabilidad de la predicci√≥n\n",
        "probabilidadpositiva = logreg_model.predict_proba(nueva_rese√±a_tfidf)\n",
        "\n",
        "# Obtener la probabilidad en la clase predicha (0 o 1)\n",
        "probabilidad = probabilidadpositiva[0][1]  # Probabilidad de la clase \"positivo\"\n",
        "\n",
        "# After tfidfvectorizer.fit_transform(df['Comentarios_lema_str']):\n",
        "print(f\"Shape of tfidf_matrix after fit_transform: {tfidf_matrix.shape}\")\n",
        "\n",
        "# After X = df_tfidf:\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "\n",
        "# After train_test_split:\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "\n",
        "# Before prediction:\n",
        "print(f\"Shape of nueva_rese√±a_tfidf: {nueva_rese√±a_tfidf.shape}\")"
      ],
      "metadata": {
        "id": "EmJqPQkc-p8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buscando el mejor modelo"
      ],
      "metadata": {
        "id": "YwZfAgSQ6UOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validaci√≥n Cruzada del modelo."
      ],
      "metadata": {
        "id": "BIiheckODjwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Pipeline: vectorizaci√≥n + modelo\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Definir el espacio de b√∫squeda\n",
        "param_grid = {\n",
        "    'tfidf__max_df': [0.8, 1.0],\n",
        "    'tfidf__min_df': [1, 5],\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'clf__C': [0.1, 1, 10]  # regularizaci√≥n de la regresi√≥n log√≠stica\n",
        "}\n",
        "\n",
        "# GridSearch con validaci√≥n cruzada\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Entrenar\n",
        "grid.fit(df['Comentarios_lema_str'], df['Valor'])\n",
        "\n",
        "# Mejor modelo\n",
        "print(\"Mejores par√°metros encontrados:\")\n",
        "print(grid.best_params_)\n",
        "print(f\"Mejor F1 score: {grid.best_score_:.3f}\")\n",
        "\n",
        "modelo_best = grid.best_params_\n",
        "\n"
      ],
      "metadata": {
        "id": "l25bWzq-2XuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisi√≥n media (F1 = 0.742): el modelo logra un buen equilibrio entre precisi√≥n y recall.\n",
        "\n",
        "Desviaci√≥n est√°ndar baja (¬±0.026): hay poca variaci√≥n entre los diferentes subconjuntos de datos usados durante la validaci√≥n cruzada, lo que sugiere que el modelo generaliza bien."
      ],
      "metadata": {
        "id": "ti2EA2jLDa0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "# Validaci√≥n cruzada con 5 particiones (k-fold = 5)\n",
        "scores = cross_val_score(pipeline, df['Comentarios_lema_str'], df['Valor'], cv=5, scoring='accuracy')\n",
        "\n",
        "# Resultados\n",
        "print(f\"Precisi√≥n media con validaci√≥n cruzada: {scores.mean():.3f}\")\n",
        "print(f\"Desviaci√≥n est√°ndar: {scores.std():.3f}\")\n"
      ],
      "metadata": {
        "id": "g-ZZmtnQC_gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba del modelo Regresion Log√≠stica usando BoW"
      ],
      "metadata": {
        "id": "MTi4rPCaqGTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
        "\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar modelo\n",
        "model_bow = LogisticRegression(solver='liblinear')\n",
        "model_bow.fit(X_train_bow, y_train_bow)\n",
        "\n",
        "# Predecir\n",
        "y_pred_bow = model_bow.predict(X_test_bow)\n",
        "y_prob_bow = model_bow.decision_function(X_test_bow)\n",
        "\n",
        "# M√©tricas\n",
        "print(\"Reporte de Clasificaci√≥n:\\n\")\n",
        "print(classification_report(y_test_bow, y_pred_bow, digits=3))\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test_bow, y_pred_bow)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negativo\", \"Positivo\"], yticklabels=[\"Negativo\", \"Positivo\"])\n",
        "plt.title(\"Matriz de Confusi√≥n\")\n",
        "plt.xlabel(\"Predicci√≥n\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()\n",
        "\n",
        "# Curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test_bow, y_prob_bow)\n",
        "roc_auc = roc_auc_score(y_test_bow, y_prob_bow)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VOvXOmo5qKDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Redes Neuronales"
      ],
      "metadata": {
        "id": "CsI99F6eci-J"
      }
    }
  ]
}