{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gN2xzVUq7vrI",
        "n4vykxSpiuW7",
        "ud0e5zbBiy_K",
        "sgmIsl-MrNIf",
        "hm-vdKNdup6v",
        "-V_8blME1WMF",
        "VWsyoCJM-2RP",
        "TUWA7XZS_sik",
        "UMhymHzYCwg7",
        "rFHzL5O5ZCMm",
        "SZHIHCJeaT-m",
        "3VSV1S7nbreo",
        "34SB2iIsbwcv",
        "YevtYLGld4D5",
        "El8ih8sT30jI",
        "vLnJpfTsQ3uf",
        "Y3jISJCHRpaQ",
        "XvHx1nbUxvpC",
        "Rnxw1uP75JPx",
        "MxuHGtiwk7OY",
        "jOi7smzXkgVv",
        "WXKbstDDl4Ei",
        "1t9nGg7NlxFp",
        "E0IoYi5ip5pu",
        "gW5hk2E6y-gd",
        "fe03QqRtziYW",
        "sngD9pOkzrlN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leucocitokiller/Proyecto-Fina-NLP/blob/main/Proyecto_final_MLIII_Libenson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto Final NLP y Redes Neuronales."
      ],
      "metadata": {
        "id": "k37euq467fIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción"
      ],
      "metadata": {
        "id": "gN2xzVUq7vrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se realizará un analisis de sentimientos usando una lista de comentarios respecto a comidas de distintos restaurantes."
      ],
      "metadata": {
        "id": "J0oUsUt_jRgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuente de datos."
      ],
      "metadata": {
        "id": "9LOubDs570jQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de librerias"
      ],
      "metadata": {
        "id": "n4vykxSpiuW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "#-----librerias para trabajar PLN\n",
        "!python -m spacy download es_core_news_md\n",
        "import spacy\n",
        "import es_core_news_md\n",
        "#es_core_news_md Medium (modelo mediano):\n",
        "#Es más pesado y más lento que el sm, pero mucho más preciso. Tiene vectores de palabras, entiende mejor el significado de las palabras.\n",
        "\n",
        "#-----instalación d librerias para análisis de sentimientos.\n",
        "!pip install spacy spacy-transformers\n",
        "!pip install pysentimiento\n",
        "from pysentimiento import create_analyzer\n",
        "\n",
        "#----librerias para normalización de textos\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#----librerias para graficar y wordcloud.\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#----librerías para trabajar con TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#----libreria para trabajar con BoW.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#----librerias para Machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n"
      ],
      "metadata": {
        "id": "WnH-JHG6YX7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura de los datos desde Github"
      ],
      "metadata": {
        "id": "ud0e5zbBiy_K"
      }
    },
    {
      "source": [
        "# Diccionario con las fuentes y sus URLs\n",
        "filepath_dict = {\n",
        "    'yelp': 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/main/yelp_comentarios.csv',\n",
        "    'amazon': 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/main/amazon_cells_comentarios.csv'\n",
        "\n",
        "}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['Comentario', 'Valor'], sep=';', encoding='latin-1')\n",
        "    df['Origen'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "df.head(1100)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IUoKBrJIfxo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalización de la fuente de datos."
      ],
      "metadata": {
        "id": "6K0QGMcM8Y_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## En el proceso de Normalización de datos, se realizará lo siguiente:\n",
        "\n",
        "### 1. Eliminación de signos de puntuación.\n",
        "### 2. Reducir a minúsculas todo el texto.\n",
        "### 3. Convertir a número la columna Valor para su postprocesamiento."
      ],
      "metadata": {
        "id": "bfNPZD-QrbuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Eliminación de signos de puntuación"
      ],
      "metadata": {
        "id": "sgmIsl-MrNIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de función para eliminar los signos de puntuación utilizando re, pero considerando no borrar las vocales con acento.\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Normaliza el texto a NFKD para separar letras y sus tildes\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    # Elimina los caracteres diacríticos (como las tildes)\n",
        "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
        "    # Elimina todo lo que no sea letras, números o espacios\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Aplicar la función a la columna 'review_lower'\n",
        "df['Comentarios'] = df['Comentario'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "IioeKtgqsMNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "M64iR-vGtejh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Se puede observar que la puntuación se eliminó, y las palabras con acentos mantuvieron las vocales para mayor legibilidad."
      ],
      "metadata": {
        "id": "6yND37eCuaUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Reducir a minúsculas todo el texto."
      ],
      "metadata": {
        "id": "hm-vdKNdup6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'Comentarios_lower' with lowercase values from 'Comentario'\n",
        "df['Comentarios_lower'] = df['Comentarios'].str.lower()"
      ],
      "metadata": {
        "id": "8C_RLf5TuuJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "hkoPOq8K1M1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.  Convertir a número la columna Valor para su postprocesamiento."
      ],
      "metadata": {
        "id": "-V_8blME1WMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos la columna rating a valor numérico\n",
        "df['Valor'] = pd.to_numeric(df['Valor'], errors='coerce')"
      ],
      "metadata": {
        "id": "pX8lMP2n1cX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Valor']"
      ],
      "metadata": {
        "id": "Uek7A0O31wX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento NLP"
      ],
      "metadata": {
        "id": "xCaWuodl3CY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación del objeto de SPacy para utilizar en el procesamiento del texto en español."
      ],
      "metadata": {
        "id": "VWsyoCJM-2RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = es_core_news_md.load()"
      ],
      "metadata": {
        "id": "ZgYQ6Jru-_zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización."
      ],
      "metadata": {
        "id": "TUWA7XZS_sik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comentarios_tokenizados'] = df['Comentarios_lower'].apply(lambda text: nlp(text))"
      ],
      "metadata": {
        "id": "UWTXya9V_2WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Comentarios_tokenizados','Comentarios_lower']].head()"
      ],
      "metadata": {
        "id": "ovh-gLQ_BbEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remoción de Stop Words"
      ],
      "metadata": {
        "id": "UMhymHzYCwg7"
      }
    },
    {
      "source": [
        "def parse_and_remove_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Remueve las stopwords de un objeto spaCy Doc.\n",
        "    \"\"\"\n",
        "    # Filtrar stopwords y obtener los tokens como texto\n",
        "    tokens_filtrados = [token.text for token in doc if not token.is_stop]\n",
        "    return tokens_filtrados\n",
        "\n",
        "# Aplicar la función al DataFrame\n",
        "df['Comentarios_sin_StopWords'] = df['Comentarios_tokenizados'].apply(parse_and_remove_stopwords)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NK7Z0iQOXvt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Comentarios_tokenizados','Comentarios_sin_StopWords']].head()"
      ],
      "metadata": {
        "id": "Ktbu3Xa3Xzbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lematizado"
      ],
      "metadata": {
        "id": "rFHzL5O5ZCMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizar_sin_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Devuelve una lista de lemas excluyendo las stopwords.\n",
        "\n",
        "    Parámetro:\n",
        "    - doc: objeto spaCy Doc\n",
        "\n",
        "    Retorna:\n",
        "    - Lista de lemas (str) sin stopwords\n",
        "    \"\"\"\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "# Aplicar la función y guardar el resultado en una nueva columna\n",
        "df['Comentarios_lema'] = df['Comentarios_tokenizados'].apply(lematizar_sin_stopwords)"
      ],
      "metadata": {
        "id": "s-a15Lu_ZDRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Comentarios_tokenizados','Comentarios_lema']].head(100)"
      ],
      "metadata": {
        "id": "qAbnf0-bZWAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78Nr-pJ54THw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordCloud (Nube de Palabras)\n"
      ],
      "metadata": {
        "id": "SZHIHCJeaT-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordCloud de Yelp"
      ],
      "metadata": {
        "id": "3VSV1S7nbreo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar el DataFrame\n",
        "df_yelp = df[df['Origen'] == 'yelp']\n",
        "\n",
        "# Unir todos los lemas en un solo string (comentarios lematizados ya están en listas)\n",
        "texto_yelp = ' '.join([' '.join(lemas) for lemas in df_yelp['Comentarios_lema']])\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_yelp)\n",
        "\n",
        "# Mostrar la nube\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios de Yelp\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kX8lCYg_aEyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word cloud de Amazon"
      ],
      "metadata": {
        "id": "34SB2iIsbwcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar el DataFrame\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Unir todos los lemas en un solo string (comentarios lematizados ya están en listas)\n",
        "texto_amazon = ' '.join([' '.join(lemas) for lemas in df_amazon['Comentarios_lema']])\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_amazon)\n",
        "\n",
        "# Mostrar la nube\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios de Yelp\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4JQKIXwdbyzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conteo de Palabras mas comunes"
      ],
      "metadata": {
        "id": "YevtYLGld4D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graficar_palabras_comunes(df, origen, top_n=10):\n",
        "    # Filtrar y aplanar los lemas\n",
        "    lemas = [lema for lemas in df[df['Origen'] == origen]['Comentarios_lema'] for lema in lemas]\n",
        "    conteo = Counter(lemas).most_common(top_n)\n",
        "\n",
        "    # Separar palabras y frecuencias\n",
        "    palabras, frecuencias = zip(*conteo)\n",
        "\n",
        "    # Crear gráfico\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(palabras, frecuencias, color='skyblue')\n",
        "    plt.xlabel('Frecuencia')\n",
        "    plt.title(f'Top {top_n} Palabras Más Comunes - {origen.capitalize()}')\n",
        "    plt.gca().invert_yaxis()  # Poner la palabra más común arriba\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Graficar para Yelp\n",
        "graficar_palabras_comunes(df, 'yelp')\n",
        "\n",
        "# Graficar para Amazon\n",
        "graficar_palabras_comunes(df, 'amazon')\n"
      ],
      "metadata": {
        "id": "4Vt-Btc6d7lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigramas + WordCloud"
      ],
      "metadata": {
        "id": "CvpcHB4Vh_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_bigramas_spacy(df, origen, top_n=50):\n",
        "    \"\"\"\n",
        "    Genera bigramas usando spaCy a partir de la columna 'Comentarios_Lema', sin stopwords.\n",
        "    Luego genera una nube de palabras.\n",
        "    \"\"\"\n",
        "    # Filtrar los comentarios por 'origen' (por ejemplo, 'yelp' o 'amazon')\n",
        "    comentarios = df[df['Origen'] == origen]['Comentarios_lema']\n",
        "\n",
        "    # Generar bigramas\n",
        "    bigramas = []\n",
        "    for comentario in comentarios:\n",
        "        # Crear un Doc de spaCy a partir de la lista de lemas (de la columna 'Comentarios_Lema')\n",
        "        doc = nlp(' '.join(comentario))  # Unimos la lista de lemas y lo procesamos con spaCy\n",
        "        # Extraer bigramas\n",
        "        for i in range(len(doc) - 1):\n",
        "            if not doc[i].is_stop and not doc[i+1].is_stop:  # Asegurarse de que no sean stopwords\n",
        "                bigramas.append((doc[i].lemma_, doc[i+1].lemma_))\n",
        "\n",
        "    # Contar los bigramas más comunes\n",
        "    conteo_bigramas = Counter(bigramas).most_common(top_n)\n",
        "\n",
        "    # Convertir los bigramas a formato texto \"palabra1 palabra2\"\n",
        "    bigramas_texto = {' '.join(bigrama): freq for bigrama, freq in conteo_bigramas}\n",
        "\n",
        "    # Generar la nube de palabras de los bigramas\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigramas_texto)\n",
        "\n",
        "    # Mostrar la nube\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud de Bigramas - {origen.capitalize()}\")\n",
        "    plt.show()\n",
        "\n",
        "# Generar la nube de bigramas para Yelp y Amazon\n",
        "generar_bigramas_spacy(df, 'yelp')\n",
        "generar_bigramas_spacy(df, 'amazon')\n"
      ],
      "metadata": {
        "id": "DruD_yJRgtMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀 Análisis de sentimiento en español con pysentimiento"
      ],
      "metadata": {
        "id": "GO6Gth5wGeZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Procesamiento del sentimiento con los comentarios sin procesar."
      ],
      "metadata": {
        "id": "yWdIRcQWLunE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pysentimiento import create_analyzer\n",
        "\n",
        "# Crear analizador de sentimientos\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "# Aplicar a una columna de texto\n",
        "df['Sentimiento'] = df['Comentarios'].apply(lambda x: analyzer.predict(x).output)\n",
        "# Sentimiento solo guarda lo predicho (POS, NEU o NEG)\n",
        "\n",
        "df['Probabilidad'] = df['Comentarios'].apply(lambda x: analyzer.predict(x).probas)\n",
        "#Ese diccionario contiene la probabilidad de cada clase: positivo, neutro, negativo Ejemplo: {'POS': 0.84, 'NEU': 0.10, 'NEG': 0.06}.\n",
        "\n"
      ],
      "metadata": {
        "id": "59rspiPIEWFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📊  Gráfico de barras de frecuencia de sentimientos\n"
      ],
      "metadata": {
        "id": "hPjwZg3VQej1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análisis de sentimientos con todos los datos de yelp y Amazon juntos."
      ],
      "metadata": {
        "id": "El8ih8sT30jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribución de Sentimientos')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zN86u1J4E2xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentimiento'].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    labels=['Positivo', 'Neutro', 'Negativo'],\n",
        "    colors=['lightgreen', 'lightblue', 'salmon']\n",
        ")\n",
        "plt.title('Distribución porcentual de Sentimientos')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JclTvB0JQWqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribución de sentiminetos de los comentarios de Yelp"
      ],
      "metadata": {
        "id": "vLnJpfTsQ3uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar por origen \"yelp\"\n",
        "df_yelp = df[df['Origen'] == 'yelp']\n",
        "\n",
        "# Graficar los sentimientos de Yelp\n",
        "sns.countplot(data=df_yelp, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribución de Sentimientos - Yelp')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8OmdpqN8Q0WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribución de sentiminetos de los comentarios de Amazon"
      ],
      "metadata": {
        "id": "Y3jISJCHRpaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar por origen \"amazon\"\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Graficar los sentimientos de Yelp\n",
        "sns.countplot(data=df_amazon, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribución de Sentimientos - Yelp')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Usa2F9ZwRtjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribución de sentimientos pero con los datos lematizados."
      ],
      "metadata": {
        "id": "XvHx1nbUxvpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "resultados = [analyzer.predict(texto) for texto in df['Comentarios_lema'].apply(' '.join)]\n",
        "\n",
        "# Aplicar a una columna de texto\n",
        "df['Sentimiento_lema'] = [resultado.output for resultado in resultados]\n",
        "df['Probabilidad_lema'] = [resultado.probas for resultado in resultados]"
      ],
      "metadata": {
        "id": "TiO-sKHrS6sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar por origen \"yelp\"\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Graficar los sentimientos de Yelp\n",
        "sns.countplot(data=df_amazon, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribución de Sentimientos - Yelp')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "coAP0uOSx3V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No se observan cambios respecto al análisis realizado con las frases sin lematizar."
      ],
      "metadata": {
        "id": "FOePV31v3pYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análisis de Confianza para filtrar comentarios con baja certeza"
      ],
      "metadata": {
        "id": "Rnxw1uP75JPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Máxima probabilidad (nivel de certeza del modelo)\n",
        "df['Confianza'] = df['Probabilidad'].apply(lambda x: max(x.values()))  # En este caso, de la lista {'POS': 0.84, 'NEU': 0.10, 'NEG': 0.06} sólo guarda 0.84 que es el valor mayor"
      ],
      "metadata": {
        "id": "FTUYtdov50XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar comentarios cuya confianza sea menor a 0.6\n",
        "comentarios_baja_confianza = df[df['Confianza'] < 0.6]\n",
        "\n",
        "# Ver los primeros resultados\n",
        "comentarios_baja_confianza[['Comentarios', 'Sentimiento', 'Confianza']]"
      ],
      "metadata": {
        "id": "25tVeE8N7z-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x='Confianza', bins=20, kde=True, color='skyblue')\n",
        "plt.axvline(0.6, color='red', linestyle='--', label='Umbral 0.6')\n",
        "plt.title('Distribución de Confianza del Sentimiento')\n",
        "plt.xlabel('Confianza')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nLb5WfkF6AUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(data=df, x='Sentimiento', y='Confianza', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribución de Confianza por Sentimiento')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Confianza')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bYBos64W_GWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x='Confianza', hue='Origen', bins=20, kde=True, palette='Set2')\n",
        "plt.axvline(0.6, color='red', linestyle='--', label='Umbral 0.6')\n",
        "plt.title('Distribución de Confianza por Origen')\n",
        "plt.xlabel('Confianza')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "93c3qmNh_V_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_baja_confianza = df[df['Confianza'] < 0.6]\n"
      ],
      "metadata": {
        "id": "Hu1Xi5-6BRMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Unir todos los lemas en un solo string (asumiendo que cada fila de 'Comentarios_lema' es una lista de palabras)\n",
        "texto_baja_confianza = ' '.join([' '.join(lemas) for lemas in df_baja_confianza['Comentarios_lema']])"
      ],
      "metadata": {
        "id": "LsaUcHuPBwfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='cool').generate(texto_baja_confianza)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios con Baja Confianza\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FDwXL5ztB3uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF_IDF"
      ],
      "metadata": {
        "id": "MmX_Ui9GjS38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🌟 Explicación del TF-IDF"
      ],
      "metadata": {
        "id": "MxuHGtiwk7OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency - Inverse Document Frequency) es una técnica de procesamiento de texto utilizada para evaluar la importancia de una palabra dentro de un conjunto de documentos. Se basa en dos conceptos:\n",
        "\n",
        "TF (Frecuencia de Término): Mide cuántas veces aparece un término en un documento específico, comparado con el número total de términos en ese documento.\n",
        "Esto ayuda a capturar cuán relevante es una palabra dentro de un documento en particular.\n",
        "\n",
        "IDF (Frecuencia Inversa de Documentos): Mide la importancia de una palabra dentro de un conjunto de documentos. Si una palabra aparece en muchos documentos, tiene menos valor. La fórmula es:\n",
        "\n",
        "Esto ayuda a reducir el peso de las palabras que aparecen frecuentemente en todos los documentos (como \"el\", \"y\", \"de\"), ya que no agregan mucha información.\n",
        "\n",
        "Así, la importancia de un término en un documento depende tanto de su frecuencia en ese documento como de cuán común es en todo el conjunto de documentos."
      ],
      "metadata": {
        "id": "__ox6x-IlBef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. TFIDF espera trabajar con strings y  no listas, por lo que se procesde a crear una nueva columna con los datos lematizados en formato str."
      ],
      "metadata": {
        "id": "yaI-9TITjbgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comentarios_lema_str'] = df['Comentarios_lema'].apply(lambda x: ' '.join(x))\n"
      ],
      "metadata": {
        "id": "TR3M-4mmjpg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Cálculo de TF-IDF con TfidVetorizer."
      ],
      "metadata": {
        "id": "bzD19US8jw10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el vectorizador\n",
        "tfidfvectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Ajustar y transformar\n",
        "tfidf_matrix = tfidfvectorizer.fit_transform(df['Comentarios_lema_str'])\n",
        "\n",
        "# Obtener los términos\n",
        "features = tfidfvectorizer.get_feature_names_out()\n",
        "\n"
      ],
      "metadata": {
        "id": "UyySUzAlj2A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Convertir a dataframe para posterior análisis."
      ],
      "metadata": {
        "id": "jOi7smzXkgVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear DataFrame TF-IDF\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=features)\n",
        "\n",
        "# Sumamos por columna para ver los términos más importantes globalmente\n",
        "tfidf_scores = df_tfidf.sum().sort_values(ascending=False)\n",
        "\n",
        "# Mostrar top 10\n",
        "print(tfidf_scores.head(10).round(3))\n",
        "\n",
        "# Add this code block after the first TF-IDF transformation and before the split\n",
        "print(f\"Number of features in tfidf_matrix after fitting: {tfidf_matrix.shape[1]}\")\n",
        "print(f\"Number of features in df_tfidf: {df_tfidf.shape[1]}\")"
      ],
      "metadata": {
        "id": "Qy0sAmK6km33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Aplicando TF-IDF y Gráficos"
      ],
      "metadata": {
        "id": "4QnMIDTMlgAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gráfico de todos los comentarios (Yelp y Amanzon)"
      ],
      "metadata": {
        "id": "WXKbstDDl4Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tomar los 10 términos con mayor puntuación TF-IDF\n",
        "top_tfidf = tfidf_scores.head(10)\n",
        "\n",
        "# Crear gráfico de barras\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_tfidf.plot(kind='barh', color='skyblue')\n",
        "plt.title(\"Top 10 Términos con Mayor TF-IDF\")\n",
        "plt.xlabel(\"Puntaje TF-IDF\")\n",
        "plt.ylabel(\"Término\")\n",
        "plt.gca().invert_yaxis()  # Para que el mayor quede arriba\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XB-PFMLXktOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gráfico para comentarios de YELP"
      ],
      "metadata": {
        "id": "1t9nGg7NlxFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar comentarios de Yelp\n",
        "df_yelp = df[df['Origen'] == 'yelp']\n",
        "\n",
        "# Calcular TF-IDF solo para los comentarios de Yelp\n",
        "tfidf_yelp_matrix = tfidfvectorizer.transform(df_yelp['Comentarios_lema_str'])\n",
        "tfidf_yelp_scores = tfidf_yelp_matrix.sum(axis=0).A1  # Sumar por columna\n",
        "yelp_features = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_yelp_tfidf = pd.DataFrame(tfidf_yelp_scores, index=yelp_features, columns=['TF-IDF'])\n",
        "df_yelp_tfidf = df_yelp_tfidf.sort_values(by='TF-IDF', ascending=False)\n",
        "\n",
        "# Graficar los top 10 términos de Yelp\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_yelp_tfidf.head(10).plot(kind='barh', color='lightgreen')\n",
        "plt.title(\"Top 10 Términos con Mayor TF-IDF - Yelp\")\n",
        "plt.xlabel(\"Puntaje TF-IDF\")\n",
        "plt.ylabel(\"Término\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MVx_NaJYl06b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gráfico para comentarios de Amazon"
      ],
      "metadata": {
        "id": "zqdrlUllmNaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar comentarios de Amazon\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Calcular TF-IDF solo para los comentarios de Amazon\n",
        "# Change fit_transform to transform here\n",
        "tfidf_amazon_matrix = tfidfvectorizer.transform(df_amazon['Comentarios_lema_str'])\n",
        "tfidf_amazon_scores = tfidf_amazon_matrix.sum(axis=0).A1  # Sumar por columna\n",
        "# Use the features learned from the full dataset\n",
        "amazon_features = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_amazon_tfidf = pd.DataFrame(tfidf_amazon_scores, index=amazon_features, columns=['TF-IDF'])\n",
        "df_amazon_tfidf = df_amazon_tfidf.sort_values(by='TF-IDF', ascending=False)\n",
        "\n",
        "# Graficar los top 10 términos de Amazon\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_amazon_tfidf.head(10).plot(kind='barh', color='yellow')\n",
        "plt.title(\"Top 10 Términos con Mayor TF-IDF - amazon\")\n",
        "plt.xlabel(\"Puntaje TF-IDF\")\n",
        "plt.ylabel(\"Término\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Ensure the rest of your code, including the cell with the error, remains the same\n",
        "# as the `tfidfvectorizer` object will now retain the vocabulary from the full dataset."
      ],
      "metadata": {
        "id": "M09ICYRomOlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BoW (Bag of Words)."
      ],
      "metadata": {
        "id": "E0IoYi5ip5pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW convierte un conjunto de documentos en una matriz de ocurrencias de palabras. A diferencia de TF-IDF, que pondera las palabras según su frecuencia e importancia en relación con todo el corpus, BoW solo cuenta cuántas veces aparece una palabra en un documento sin considerar la frecuencia global de la palabra."
      ],
      "metadata": {
        "id": "CYjaC_KyqTck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciamos el vectorizador BoW\n",
        "vectorizer_bow = CountVectorizer()\n",
        "\n",
        "# Aplicamos el vectorizador a los comentarios lematizados\n",
        "X_bow = vectorizer_bow.fit_transform(df['Comentarios_lema_str'])\n",
        "\n",
        "# Convertimos la matriz de características en un DataFrame para visualizar\n",
        "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
        "\n",
        "# Obtener los nombres de las características (palabras)\n",
        "features_bow = vectorizer_bow.get_feature_names_out()\n",
        "\n",
        "# Crear un DataFrame con las frecuencias de las palabras\n",
        "df_bow = pd.DataFrame(X_bow.toarray(), columns=features_bow)\n",
        "\n",
        "# Sumar las frecuencias de palabras por columna\n",
        "bow_word_frequencies = df_bow.sum().sort_values(ascending=False)\n",
        "\n",
        "# Mostrar las top 10 palabras más frecuentes\n",
        "print(bow_word_frequencies.head(10))"
      ],
      "metadata": {
        "id": "SbBUwjR8p9rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico de las 10 palabras más frecuentes en BoW\n",
        "top_words = bow_word_frequencies.head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_words.index, y=top_words.values, palette='viridis')\n",
        "plt.title('Top 10 palabras más frecuentes (BoW)')\n",
        "plt.xlabel('Palabras')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aK4Up7kLsCgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la nube de palabras con BoW\n",
        "wordcloud_bow = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bow_word_frequencies)\n",
        "\n",
        "# Mostrar la nube de palabras\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud_bow, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de palabras (BoW)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XogoJQjdsG5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruebas de Modelos de Machine Learning"
      ],
      "metadata": {
        "id": "s755wLyIyO4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separación Train-Test"
      ],
      "metadata": {
        "id": "gW5hk2E6y-gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino las características (X) y las etiquetas (y)\n",
        "X = df_tfidf  # Las características son los valores del TF-IDF\n",
        "y = df['Valor']  # La variable objetivo es 'Valor' (0 o 1)\n",
        "\n",
        "# Paso 2: Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "WhtOCFZsyR4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresión Logistica"
      ],
      "metadata": {
        "id": "BUm2hZ5g375q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento del modelo con TI-FID."
      ],
      "metadata": {
        "id": "fe03QqRtziYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear y entrenar el modelo de regresión logística\n",
        "logreg_model = LogisticRegression(max_iter=1000)\n",
        "logreg_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2Wf41pX0zi9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Realizar predicción con Test y evaluar el Accuarcy"
      ],
      "metadata": {
        "id": "sngD9pOkzrlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacer predicciones\n",
        "y_pred_logreg = logreg_model.predict(X_test)\n",
        "\n",
        "# Evaluar precisión\n",
        "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
        "print(f'Precisión de la regresión logística: {accuracy_logreg:.3f}')"
      ],
      "metadata": {
        "id": "SX2z04VNzxbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluacion del modelo."
      ],
      "metadata": {
        "id": "Vt98TGLTz8jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score,\n",
        "    accuracy_score, precision_score, recall_score, f1_score\n",
        ")\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred_logreg)\n",
        "labels = ['Negativo', 'Positivo']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()\n",
        "\n",
        "# 2. Curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, logreg_model.decision_function(X_test))\n",
        "roc_auc = roc_auc_score(y_test, logreg_model.decision_function(X_test))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Área bajo la curva = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# 3. Métricas\n",
        "accuracy = accuracy_score(y_test, y_pred_logreg)\n",
        "precision = precision_score(y_test, y_pred_logreg)\n",
        "recall = recall_score(y_test, y_pred_logreg)\n",
        "f1 = f1_score(y_test, y_pred_logreg)\n",
        "\n",
        "print(\"Métricas de desempeño del modelo:\")\n",
        "print(f\"Accuracy : {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall   : {recall:.2f}\")\n",
        "print(f\"F1 Score : {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "VYXEP3Ck1hqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Métricas basadas en la matriz:\n",
        "\n",
        "True Positives (TP) = 136 → positivos bien predichos\n",
        "\n",
        "True Negatives (TN) = 158 → negativos bien predichos\n",
        "\n",
        "False Positives (FP) = 61 → negativos clasificados como positivos\n",
        "\n",
        "False Negatives (FN) = 45 → positivos clasificados como negativos\n",
        "\n",
        "📊 Métricas basadas en el desempeño:\n",
        "\n",
        "🔹 Accuracy (0.73): el modelo predice correctamente el 73% de los casos totales.\n",
        "\n",
        "🔹 Precision (0.69): de todos los comentarios que el modelo predijo como positivos, el 69% realmente lo eran. Podría estar dando algunos falsos positivos.\n",
        "\n",
        "🔹 Recall (0.75): de todos los comentarios positivos reales, el modelo logró detectar el 75%. Esto está bastante bien.\n",
        "\n",
        "🔹 F1 Score (0.72): el equilibrio entre precisión y recall es bueno."
      ],
      "metadata": {
        "id": "1VyOSFCN3nmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba de modelo"
      ],
      "metadata": {
        "id": "kGG3NVXJ_wb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nueva_reseña = \"Estoy disconforme con la atención\"  # Reemplaza con la reseña que deseas probar\n",
        "nueva_reseña_tfidf = tfidfvectorizer.transform([nueva_reseña])\n",
        "prediccion = logreg_model.predict(nueva_reseña_tfidf)\n",
        "# Obtener la probabilidad de la predicción\n",
        "probabilidadpositiva = logreg_model.predict_proba(nueva_reseña_tfidf)\n",
        "\n",
        "# Obtener la probabilidad en la clase predicha (0 o 1)\n",
        "probabilidad = probabilidadpositiva[0][1]  # Probabilidad de la clase \"positivo\"\n",
        "\n",
        "# After tfidfvectorizer.fit_transform(df['Comentarios_lema_str']):\n",
        "print(f\"Shape of tfidf_matrix after fit_transform: {tfidf_matrix.shape}\")\n",
        "\n",
        "# After X = df_tfidf:\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "\n",
        "# After train_test_split:\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "\n",
        "# Before prediction:\n",
        "print(f\"Shape of nueva_reseña_tfidf: {nueva_reseña_tfidf.shape}\")"
      ],
      "metadata": {
        "id": "EmJqPQkc-p8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buscando el mejor modelo"
      ],
      "metadata": {
        "id": "YwZfAgSQ6UOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validación Cruzada del modelo."
      ],
      "metadata": {
        "id": "BIiheckODjwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Pipeline: vectorización + modelo\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Definir el espacio de búsqueda\n",
        "param_grid = {\n",
        "    'tfidf__max_df': [0.8, 1.0],\n",
        "    'tfidf__min_df': [1, 5],\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'clf__C': [0.1, 1, 10]  # regularización de la regresión logística\n",
        "}\n",
        "\n",
        "# GridSearch con validación cruzada\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Entrenar\n",
        "grid.fit(df['Comentarios_lema_str'], df['Valor'])\n",
        "\n",
        "# Mejor modelo\n",
        "print(\"Mejores parámetros encontrados:\")\n",
        "print(grid.best_params_)\n",
        "print(f\"Mejor F1 score: {grid.best_score_:.3f}\")\n",
        "\n",
        "modelo_best = grid.best_params_\n",
        "\n"
      ],
      "metadata": {
        "id": "l25bWzq-2XuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisión media (F1 = 0.742): el modelo logra un buen equilibrio entre precisión y recall.\n",
        "\n",
        "Desviación estándar baja (±0.026): hay poca variación entre los diferentes subconjuntos de datos usados durante la validación cruzada, lo que sugiere que el modelo generaliza bien."
      ],
      "metadata": {
        "id": "ti2EA2jLDa0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "# Validación cruzada con 5 particiones (k-fold = 5)\n",
        "scores = cross_val_score(pipeline, df['Comentarios_lema_str'], df['Valor'], cv=5, scoring='accuracy')\n",
        "\n",
        "# Resultados\n",
        "print(f\"Precisión media con validación cruzada: {scores.mean():.3f}\")\n",
        "print(f\"Desviación estándar: {scores.std():.3f}\")\n"
      ],
      "metadata": {
        "id": "g-ZZmtnQC_gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba del modelo Regresion Logística usando BoW"
      ],
      "metadata": {
        "id": "MTi4rPCaqGTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
        "\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar modelo\n",
        "model_bow = LogisticRegression(solver='liblinear')\n",
        "model_bow.fit(X_train_bow, y_train_bow)\n",
        "\n",
        "# Predecir\n",
        "y_pred_bow = model_bow.predict(X_test_bow)\n",
        "y_prob_bow = model_bow.decision_function(X_test_bow)\n",
        "\n",
        "# Métricas\n",
        "print(\"Reporte de Clasificación:\\n\")\n",
        "print(classification_report(y_test_bow, y_pred_bow, digits=3))\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_test_bow, y_pred_bow)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negativo\", \"Positivo\"], yticklabels=[\"Negativo\", \"Positivo\"])\n",
        "plt.title(\"Matriz de Confusión\")\n",
        "plt.xlabel(\"Predicción\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()\n",
        "\n",
        "# Curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test_bow, y_prob_bow)\n",
        "roc_auc = roc_auc_score(y_test_bow, y_prob_bow)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VOvXOmo5qKDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Redes Neuronales"
      ],
      "metadata": {
        "id": "CsI99F6eci-J"
      }
    }
  ]
}