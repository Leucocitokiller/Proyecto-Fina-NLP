{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leucocitokiller/Proyecto-Fina-NLP/blob/main/NLP_Jack_Reacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7624ac4",
      "metadata": {
        "id": "f7624ac4"
      },
      "source": [
        "# üìö An√°lisis NLP sobre texto de una novela."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL del archivo en GitHub (debe ser la URL raw)\n",
        "url = 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/refs/heads/main/Zona%20peligrosa%20-%20Lee%20Child.txt'\n",
        "\n",
        "# Descargar el contenido del archivo\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar si la descarga fue exitosa\n",
        "if response.status_code == 200:\n",
        "    libro = response.text\n",
        "    print(\"Archivo cargado correctamente.\")\n",
        "else:\n",
        "    print(\"Hubo un error al cargar el archivo.\")\n",
        "\n",
        "# Mostrar las primeras 500 palabras del texto\n",
        "\n",
        "print(libro[:500])\n"
      ],
      "metadata": {
        "id": "FXzghY-_-Ife",
        "outputId": "9711cfa8-e940-4992-ae3a-3a7616582e76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FXzghY-_-Ife",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo cargado correctamente.\n",
            "A primera vista, Margrave parece uno de esos pueblos apacibles donde nunca pasa nada. Jack Reacher, un exmilitar convertido en trotamundos, acaba de llegar all√≠ y tarda menos de una hora en comprobar que las apariencias enga√±an. Detenido mientras desayunaba en una cafeter√≠a, Reacher, el √∫nico forastero de la ciudad, es acusado de asesinato. A pesar de su inocencia, los indicios empiezan a acumularse en su contra. Si quiere escapar con vida del nido de serpientes en el que se encuentra, tendr√° qu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizaci√≥n\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing punkt_tab dataset\n",
        "\n",
        "tokens = word_tokenize(libro)\n",
        "\n",
        "# Eliminar stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))  # O 'english' si est√° en ingl√©s\n",
        "tokens_filtrados = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Lematizaci√≥n\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in tokens_filtrados]\n",
        "\n",
        "# Ver los primeros 50 lemas\n",
        "print(lemmas[:50])\n"
      ],
      "metadata": {
        "id": "sfO_VXXf-2jX",
        "outputId": "9079b4ab-6b4c-4b63-ab69-680a7a2802c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sfO_VXXf-2jX",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['primera', 'vista', ',', 'Margrave', 'parece', 'pueblo', 'apacibles', 'nunca', 'pasa', '.', 'Jack', 'Reacher', ',', 'exmilitar', 'convertido', 'trotamundos', ',', 'acaba', 'llegar', 'all√≠', 'tarda', 'menos', 'hora', 'comprobar', 'apariencias', 'enga√±an', '.', 'Detenido', 'mientras', 'desayunaba', 'cafeter√≠a', ',', 'Reacher', ',', '√∫nico', 'forastero', 'ciudad', ',', 'acusado', 'asesinato', '.', 'pesar', 'inocencia', ',', 'indicios', 'empiezan', 'acumularse', '.', 'Si', 'quiere']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Zsvhqb1L4LUm"
      },
      "id": "Zsvhqb1L4LUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7818e3f",
      "metadata": {
        "id": "a7818e3f"
      },
      "outputs": [],
      "source": [
        "# üì• 1. Cargar texto desde URL\n",
        "import requests\n",
        "\n",
        "url = 'https://www.gutenberg.org/files/2000/2000-0.txt'\n",
        "texto = requests.get(url).text\n",
        "print(texto[:1000])  # Mostrar los primeros 1000 caracteres\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04098ce",
      "metadata": {
        "id": "a04098ce"
      },
      "outputs": [],
      "source": [
        "# üß† 2. Procesamiento NLP con spaCy\n",
        "import spacy\n",
        "from spacy.lang.es.stop_words import STOP_WORDS\n",
        "\n",
        "# Download the model if it's not installed\n",
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Increase the max_length limit\n",
        "nlp.max_length = len(texto)  # Set to the length of your text\n",
        "\n",
        "doc = nlp(texto)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc39a0f1",
      "metadata": {
        "id": "cc39a0f1"
      },
      "outputs": [],
      "source": [
        "# üßπ 3. Lematizaci√≥n, remoci√≥n de stopwords y puntuaci√≥n\n",
        "tokens_limpios = [token.lemma_.lower() for token in doc\n",
        "                  if token.is_alpha and token.lemma_.lower() not in STOP_WORDS]\n",
        "print(tokens_limpios[:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e120f07c",
      "metadata": {
        "id": "e120f07c"
      },
      "outputs": [],
      "source": [
        "# üî† 4. POS Tagging (etiquetado gramatical)\n",
        "for token in doc[:10]:\n",
        "    print(f\"{token.text} -> {token.pos_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7f90ff",
      "metadata": {
        "id": "4c7f90ff"
      },
      "outputs": [],
      "source": [
        "# üßæ 5. Named Entity Recognition (NER)\n",
        "for ent in doc.ents[:10]:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3376299",
      "metadata": {
        "id": "a3376299"
      },
      "outputs": [],
      "source": [
        "# üìä 6. Palabras m√°s frecuentes\n",
        "from collections import Counter\n",
        "\n",
        "frecuencia = Counter(tokens_limpios).most_common(10)\n",
        "for palabra, freq in frecuencia:\n",
        "    print(f\"{palabra}: {freq}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7451696d",
      "metadata": {
        "id": "7451696d"
      },
      "outputs": [],
      "source": [
        "# ‚òÅÔ∏è 7. WordCloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(tokens_limpios))\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud del texto\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89959e82",
      "metadata": {
        "id": "89959e82"
      },
      "outputs": [],
      "source": [
        "# üìõ 8. Bigramas m√°s frecuentes\n",
        "bigrams = zip(tokens_limpios, tokens_limpios[1:])\n",
        "bigrams_freq = Counter(bigrams).most_common(10)\n",
        "\n",
        "for par, freq in bigrams_freq:\n",
        "    print(f\"{par}: {freq}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "palabras_df = pd.DataFrame(frecuencia, columns=['palabra', 'frecuencia'])\n",
        "sns.barplot(data=palabras_df, x='frecuencia', y='palabra')\n",
        "plt.title(\"Top 10 palabras m√°s frecuentes\")\n",
        "plt.xlabel(\"Frecuencia\")\n",
        "plt.ylabel(\"Palabra\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wXmJS9e8p4Zt"
      },
      "id": "wXmJS9e8p4Zt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ents = [ent.text for ent in doc.ents if ent.label_ in ['PER', 'LOC', 'ORG']]\n",
        "ent_freq = Counter(ents).most_common(10)\n",
        "pd.DataFrame(ent_freq, columns=[\"Entidad\", \"Frecuencia\"]).plot.bar(x='Entidad', y='Frecuencia', legend=False)\n",
        "plt.title(\"Entidades nombradas m√°s frecuentes\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YgUdLNePqIYN"
      },
      "id": "YgUdLNePqIYN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "pos_counts = Counter([token.pos_ for token in doc if token.is_alpha])\n",
        "sns.barplot(x=list(pos_counts.keys()), y=list(pos_counts.values()))\n",
        "plt.title(\"Distribuci√≥n de categor√≠as gramaticales\")\n",
        "plt.xlabel(\"POS\")\n",
        "plt.ylabel(\"Cantidad\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zW9p1JGOqQDO"
      },
      "id": "zW9p1JGOqQDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "G = nx.Graph()\n",
        "for (w1, w2), freq in bigrams_freq:\n",
        "    G.add_edge(w1, w2, weight=freq)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "nx.draw_networkx(G, with_labels=True, node_size=1500, font_size=10)\n",
        "plt.title(\"Bigramas m√°s frecuentes\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HbGTdMf0qx62"
      },
      "id": "HbGTdMf0qx62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Assuming 'tokens_limpios' from previous cell contains the processed text\n",
        "texto_procesado = ' '.join(tokens_limpios)  # Join the tokens into a string\n",
        "\n",
        "# An√°lisis de sentimientos por p√°rrafo o l√≠nea\n",
        "sentimientos = []\n",
        "for frase in texto_procesado.split('\\n'):\n",
        "    blob = TextBlob(frase)\n",
        "    sentimientos.append((frase, blob.sentiment.polarity))\n",
        "\n",
        "# Mostrar las frases m√°s positivas y m√°s negativas\n",
        "sentimientos_ordenados = sorted(sentimientos, key=lambda x: x[1])\n",
        "print(\"Frase m√°s negativa:\\n\", sentimientos_ordenados[0])\n",
        "print(\"\\nFrase m√°s positiva:\\n\", sentimientos_ordenados[-1])\n"
      ],
      "metadata": {
        "id": "4lJgx5YtrfLM"
      },
      "id": "4lJgx5YtrfLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "sentimientos = []\n",
        "for frase in texto_procesado.split('\\n'):\n",
        "    score = sia.polarity_scores(frase)['compound']\n",
        "    sentimientos.append((frase, score))\n",
        "\n",
        "# Frases con sentimiento m√°s marcado\n",
        "sentimientos_ordenados = sorted(sentimientos, key=lambda x: x[1])\n",
        "print(\"Frase m√°s negativa:\\n\", sentimientos_ordenados[0])\n",
        "print(\"\\nFrase m√°s positiva:\\n\", sentimientos_ordenados[-1])\n"
      ],
      "metadata": {
        "id": "PXIYU6y3r7HO"
      },
      "id": "PXIYU6y3r7HO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "polaridades = [s[1] for s in sentimientos]\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(polaridades)\n",
        "plt.title(\"Evoluci√≥n del sentimiento a lo largo del texto\")\n",
        "plt.xlabel(\"L√≠nea del texto\")\n",
        "plt.ylabel(\"Polaridad (-1 a 1)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ipn8AB--sDBh"
      },
      "id": "ipn8AB--sDBh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CdgfV6oStFvj"
      },
      "id": "CdgfV6oStFvj"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}