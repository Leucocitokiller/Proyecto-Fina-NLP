{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leucocitokiller/Proyecto-Fina-NLP/blob/main/NLP_Jack_Reacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7624ac4",
      "metadata": {
        "id": "f7624ac4"
      },
      "source": [
        "# üìö An√°lisis NLP sobre texto de una novela."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL del archivo en GitHub (debe ser la URL raw)\n",
        "url = 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/refs/heads/main/Zona%20peligrosa%20-%20Lee%20Child.txt'\n",
        "\n",
        "# Descargar el contenido del archivo\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar si la descarga fue exitosa\n",
        "if response.status_code == 200:\n",
        "    libro = response.text\n",
        "    print(\"Archivo cargado correctamente.\")\n",
        "else:\n",
        "    print(\"Hubo un error al cargar el archivo.\")\n",
        "\n",
        "# Mostrar las primeras 500 palabras del texto\n",
        "\n",
        "print(libro[:500])\n"
      ],
      "metadata": {
        "id": "FXzghY-_-Ife"
      },
      "id": "FXzghY-_-Ife",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† T√©cnicas de NLP aplicadas"
      ],
      "metadata": {
        "id": "quQ4TNPEC2s_"
      },
      "id": "quQ4TNPEC2s_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tokenizaci√≥n**\n",
        "### **Qu√© es:**\n",
        "### Es el proceso de dividir un texto en unidades m√°s peque√±as llamadas tokens (normalmente palabras).\n",
        "\n",
        "### **Para qu√© sirve:**\n",
        "### Permite analizar el texto palabra por palabra. Es el primer paso para casi todas las tareas de NLP.\n",
        "\n"
      ],
      "metadata": {
        "id": "kCsOBRF1C6zV"
      },
      "id": "kCsOBRF1C6zV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizaci√≥n\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing punkt_tab dataset\n",
        "\n",
        "tokens = word_tokenize(libro)\n",
        "\n"
      ],
      "metadata": {
        "id": "sfO_VXXf-2jX"
      },
      "id": "sfO_VXXf-2jX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-oUp_4fHYaa"
      },
      "id": "0-oUp_4fHYaa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Lematizaci√≥n**\n",
        "## **Qu√© es:**\n",
        "## Consiste en reducir las palabras a su forma base o ra√≠z (lema).\n",
        "## Ejemplo: \"corriendo\", \"corr√≠a\", \"corriste\" ‚Üí \"correr\".\n",
        "\n",
        "## **Para qu√© sirve:**\n",
        "## Ayuda a agrupar palabras similares para an√°lisis m√°s precisos. Muy √∫til en an√°lisis de sentimientos, b√∫squeda de informaci√≥n o resumen autom√°tico."
      ],
      "metadata": {
        "id": "lxdjifHCDULf"
      },
      "id": "lxdjifHCDULf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Lematizaci√≥n\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Ver los primeros 50 lemas\n",
        "print(lemmas[:100])\n"
      ],
      "metadata": {
        "id": "NmxrIEgpDhNA"
      },
      "id": "NmxrIEgpDhNA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üßπ 3. Limpieza **\n",
        "## -STOP_WORDS: quita palabras comunes como \"de\", \"que\", \"en\", \"la\", que no aportan significado √∫til.\n",
        "\n",
        "## -string.punctuation: elimina signos como .,!?¬ø...\n",
        "\n",
        "## -isalpha(): se asegura de quedarte solo con palabras (sin n√∫meros ni s√≠mbolos)."
      ],
      "metadata": {
        "id": "wM8XvHidD0xv"
      },
      "id": "wM8XvHidD0xv"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.es.stop_words import STOP_WORDS\n",
        "import string\n",
        "\n",
        "# Cargar modelo\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Suponemos que ya ten√©s esto:\n",
        "# tokens -> lista de tokens\n",
        "# lemmas -> lista de lemas (usando tokens)\n",
        "\n",
        "# Limpieza de lemas:\n",
        "lemmas_limpios = [\n",
        "    palabra.lower()\n",
        "    for palabra in lemmas\n",
        "    if palabra.lower() not in STOP_WORDS             # eliminar stopwords\n",
        "    and palabra not in string.punctuation             # eliminar puntuaci√≥n\n",
        "    and palabra.isalpha()                             # solo palabras (no n√∫meros, etc.)\n",
        "]\n",
        "\n",
        "print(lemmas_limpios[:30])  # Mostrar primeros 30 para chequear\n",
        "\n"
      ],
      "metadata": {
        "id": "qMF8HfJ8EBMu"
      },
      "id": "qMF8HfJ8EBMu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. POS-tagging (Part-of-Speech Tagging)**\n",
        "## **Qu√© es:**\n",
        "## Es etiquetar cada palabra con su categor√≠a gramatical: sustantivo, verbo, adjetivo, etc.\n",
        "\n",
        "## **Para qu√© sirve:**\n",
        "## Permite hacer an√°lisis gramaticales y entender mejor la estructura del texto. Es √∫til en traducci√≥n autom√°tica, an√°lisis sint√°ctico y generaci√≥n de texto."
      ],
      "metadata": {
        "id": "RuAzMBpMENIe"
      },
      "id": "RuAzMBpMENIe"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Download the model if it's not installed\n",
        "!python -m spacy download es_core_news_sm # This line downloads the model\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "doc = nlp(libro)\n",
        "\n",
        "# Lemas y POS por palabra\n",
        "for token in doc[:10]:\n",
        "    print(f\"{token.text} ‚Üí  POS: {token.pos_}\")"
      ],
      "metadata": {
        "id": "XhdMxY9RFbK1"
      },
      "id": "XhdMxY9RFbK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Contar cu√°ntos verbos, sustantivos, etc. hay:"
      ],
      "metadata": {
        "id": "t575xIstJmRA"
      },
      "id": "t575xIstJmRA"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "pos_counts = Counter(token.pos_ for token in doc)\n",
        "print(pos_counts)\n"
      ],
      "metadata": {
        "id": "HHTbS_1lJd3B"
      },
      "id": "HHTbS_1lJd3B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Gr√°fica de los resultados:"
      ],
      "metadata": {
        "id": "1kNBcV1bJsYg"
      },
      "id": "1kNBcV1bJsYg"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels, values = zip(*pos_counts.items())\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(labels, values, color='skyblue')\n",
        "plt.title(\"üìä Distribuci√≥n de Partes del Discurso (POS)\", fontsize=14)\n",
        "plt.xlabel(\"Categor√≠a Gramatical\", fontsize=12)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=12)\n",
        "\n",
        "# Mejoras visuales\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.tight_layout()  # Ajusta todo para que no se solapen los elementos\n",
        "\n",
        "# Agrega etiquetas de valor sobre las barras si quer√©s\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, height + 2, str(height), ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nB3UlWcjJi6I"
      },
      "id": "nB3UlWcjJi6I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Reconocimiento de Entidades Nombradas (NER)**\n",
        "## **Qu√© es:**\n",
        "## Detecta entidades importantes como nombres de personas, lugares, fechas, organizaciones, etc.\n",
        "\n",
        "## **Para qu√© sirve:**\n",
        "## Es fundamental para tareas como extracci√≥n de informaci√≥n, motores de b√∫squeda inteligentes o asistentes virtuales."
      ],
      "metadata": {
        "id": "3O8TiholEaoG"
      },
      "id": "3O8TiholEaoG"
    },
    {
      "cell_type": "code",
      "source": [
        "# üßæ 5. Named Entity Recognition (NER)\n",
        "entidades = [{\"texto\": ent.text, \"tipo\": ent.label_} for ent in doc.ents]\n",
        "\n",
        "# Mostrar algunas entidades\n",
        "for entidad in entidades[:10]:\n",
        "    print(entidad)\n"
      ],
      "metadata": {
        "id": "W5whDwJdFzVl"
      },
      "id": "W5whDwJdFzVl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Palabras m√°s frecuentes**\n",
        "## **Qu√© es:**\n",
        "## Contar qu√© palabras aparecen m√°s veces en el texto despu√©s de limpiar el contenido.\n",
        "\n",
        "## **Para qu√© sirve:**\n",
        "## Ayuda a identificar temas principales o patrones en el texto. Es com√∫n en an√°lisis exploratorios y visualizaci√≥n de texto."
      ],
      "metadata": {
        "id": "dmfxig0pEoKu"
      },
      "id": "dmfxig0pEoKu"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä 6. Palabras m√°s frecuentes\n",
        "from collections import Counter\n",
        "\n",
        "frecuencia = Counter(tokens_filtrados).most_common(50)\n",
        "for palabra, freq in frecuencia:\n",
        "    print(f\"{palabra}: {freq}\")"
      ],
      "metadata": {
        "id": "KrZ_3naKF2jV"
      },
      "id": "KrZ_3naKF2jV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. WordCloud (Nube de Palabras)**\n",
        "## **Qu√© es:**\n",
        "## Una visualizaci√≥n que muestra las palabras m√°s frecuentes en tama√±o proporcional a su frecuencia.\n",
        "\n",
        "## **Para qu√© sirve:**\n",
        "## Es una forma r√°pida y visual de entender de qu√© trata un texto sin leerlo todo."
      ],
      "metadata": {
        "id": "x4hyEMCKE3Pu"
      },
      "id": "x4hyEMCKE3Pu"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚òÅÔ∏è 7. WordCloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(lemmas_limpios))\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud del texto\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HpIxD9UyF47d"
      },
      "id": "HpIxD9UyF47d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Bigramas / Trigramas (n-gramas)**\n",
        "## **Qu√© es:**\n",
        "## Son combinaciones de palabras consecutivas.\n",
        "## Ejemplo: bigrama de ‚ÄúDon Quijote cabalgaba‚Äù ‚Üí (‚ÄúDon‚Äù, ‚ÄúQuijote‚Äù), (‚ÄúQuijote‚Äù, ‚Äúcabalgaba‚Äù).\n",
        "\n",
        "## **Para qu√© sirve:**\n",
        "## Permite detectar frases frecuentes y patrones en c√≥mo se usan las palabras juntas. Muy usado en modelado de lenguaje, traducci√≥n y detecci√≥n de estilo."
      ],
      "metadata": {
        "id": "9KfFIpxZFDcv"
      },
      "id": "9KfFIpxZFDcv"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìõ 8. Bigramas m√°s frecuentes\n",
        "from collections import Counter\n",
        "\n",
        "# Crear bigramas correctamente (palabras consecutivas)\n",
        "bigrams = list(zip(lemmas_limpios, lemmas_limpios[1:]))\n",
        "\n",
        "# Contar frecuencia de los bigramas\n",
        "bigrams_freq = Counter(bigrams).most_common(10)\n",
        "\n",
        "# Guardar los resultados en una variable\n",
        "bigrams_mas_frecuentes = [{\"bigrama\": bg, \"frecuencia\": freq} for bg, freq in bigrams_freq]\n",
        "\n",
        "# Mostrar los bigramas\n",
        "for item in bigrams_mas_frecuentes:\n",
        "    print(f\"{item['bigrama']}: {item['frecuencia']}\")\n"
      ],
      "metadata": {
        "id": "TTQcWIcAF-Rd"
      },
      "id": "TTQcWIcAF-Rd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß© ¬øC√≥mo ver frases completas?**"
      ],
      "metadata": {
        "id": "x3hvgOA-7-dX"
      },
      "id": "x3hvgOA-7-dX"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Cargar el modelo en espa√±ol\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Procesar el texto completo\n",
        "doc = nlp(libro)\n",
        "\n",
        "# Extraer todas las oraciones\n",
        "oraciones = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "# Mostrar las primeras 10 oraciones\n",
        "for i, oracion in enumerate(oraciones[:10]):\n",
        "    print(f\"{i+1}: {oracion}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bSVjujJm8FpP"
      },
      "id": "bSVjujJm8FpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 üß† Palabras clave de pelea.**\n",
        "\n"
      ],
      "metadata": {
        "id": "UU7UydC19qlK"
      },
      "id": "UU7UydC19qlK"
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_pelea = [\"pelea\", \"golpear\", \"golpe√≥\", \"pu√±etazo\", \"disparo\", \"dispar√≥\",\n",
        "                  \"patada\", \"empuj√≥\", \"empuj√≥n\", \"estrangul√≥\", \"golpes\", \"agresi√≥n\",\n",
        "                  \"violencia\", \"derrib√≥\", \"lucha\", \"forcejeo\", \"combate\"]\n"
      ],
      "metadata": {
        "id": "ew31pOpE9q_v"
      },
      "id": "ew31pOpE9q_v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üîß C√≥digo completo para detectar peleas por cap√≠tulo**"
      ],
      "metadata": {
        "id": "oMJoyz9N9z5C"
      },
      "id": "oMJoyz9N9z5C"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar modelo de spaCy en espa√±ol\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Palabras clave\n",
        "palabras_pelea = [\"pelea\", \"golpear\", \"golpe√≥\", \"pu√±etazo\", \"disparo\", \"dispar√≥\",\n",
        "                  \"patada\", \"empuj√≥\", \"empuj√≥n\", \"estrangul√≥\", \"golpes\", \"agresi√≥n\",\n",
        "                  \"violencia\", \"derrib√≥\", \"lucha\", \"forcejeo\", \"combate\"]\n",
        "\n",
        "# Separar por cap√≠tulos (suponiendo que empiezan con \"CAP√çTULO\" o n√∫mero)\n",
        "import re\n",
        "\n",
        "# Divide cuando encuentra un n√∫mero entero al principio de una l√≠nea (cap√≠tulo nuevo)\n",
        "capitulos = re.split(r'\\n\\s*\\d+\\s*\\n', libro)\n",
        "capitulos = [c.strip() for c in capitulos if len(c.strip()) > 100]  # eliminamos cap√≠tulos vac√≠os\n",
        "\n",
        "\n",
        "# Guardar cantidad de frases con pelea por cap√≠tulo\n",
        "conteo_pelea_por_capitulo = []\n",
        "frases_pelea = []\n",
        "\n",
        "for cap in capitulos:\n",
        "    doc = nlp(cap)\n",
        "    oraciones = [sent.text.strip() for sent in doc.sents]\n",
        "    frases = [frase for frase in oraciones if any(pal in frase.lower() for pal in palabras_pelea)]\n",
        "    frases_pelea.append(frases)  # para verlas despu√©s si quer√©s\n",
        "    conteo_pelea_por_capitulo.append(len(frases))\n",
        "\n",
        "# üìä Graficar\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(conteo_pelea_por_capitulo)+1), conteo_pelea_por_capitulo, marker='o', color='crimson')\n",
        "plt.title(\"N√∫mero de escenas de pelea por cap√≠tulo\")\n",
        "plt.xlabel(\"Cap√≠tulo\")\n",
        "plt.ylabel(\"Cantidad de escenas con palabras clave\")\n",
        "plt.xticks(range(1, len(conteo_pelea_por_capitulo)+1))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qGQNQSJ_-n9i"
      },
      "id": "qGQNQSJ_-n9i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar solo cap√≠tulos con frases de pelea\n",
        "for i, frases in enumerate(frases_pelea, 1):  # empieza en 1\n",
        "    if frases:  # Solo mostrar si hay frases\n",
        "        print(f\"üìò Cap√≠tulo {i}\")\n",
        "        for frase in frases:\n",
        "            print(f\" - {frase}\")\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "A7vDGFaiA7V-"
      },
      "id": "A7vDGFaiA7V-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2 üß† Palabras clave de Muertes.**"
      ],
      "metadata": {
        "id": "u8sgwkN3_QkY"
      },
      "id": "u8sgwkN3_QkY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras clave\n",
        "\n",
        "palabras_muertes = [\"el amor\"]"
      ],
      "metadata": {
        "id": "2dvRPv0n_Wn9"
      },
      "id": "2dvRPv0n_Wn9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Guardar cantidad de frases con muerte por cap√≠tulo\n",
        "conteo_muertes_por_capitulo = []\n",
        "frases_muerte = []\n",
        "\n",
        "for cap in capitulos:\n",
        "    doc = nlp(cap)\n",
        "    oraciones = [sent.text.strip() for sent in doc.sents]\n",
        "    frases = [frase for frase in oraciones if any(pal in frase.lower() for pal in palabras_muertes)]\n",
        "    frases_muerte.append(frases)  # para verlas despu√©s si quer√©s\n",
        "    conteo_muertes_por_capitulo.append(len(frases))\n",
        "\n",
        "# üìä Graficar\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(conteo_muertes_por_capitulo)+1), conteo_muertes_por_capitulo, marker='o', color='crimson')\n",
        "plt.title(\"N√∫mero de escenas de muerte por cap√≠tulo\")\n",
        "plt.xlabel(\"Cap√≠tulo\")\n",
        "plt.ylabel(\"Cantidad de escenas con palabras clave\")\n",
        "plt.xticks(range(1, len(conteo_muertes_por_capitulo)+1))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tYn8Vgrz_jIV"
      },
      "id": "tYn8Vgrz_jIV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar solo cap√≠tulos con frases de pelea\n",
        "for i, frases in enumerate(frases_muerte, 1):  # empieza en 1\n",
        "    if frases:  # Solo mostrar si hay frases\n",
        "        print(f\"üìò Cap√≠tulo {i}\")\n",
        "        for frase in frases:\n",
        "            print(f\" - {frase}\")\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "KLghYpzEBrk8"
      },
      "id": "KLghYpzEBrk8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analisis de Sentimientos por Oraci√≥n**"
      ],
      "metadata": {
        "id": "17rCa48nCk_Z"
      },
      "id": "17rCa48nCk_Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‚úÖ Carga del modelo de sentimientos**"
      ],
      "metadata": {
        "id": "HNjIjrD3CyT9"
      },
      "id": "HNjIjrD3CyT9"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from spacy.lang.es import Spanish\n",
        "\n",
        "# Cargar el modelo de sentimiento en espa√±ol\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "# Cargar el NLP de spaCy para espa√±ol\n",
        "nlp = Spanish()\n",
        "nlp.add_pipe(\"sentencizer\")  # Solo segmenta en oraciones"
      ],
      "metadata": {
        "id": "0Ecpp47GCr3p"
      },
      "id": "0Ecpp47GCr3p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‚úÖ Paso 4: Analizar sentimientos por oraci√≥n**\n"
      ],
      "metadata": {
        "id": "rCdISZN7D3pD"
      },
      "id": "rCdISZN7D3pD"
    },
    {
      "cell_type": "code",
      "source": [
        "sentimientos_por_capitulo = []\n",
        "\n",
        "for oraciones in oraciones:\n",
        "    sentimientos = []\n",
        "    for oracion in oraciones:\n",
        "        try:\n",
        "            result = sentiment_pipeline(oracion[:512])  # l√≠mite de tokens\n",
        "            sentimientos.append({\n",
        "                'oracion': oracion,\n",
        "                'sentimiento': result[0]['label'],\n",
        "                'score': result[0]['score']\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error con oraci√≥n: {oracion}\\n{e}\")\n",
        "    sentimientos_por_capitulo.append(sentimientos)\n"
      ],
      "metadata": {
        "id": "TQOxPmEDDF_1"
      },
      "id": "TQOxPmEDDF_1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‚úÖ  Ver resultados**\n"
      ],
      "metadata": {
        "id": "gm0_7N7jEKQ2"
      },
      "id": "gm0_7N7jEKQ2"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, cap in enumerate(sentimientos_por_capitulo, 1):\n",
        "    print(f\"üìò Cap√≠tulo {i}\")\n",
        "    for s in cap[:5]:  # Las primeras 5 oraciones\n",
        "        print(f\"{s['oracion']} ‚Üí {s['sentimiento']} ({s['score']:.2f})\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "xoA_BThPEH3Q"
      },
      "id": "xoA_BThPEH3Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#///////////////////////////////"
      ],
      "metadata": {
        "id": "WM83mUCbGCf2"
      },
      "id": "WM83mUCbGCf2"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zsvhqb1L4LUm"
      },
      "id": "Zsvhqb1L4LUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "palabras_df = pd.DataFrame(frecuencia, columns=['palabra', 'frecuencia'])\n",
        "sns.barplot(data=palabras_df, x='frecuencia', y='palabra')\n",
        "plt.title(\"Top 10 palabras m√°s frecuentes\")\n",
        "plt.xlabel(\"Frecuencia\")\n",
        "plt.ylabel(\"Palabra\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wXmJS9e8p4Zt"
      },
      "id": "wXmJS9e8p4Zt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ents = [ent.text for ent in doc.ents if ent.label_ in ['PER', 'LOC', 'ORG']]\n",
        "ent_freq = Counter(ents).most_common(10)\n",
        "pd.DataFrame(ent_freq, columns=[\"Entidad\", \"Frecuencia\"]).plot.bar(x='Entidad', y='Frecuencia', legend=False)\n",
        "plt.title(\"Entidades nombradas m√°s frecuentes\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YgUdLNePqIYN"
      },
      "id": "YgUdLNePqIYN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "pos_counts = Counter([token.pos_ for token in doc if token.is_alpha])\n",
        "sns.barplot(x=list(pos_counts.keys()), y=list(pos_counts.values()))\n",
        "plt.title(\"Distribuci√≥n de categor√≠as gramaticales\")\n",
        "plt.xlabel(\"POS\")\n",
        "plt.ylabel(\"Cantidad\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zW9p1JGOqQDO"
      },
      "id": "zW9p1JGOqQDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "G = nx.Graph()\n",
        "for (w1, w2), freq in bigrams_freq:\n",
        "    G.add_edge(w1, w2, weight=freq)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "nx.draw_networkx(G, with_labels=True, node_size=1500, font_size=10)\n",
        "plt.title(\"Bigramas m√°s frecuentes\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HbGTdMf0qx62"
      },
      "id": "HbGTdMf0qx62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Assuming 'tokens_limpios' from previous cell contains the processed text\n",
        "texto_procesado = ' '.join(tokens_filtrados)  # Join the tokens into a string\n",
        "\n",
        "# An√°lisis de sentimientos por p√°rrafo o l√≠nea\n",
        "sentimientos = []\n",
        "for frase in texto_procesado.split('\\n'):\n",
        "    blob = TextBlob(frase)\n",
        "    sentimientos.append((frase, blob.sentiment.polarity))\n",
        "\n",
        "# Mostrar las frases m√°s positivas y m√°s negativas\n",
        "sentimientos_ordenados = sorted(sentimientos, key=lambda x: x[1])\n",
        "print(\"Frase m√°s negativa:\\n\", sentimientos_ordenados[0])\n",
        "print(\"\\nFrase m√°s positiva:\\n\", sentimientos_ordenados[-1])\n"
      ],
      "metadata": {
        "id": "4lJgx5YtrfLM"
      },
      "id": "4lJgx5YtrfLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "sentimientos = []\n",
        "for frase in texto_procesado.split('\\n'):\n",
        "    score = sia.polarity_scores(frase)['compound']\n",
        "    sentimientos.append((frase, score))\n",
        "\n",
        "# Frases con sentimiento m√°s marcado\n",
        "sentimientos_ordenados = sorted(sentimientos, key=lambda x: x[1])\n",
        "print(\"Frase m√°s negativa:\\n\", sentimientos_ordenados[0])\n",
        "print(\"\\nFrase m√°s positiva:\\n\", sentimientos_ordenados[-1])\n"
      ],
      "metadata": {
        "id": "PXIYU6y3r7HO"
      },
      "id": "PXIYU6y3r7HO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "polaridades = [s[1] for s in sentimientos]\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(polaridades)\n",
        "plt.title(\"Evoluci√≥n del sentimiento a lo largo del texto\")\n",
        "plt.xlabel(\"L√≠nea del texto\")\n",
        "plt.ylabel(\"Polaridad (-1 a 1)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ipn8AB--sDBh"
      },
      "id": "ipn8AB--sDBh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch\n"
      ],
      "metadata": {
        "id": "uAXgNCvUsGS5"
      },
      "id": "uAXgNCvUsGS5",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import re\n",
        "from transformers import pipelinefrom collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "# --- 1. Separar el libro por cap√≠tulos ---\n",
        "def separar_capitulos(texto):\n",
        "    # Suponemos que los cap√≠tulos empiezan con \"Cap√≠tulo\", \"CAP√çTULO\", o un n√∫mero solo\n",
        "    caps = re.split(r'\\bCap[i√≠]tulo\\s+\\d+\\b', texto, flags=re.IGNORECASE)\n",
        "    capitulos = [cap.strip() for cap in caps if len(cap.strip()) > 200]  # filtramos los vac√≠os\n",
        "    return capitulos\n",
        "\n",
        "capitulos = separar_capitulos(libro)\n",
        "print(f\"Cantidad de cap√≠tulos encontrados: {len(capitulos)}\")\n",
        "\n",
        "# --- 2. Resumen por cap√≠tulo ---\n",
        "def resumir_texto(texto, num_oraciones=3):\n",
        "    parser = PlaintextParser.from_string(texto, Tokenizer(\"spanish\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    resumen = summarizer(parser.document, num_oraciones)\n",
        "    return \" \".join(str(oracion) for oracion in resumen)\n",
        "\n",
        "resumenes = [resumir_texto(cap) for cap in capitulos]\n",
        "\n",
        "# --- 3. Sentimiento por cap√≠tulo ---\n",
        "sentimientos = []\n",
        "for cap in capitulos:\n",
        "    polaridad, subjetividad = pattern_sentiment(cap) # pattern_sentiment is used\n",
        "    sentimientos.append(polaridad)\n",
        "\n",
        "# --- 4. Buscar menciones de peleas ---\n",
        "palabras_pelea = [\"golpe\", \"disparo\", \"pelea\", \"lucha\", \"patada\", \"pu√±etazo\", \"forcejeo\"]\n",
        "conteo_pelea = []\n",
        "\n",
        "for cap in capitulos:\n",
        "    texto_min = cap.lower()\n",
        "    count = sum(texto_min.count(p) for p in palabras_pelea)\n",
        "    conteo_pelea.append(count)\n",
        "\n",
        "# --- 5. Gr√°fico de sentimientos y peleas ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(sentimientos, marker='o', color='blue')\n",
        "plt.title(\"Sentimiento por cap√≠tulo (polaridad)\")\n",
        "plt.xlabel(\"Cap√≠tulo\")\n",
        "plt.ylabel(\"Polaridad (m√°s positivo ‚Üí m√°s alto)\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(conteo_pelea)), conteo_pelea, color='red')\n",
        "plt.title(\"Cantidad de peleas por cap√≠tulo\")\n",
        "plt.xlabel(\"Cap√≠tulo\")\n",
        "plt.ylabel(\"Menciones relacionadas a pelea\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2jlcn2nErRn3"
      },
      "id": "2jlcn2nErRn3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Cargar modelo en espa√±ol\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Cargar tu texto (puedes modificar para cargar un archivo .txt)\n",
        "with open(\"libro.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    texto = file.read()\n",
        "\n",
        "# Preprocesamiento: pasar a min√∫sculas y quitar saltos de l√≠nea\n",
        "texto = texto.lower().replace('\\n', ' ')\n",
        "\n",
        "# Procesar el texto con SpaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# 1Ô∏è‚É£ Tokenizaci√≥n, Limpieza y Lematizaci√≥n\n",
        "tokens = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
        "lemmas = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "# 2Ô∏è‚É£ POS-tagging\n",
        "pos_tags = [(token.text, token.pos_) for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "# 3Ô∏è‚É£ Bigramas m√°s frecuentes\n",
        "bigrams = list(zip(lemmas, lemmas[1:]))  # o usar tokens si prefer√≠s sin lematizar\n",
        "bigrams_freq = Counter(bigrams).most_common(10)\n",
        "\n",
        "# Imprimir resultados de POS-tagging y bigramas\n",
        "print(\"üî† Tokens limpiados:\", tokens[:10])\n",
        "print(\"\\nüå± Lemmas:\", lemmas[:10])\n",
        "print(\"\\nüßæ POS-tagging:\", pos_tags[:10])\n",
        "print(\"\\nüìõ Bigramas m√°s frecuentes:\", bigrams_freq)\n",
        "\n",
        "# 4Ô∏è‚É£ Resumen por Cap√≠tulos\n",
        "def separar_capitulos(texto):\n",
        "    capitulos = re.split(r'\\n\\d+\\n', texto)  # Asume que los cap√≠tulos empiezan con el n√∫mero en una l√≠nea sola\n",
        "    return capitulos\n",
        "\n",
        "capitulos = separar_capitulos(texto)\n",
        "\n",
        "# Resumir cada cap√≠tulo (usando TextBlob o alguna librer√≠a de resumen)\n",
        "def resumen_capitulo(texto):\n",
        "    blob = TextBlob(texto)\n",
        "    return blob.sentences[:3]  # Resumir las primeras 3 oraciones del cap√≠tulo\n",
        "\n",
        "resumenes = [resumen_capitulo(capitulo) for capitulo in capitulos]\n",
        "\n",
        "# Imprimir los res√∫menes de los primeros 3 cap√≠tulos\n",
        "for i, resumen in enumerate(resumenes[:3]):\n",
        "    print(f\"\\nüìò Resumen del Cap√≠tulo {i+1}:\")\n",
        "    for oracion in resumen:\n",
        "        print(f\" - {oracion}\")\n",
        "\n",
        "# 5Ô∏è‚É£ An√°lisis de Sentimientos por Cap√≠tulo\n",
        "def analisis_sentimientos(texto):\n",
        "    blob = TextBlob(texto)\n",
        "    return blob.sentiment.polarity  # Retorna el valor de polaridad (positivo/negativo)\n",
        "\n",
        "sentimientos = [analisis_sentimientos(capitulo) for capitulo in capitulos]\n",
        "\n",
        "# 6Ô∏è‚É£ Clustering de Cap√≠tulos\n",
        "# Vectorizar el texto con TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='spanish')\n",
        "X = vectorizer.fit_transform(capitulos)\n",
        "\n",
        "# Aplicar KMeans para hacer clustering de cap√≠tulos\n",
        "kmeans = KMeans(n_clusters=5)  # Dividir en 5 clusters (ajustar seg√∫n el libro)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Obtener las etiquetas de cada cap√≠tulo\n",
        "etiquetas = kmeans.labels_\n",
        "\n",
        "# Visualizaci√≥n de Clustering en 2D (usando PCA)\n",
        "pca = PCA(n_components=2)\n",
        "X_reducido = pca.fit_transform(X.toarray())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_reducido[:, 0], X_reducido[:, 1], c=etiquetas, cmap='viridis')\n",
        "plt.title('Clustering de Cap√≠tulos del Libro')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Mostrar los cap√≠tulos asignados a cada cluster\n",
        "for cluster in range(5):\n",
        "    print(f\"\\nCap√≠tulos en el Cluster {cluster}:\")\n",
        "    for i, label in enumerate(etiquetas):\n",
        "        if label == cluster:\n",
        "            print(f\" - Cap√≠tulo {i+1}\")\n"
      ],
      "metadata": {
        "id": "Apgl8niyOgTv"
      },
      "id": "Apgl8niyOgTv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}