{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5AH8MLYjfwgp",
        "VMtpSPyriRkl",
        "ubaprtVKf88y",
        "Eagv-jaJgM_z",
        "D12fghhagRjE",
        "SDKeqnp-gqyc",
        "39XV91NgkM9r",
        "R79sGteNkpVT",
        "_NKoVBUUkw0D",
        "1KM1qtISk-6T",
        "ZhnBUrQTl_BT",
        "ZBXkyZRzmSdj",
        "9bW_hSCRm02p",
        "GNDNz9V5m_EU",
        "GVqmZ6rRoDTd",
        "VCXSZ4glokBF",
        "njpl5RHiosuU",
        "3h4HlXQzpWl0",
        "lZQPvZfZszkC",
        "qD43WO8o5LaU",
        "AvQezV_qx8g4",
        "ELvckxoswXK5"
      ],
      "authorship_tag": "ABX9TyNE0nsR/nj9g102/PYhdIS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leucocitokiller/Proyecto-Fina-NLP/blob/main/Proyecto_final_NLP_Redes_Neuronales_Libenson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importaci칩n de Librer칤as."
      ],
      "metadata": {
        "id": "5AH8MLYjfwgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "#-----librerias para trabajar PLN\n",
        "!python -m spacy download es_core_news_md\n",
        "import spacy\n",
        "import es_core_news_md\n",
        "#es_core_news_md Medium (modelo mediano):\n",
        "#Es m치s pesado y m치s lento que el sm, pero mucho m치s preciso. Tiene vectores de palabras, entiende mejor el significado de las palabras.\n",
        "\n",
        "#-----instalaci칩n d librerias para an치lisis de sentimientos.\n",
        "!pip install spacy spacy-transformers\n",
        "!pip install pysentimiento\n",
        "from pysentimiento import create_analyzer\n",
        "\n",
        "#----librerias para normalizaci칩n de textos\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#----librerias para graficar y wordcloud.\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#----librer칤as para trabajar con TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#----libreria para trabajar con BoW.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#----librerias para Machine learning\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "64jaT-pwfyzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento de la Fuente de Datos."
      ],
      "metadata": {
        "id": "VMtpSPyriRkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conexi칩n con la fuente de datos.\n"
      ],
      "metadata": {
        "id": "ubaprtVKf88y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diccionario con las fuentes y sus URLs\n",
        "filepath_dict = {\n",
        "    'yelp': 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/main/yelp_comentarios.csv',\n",
        "    'amazon': 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/main/amazon_cells_comentarios.csv'\n",
        "\n",
        "}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['Comentario', 'Valor'], sep=';', encoding='latin-1')\n",
        "    df['Origen'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "df.head(1100)"
      ],
      "metadata": {
        "id": "QA3En5EYgAtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eMRxpuFsht3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizaci칩n de la Fuente de datos.\n"
      ],
      "metadata": {
        "id": "Eagv-jaJgM_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminaci칩n de signos de puntuaci칩n."
      ],
      "metadata": {
        "id": "D12fghhagRjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definici칩n de funci칩n para eliminar los signos de puntuaci칩n utilizando re, pero considerando no borrar las vocales con acento.\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Normaliza el texto a NFKD para separar letras y sus tildes\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    # Elimina los caracteres diacr칤ticos (como las tildes)\n",
        "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
        "    # Elimina todo lo que no sea letras, n칰meros o espacios\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Aplicar la funci칩n a la columna 'review_lower'\n",
        "df['Comentarios'] = df['Comentario'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "TlTn8VuygQxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ez4O1zkbhB2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reducir a min칰sculas el texto."
      ],
      "metadata": {
        "id": "SDKeqnp-gqyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'Comentarios_lower' with lowercase values from 'Comentario'\n",
        "df['Comentarios_lower'] = df['Comentarios'].str.lower()"
      ],
      "metadata": {
        "id": "ExcMpIx7gw-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "_O0hOspchMl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertir a n칰mero la columna Valor para su postprocesamiento."
      ],
      "metadata": {
        "id": "jk6_6oiZg5Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos la columna rating a valor num칠rico\n",
        "df['Valor'] = pd.to_numeric(df['Valor'], errors='coerce')"
      ],
      "metadata": {
        "id": "a_urHIsIg7Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Valor']"
      ],
      "metadata": {
        "id": "yUwGYj_jhIud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP"
      ],
      "metadata": {
        "id": "39XV91NgkM9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre Procesamiento."
      ],
      "metadata": {
        "id": "R79sGteNkpVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generaci칩n del objeto de SPacy para utilizar en el procesamiento del texto en espa침ol."
      ],
      "metadata": {
        "id": "_NKoVBUUkw0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = es_core_news_md.load()"
      ],
      "metadata": {
        "id": "aiW_XvTOkm2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertir texto a min칰sculas y Tokenizaci칩n."
      ],
      "metadata": {
        "id": "1KM1qtISk-6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comentarios_tokenizados'] = df['Comentarios_lower'].apply(lambda text: nlp(text))\n",
        "df[['Comentarios_lower','Comentarios_tokenizados']].head()"
      ],
      "metadata": {
        "id": "uVF3ar5olBDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remoci칩n de StopWords"
      ],
      "metadata": {
        "id": "ZhnBUrQTl_BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de palabras que NO queremos eliminar (tienen carga emocional)\n",
        "palabras_sentimiento = {\n",
        "    # Positivas\n",
        "    \"bueno\", \"buena\",\"si\",\"buen칤simo\", \"excelentes\", \"excelente\", \"genial\", \"maravilloso\", \"maravilla\", \"fant치stico\", \"fabuloso\", \"incre칤ble\",\n",
        "    \"perfecto\", \"perfecta\", \"agradable\", \"satisfecho\", \"satisfecha\", \"contento\", \"contenta\", \"encantado\", \"encantada\",\n",
        "    \"amable\", \"simp치tico\", \"simp치tica\", \"r치pido\", \"r치pida\", \"c칩modo\", \"c칩moda\", \"eficaz\", \"eficiente\", \"f치cil\",\n",
        "    \"recomendable\", \"ideal\", \"espectacular\", \"feliz\", \"brillante\", \"cumpli칩\", \"cumple\", \"funciona\", \"funciona bien\",\n",
        "    \"inmejorable\", \"confiable\", \"duradero\", \"cumplidor\", \"seguro\", \"preciso\", \"elegante\", \"atento\", \"responsable\",\n",
        "    \"acertado\", \"destacado\", \"excepcional\", \"impecable\", \"sensacional\", \"칰til\", \"accesible\", \"econ칩mico\", \"funcional\",\n",
        "    \"intuitivo\", \"conveniente\", \"hermoso\", \"linda\", \"precioso\", \"excelente calidad\", \"vale la pena\",\n",
        "\n",
        "    # Negativas\n",
        "    \"malo\",\"no\", \"mala\", \"mal\", \"p칠simo\", \"p칠sima\",\"nunca\", \"horrible\", \"fatal\", \"insoportable\", \"lento\", \"lenta\", \"inc칩modo\", \"inc칩moda\",\n",
        "    \"decepcionante\", \"decepcionado\", \"decepcionada\", \"sucio\", \"sucia\", \"caro\", \"cara\", \"in칰til\", \"deficiente\", \"desagradable\",\n",
        "    \"complicado\", \"problem치tico\", \"estafa\", \"enga침ado\", \"enga침ada\", \"roto\", \"rota\", \"desastroso\", \"error\", \"errores\",\n",
        "    \"retraso\", \"tardanza\", \"fr치gil\", \"inestable\", \"poco fiable\", \"nunca m치s\", \"no volver칠\", \"no recomiendo\", \"no sirve\",\n",
        "    \"no funciona\", \"arruinado\", \"fall칩\", \"fallando\", \"demora\", \"p칠sima atenci칩n\", \"servicio malo\", \"mala calidad\", \"molesto\",\n",
        "    \"defecto\", \"problemas\", \"fallas\", \"sin sentido\", \"basura\", \"p칠rdida de dinero\", \"decepci칩n\"\n",
        "}\n",
        "\n",
        "# Actualizamos spaCy para que NO considere esas palabras como stopwords\n",
        "for palabra in palabras_sentimiento:\n",
        "    lex = nlp.vocab[palabra]\n",
        "    lex.is_stop = False\n",
        "\n",
        "\n",
        "\n",
        "def parse_and_remove_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Remueve las stopwords de un objeto spaCy Doc.\n",
        "    \"\"\"\n",
        "    # Filtrar stopwords y obtener los tokens como texto\n",
        "    tokens_filtrados = [token.text for token in doc if not token.is_stop]\n",
        "    return tokens_filtrados\n",
        "\n",
        "# Aplicar la funci칩n al DataFrame\n",
        "df['Comentarios_sin_StopWords'] = df['Comentarios_tokenizados'].apply(parse_and_remove_stopwords)\n",
        "\n",
        "df[['Comentarios_tokenizados','Comentarios_sin_StopWords']].head()"
      ],
      "metadata": {
        "id": "2_cr__EqmG4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematizado."
      ],
      "metadata": {
        "id": "ZBXkyZRzmSdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizar_sin_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Devuelve una lista de lemas excluyendo las stopwords.\n",
        "\n",
        "    Par치metro:\n",
        "    - doc: objeto spaCy Doc\n",
        "\n",
        "    Retorna:\n",
        "    - Lista de lemas (str) sin stopwords\n",
        "    \"\"\"\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "# Aplicar la funci칩n y guardar el resultado en una nueva columna\n",
        "df['Comentarios_lema'] = df['Comentarios_tokenizados'].apply(lematizar_sin_stopwords)\n",
        "\n",
        "df[['Comentarios_tokenizados','Comentarios_lema']].head(20)"
      ],
      "metadata": {
        "id": "DsRRS29ImVnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento"
      ],
      "metadata": {
        "id": "9bW_hSCRm02p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conteo de Palabras mas comunes."
      ],
      "metadata": {
        "id": "GNDNz9V5m_EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graficar_palabras_comunes(df, origen, top_n=10):\n",
        "    # Filtrar y aplanar los lemas\n",
        "    lemas = [lema for lemas in df[df['Origen'] == origen]['Comentarios_lema'] for lema in lemas]\n",
        "    conteo = Counter(lemas).most_common(top_n)\n",
        "\n",
        "    # Separar palabras y frecuencias\n",
        "    palabras, frecuencias = zip(*conteo)\n",
        "\n",
        "    # Crear gr치fico\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(palabras, frecuencias, color='skyblue')\n",
        "    plt.xlabel('Frecuencia')\n",
        "    plt.title(f'Top {top_n} Palabras M치s Comunes - {origen.capitalize()}')\n",
        "    plt.gca().invert_yaxis()  # Poner la palabra m치s com칰n arriba\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Graficar para Yelp\n",
        "graficar_palabras_comunes(df, 'yelp')\n",
        "\n",
        "# Graficar para Amazon\n",
        "graficar_palabras_comunes(df, 'amazon')"
      ],
      "metadata": {
        "id": "cHPKUNjjoJQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conteo de bigramas m치s comunes."
      ],
      "metadata": {
        "id": "GVqmZ6rRoDTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import tee\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generar_bigramas(lista):\n",
        "    \"\"\"Devuelve bigramas como tuplas a partir de una lista de palabras\"\"\"\n",
        "    a, b = tee(lista)\n",
        "    next(b, None)\n",
        "    return list(zip(a, b))\n",
        "\n",
        "def graficar_bigramas_comunes(df, origen, top_n=10):\n",
        "    # Filtrar solo los comentarios del origen y generar bigramas\n",
        "    bigramas = [\n",
        "        bigrama\n",
        "        for lemas in df[df['Origen'] == origen]['Comentarios_lema']\n",
        "        for bigrama in generar_bigramas(lemas)\n",
        "    ]\n",
        "\n",
        "    conteo = Counter(bigramas).most_common(top_n)\n",
        "\n",
        "    # Convertir tuplas de bigramas a string para graficar\n",
        "    etiquetas = [' '.join(b) for b, _ in conteo]\n",
        "    frecuencias = [f for _, f in conteo]\n",
        "\n",
        "    # Crear gr치fico\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(etiquetas, frecuencias, color='mediumseagreen')\n",
        "    plt.xlabel('Frecuencia')\n",
        "    plt.title(f'Top {top_n} Bigramas M치s Comunes - {origen.capitalize()}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Ejemplo de uso\n",
        "graficar_bigramas_comunes(df, 'yelp')\n",
        "graficar_bigramas_comunes(df, 'amazon')\n"
      ],
      "metadata": {
        "id": "_B3v3nOknL2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordClouds"
      ],
      "metadata": {
        "id": "tn6x4GaEoYRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WordCloud Yelp."
      ],
      "metadata": {
        "id": "VCXSZ4glokBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar el DataFrame\n",
        "df_yelp = df[df['Origen'] == 'yelp']\n",
        "\n",
        "# Unir todos los lemas en un solo string (comentarios lematizados ya est치n en listas)\n",
        "texto_yelp = ' '.join([' '.join(lemas) for lemas in df_yelp['Comentarios_lema']])\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_yelp)\n",
        "\n",
        "# Mostrar la nube\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios de Yelp\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UXGDKZCaocLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WordCloud Amazon."
      ],
      "metadata": {
        "id": "njpl5RHiosuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar el DataFrame\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Unir todos los lemas en un solo string (comentarios lematizados ya est치n en listas)\n",
        "texto_amazon = ' '.join([' '.join(lemas) for lemas in df_amazon['Comentarios_lema']])\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_amazon)\n",
        "\n",
        "# Mostrar la nube\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios de Yelp\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PHfxMdccou0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WordCloud + Bigramas."
      ],
      "metadata": {
        "id": "_ryxqKXUo6Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_bigramas_spacy(df, origen, top_n=50):\n",
        "    \"\"\"\n",
        "    Genera bigramas usando spaCy a partir de la columna 'Comentarios_Lema', sin stopwords.\n",
        "    Luego genera una nube de palabras.\n",
        "    \"\"\"\n",
        "    # Filtrar los comentarios por 'origen' (por ejemplo, 'yelp' o 'amazon')\n",
        "    comentarios = df[df['Origen'] == origen]['Comentarios_sin_StopWords']\n",
        "\n",
        "    # Generar bigramas\n",
        "    bigramas = []\n",
        "    for comentario in comentarios:\n",
        "        # Crear un Doc de spaCy a partir de la lista de lemas (de la columna 'Comentarios_Lema')\n",
        "        doc = nlp(' '.join(comentario))  # Unimos la lista de lemas y lo procesamos con spaCy\n",
        "        # Extraer bigramas\n",
        "        for i in range(len(doc) - 1):\n",
        "            if not doc[i].is_stop and not doc[i+1].is_stop:  # Asegurarse de que no sean stopwords\n",
        "                bigramas.append((doc[i].lemma_, doc[i+1].lemma_))\n",
        "\n",
        "    # Contar los bigramas m치s comunes\n",
        "    conteo_bigramas = Counter(bigramas).most_common(top_n)\n",
        "\n",
        "    # Convertir los bigramas a formato texto \"palabra1 palabra2\"\n",
        "    bigramas_texto = {' '.join(bigrama): freq for bigrama, freq in conteo_bigramas}\n",
        "\n",
        "    # Generar la nube de palabras de los bigramas\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigramas_texto)\n",
        "\n",
        "    # Mostrar la nube\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud de Bigramas - {origen.capitalize()}\")\n",
        "    plt.show()\n",
        "\n",
        "# Generar la nube de bigramas para Yelp y Amazon\n",
        "generar_bigramas_spacy(df, 'yelp')\n",
        "generar_bigramas_spacy(df, 'amazon')"
      ],
      "metadata": {
        "id": "qW644mkPo8qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 游 An치lisis de sentimiento en espa침ol con pysentimiento"
      ],
      "metadata": {
        "id": "3h4HlXQzpWl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear analizador de sentimientos\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "# Aplicar a una columna de texto\n",
        "df['Sentimiento'] = df['Comentario'].apply(lambda x: analyzer.predict(x).output)\n",
        "# Sentimiento solo guarda lo predicho (POS, NEU o NEG)\n",
        "\n",
        "df['Probabilidad'] = df['Comentario'].apply(lambda x: analyzer.predict(x).probas)\n",
        "#Ese diccionario contiene la probabilidad de cada clase: positivo, neutro, negativo Ejemplo: {'POS': 0.84, 'NEU': 0.10, 'NEG': 0.06}."
      ],
      "metadata": {
        "id": "r5feGF2Gpias"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 游늵  Gr치fico de barras de frecuencia de sentimientos"
      ],
      "metadata": {
        "id": "lZQPvZfZszkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribuci칩n de Sentimientos totales entre Yelp y Amazon')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ddaD7cous3SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distribuci칩n de los sentimientos."
      ],
      "metadata": {
        "id": "NyE94c4JtDWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentimiento'].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    labels=['Positivo', 'Neutro', 'Negativo'],\n",
        "    colors=['lightgreen', 'lightblue', 'salmon']\n",
        ")\n",
        "plt.title('Distribuci칩n porcentual de Sentimientos')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dI2qGwfVs837"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### An치lisis de Confianza para filtrar comentarios con baja certeza"
      ],
      "metadata": {
        "id": "N9Ehqc_htP8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# M치xima probabilidad (nivel de certeza del modelo)\n",
        "df['Confianza'] = df['Probabilidad'].apply(lambda x: max(x.values()))  # En este caso, de la lista {'POS': 0.84, 'NEU': 0.10, 'NEG': 0.06} s칩lo guarda 0.84 que es el valor mayor\n",
        "\n",
        "# Filtrar comentarios cuya confianza sea menor a 0.6\n",
        "comentarios_baja_confianza = df[df['Confianza'] < 0.6]\n",
        "comentarios_alta_confianza = df[df['Confianza'] >= 0.6]\n",
        "# Ver los primeros resultados\n",
        "#comentarios_baja_confianza[['Comentarios', 'Sentimiento', 'Confianza']]"
      ],
      "metadata": {
        "id": "cp_TPbgYtUPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distribuci칩n de los comentarios filtrados."
      ],
      "metadata": {
        "id": "Np0Wbkh-t9ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Confianza'] >= 0.6]['Sentimiento'].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    labels=['Positivo', 'Neutro', 'Negativo'],\n",
        "    colors=['lightgreen', 'lightblue', 'salmon']\n",
        ")\n",
        "plt.title('Distribuci칩n porcentual de Sentimientos con alta confianza')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DKGk25nltsFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distribuci칩n de la Confianza de los comentarios."
      ],
      "metadata": {
        "id": "8UwgeOBbuhWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x='Confianza', bins=20, kde=True, color='skyblue')\n",
        "plt.axvline(0.6, color='red', linestyle='--', label='Umbral 0.6')\n",
        "plt.title('Distribuci칩n de Confianza del Sentimiento')\n",
        "plt.xlabel('Confianza')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W1mX2p80uabR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruebas de Modelos de Machine Learning."
      ],
      "metadata": {
        "id": "vk7QPjtIv4rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Divisi칩n de datos de entrenamiento y prueba."
      ],
      "metadata": {
        "id": "qD43WO8o5LaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['Comentario'], df['Valor'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "jkK0y7oy5Z_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi칩n Log칤stica."
      ],
      "metadata": {
        "id": "2189jWXqwNhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizando TF-IFD."
      ],
      "metadata": {
        "id": "JBBoS9QowA6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency - Inverse Document Frequency) es una t칠cnica de procesamiento de texto utilizada para evaluar la importancia de una palabra dentro de un conjunto de documentos. Se basa en dos conceptos:\n",
        "\n",
        "TF (Frecuencia de T칠rmino): Mide cu치ntas veces aparece un t칠rmino en un documento espec칤fico, comparado con el n칰mero total de t칠rminos en ese documento. Esto ayuda a capturar cu치n relevante es una palabra dentro de un documento en particular.\n",
        "\n",
        "IDF (Frecuencia Inversa de Documentos): Mide la importancia de una palabra dentro de un conjunto de documentos. Si una palabra aparece en muchos documentos, tiene menos valor. La f칩rmula es:\n",
        "\n",
        "Esto ayuda a reducir el peso de las palabras que aparecen frecuentemente en todos los documentos (como \"el\", \"y\", \"de\"), ya que no agregan mucha informaci칩n.\n",
        "\n",
        "As칤, la importancia de un t칠rmino en un documento depende tanto de su frecuencia en ese documento como de cu치n com칰n es en todo el conjunto de documentos."
      ],
      "metadata": {
        "id": "Z5Qu0o3RwyoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C치lculo de TF-IDF con TfidVetorizer."
      ],
      "metadata": {
        "id": "AvQezV_qx8g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # TFIDF espera trabajar con strings y  no listas, por lo que se procede a crear una nueva columna con los datos tokenizados en formato str.\n",
        "df['Comentarios_sin_StopWords_str'] = df['Comentarios_sin_StopWords'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Crear el vectorizador\n",
        "tfidfvectorizer = TfidfVectorizer(ngram_range=(1,5))\n",
        "#inlcuyo bigramas y trigramas para que le de contexto a los comentarios. Esto me permite ver un \"No conforme\" y no solamente le \"No\" y el \"Conforme\" por separado.\n",
        "\n",
        "# Ajustar y transformar\n",
        "tfidf_matrix = tfidfvectorizer.fit_transform(df['Comentarios_sin_StopWords_str'])\n",
        "\n",
        "# Obtener los t칠rminos\n",
        "features = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "# Convertir la matriz a DataFrame\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=features)\n",
        "\n",
        "# Sumar TF-IDF por columna\n",
        "tfidf_scores = df_tfidf.sum().sort_values(ascending=False)\n",
        "\n",
        "# Mostrar top 10\n",
        "print(\"游댛 Top 10 n-gramas por score TF-IDF:\")\n",
        "print(tfidf_scores.head(10).round(3))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yOA1KulbyJG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame auxiliar con el tipo de n-grama\n",
        "df_scores = pd.DataFrame({\n",
        "    'ngram': tfidf_scores.index,\n",
        "    'score': tfidf_scores.values,\n",
        "    'tipo': tfidf_scores.index.to_series().apply(lambda x: f'{len(x.split())}-grama')\n",
        "})\n",
        "\n",
        "# Ver los 5 m치s importantes por tipo\n",
        "top_n = 5\n",
        "for tipo in ['1-grama', '2-grama', '3-grama']:\n",
        "    print(f\"\\n游댛 Top {top_n} {tipo}s:\")\n",
        "    print(df_scores[df_scores['tipo'] == tipo].head(top_n).to_string(index=False))"
      ],
      "metadata": {
        "id": "8TncVynMzZlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b4VhVp3p41Q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ajuste de datos de Entrenamiento."
      ],
      "metadata": {
        "id": "jGzhxUtp6DAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar y transformar los datos de entrenamiento\n",
        "X_train_tfidf = tfidfvectorizer.fit_transform(X_train)\n",
        "# Transformar los datos de prueba\n",
        "X_test_tfidf = tfidfvectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "C9X3Y3tX6JQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generaci칩n y prueba de modelo Regresi칩n Log칤stica."
      ],
      "metadata": {
        "id": "4zXnyKl47GSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un modelo de regresi칩n log칤stica\n",
        "# Abajo ten칠s un c칩digo con los par치metros expresados de forma que puedas ir modificandolos\n",
        "model_log_reg = LogisticRegression() #Instanciamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model_log_reg.fit(X_train_tfidf, y_train) # Fiteamos, es decir, el modelo aprende a partir de los datos de entrenamiento\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred_log_reg = model_log_reg.predict(X_test_tfidf) # Predecir"
      ],
      "metadata": {
        "id": "lmGzeGnU7NlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluaci칩n del Modelo."
      ],
      "metadata": {
        "id": "BKMGMFHX8OKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriz de Confusi칩n."
      ],
      "metadata": {
        "id": "i8zfSea79_Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Matriz de confusi칩n\n",
        "\n",
        "# La Matriz de Confusi칩n es 칰til para Muestra los aciertos y errores del modelo organizados por clase.\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
        "labels = ['Negativo', 'Positivo']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicci칩n')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusi칩n')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SZu-1myh8k3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curva ROC AUC."
      ],
      "metadata": {
        "id": "YAkrMHlM-DEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Curva ROC\n",
        "# ROC AUC SCORE eval칰a qu칠 tan bien el modelo separa las clases.\n",
        "fpr, tpr, _ = roc_curve(y_test, model_log_reg.decision_function(X_test_tfidf))\n",
        "roc_auc = roc_auc_score(y_test, model_log_reg.decision_function(X_test_tfidf))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'츼rea bajo la curva ROC AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o6Iu7ehz9ZCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "M칠tricas de Predicci칩n."
      ],
      "metadata": {
        "id": "Tzf7JNqq-FnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. M칠tricas\n",
        "# Accuracy:Para medir qu칠 tan bien predice el modelo en datos nuevos (exactitud).\n",
        "# Accuracy mide el porcentaje total de predicciones correctas sobre el total de casos.\n",
        "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
        "# Precision: Para medir el costo de un falso positivo es alto (por ejemplo, recomendar una pel칤cula mala como buena).\n",
        "# Precision mide qu칠 proporci칩n de las predicciones positivas hechas por el modelo son realmente positivas.\n",
        "precision = precision_score(y_test, y_pred_log_reg)\n",
        "# Recall: Para medir cu치ntos de los casos positivos reales fueron capturados por el modelo.\n",
        "recall = recall_score(y_test, y_pred_log_reg)\n",
        "# f1 Score: Para medir el promedio arm칩nico entre precisi칩n y recall. Un buen balance si ambas cosas son importantes.\n",
        "f1 = f1_score(y_test, y_pred_log_reg)\n",
        "\n",
        "print(\"M칠tricas de desempe침o del modelo:\")\n",
        "print(f\"Accuracy : {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall   : {recall:.2f}\")\n",
        "print(f\"F1 Score : {f1:.2f}\")"
      ],
      "metadata": {
        "id": "yhq9A09B9Y2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validaci칩n Cruzada."
      ],
      "metadata": {
        "id": "YhVd_UpcIthW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline que junta vectorizador y modelo\n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(max_features=5000),\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "# Validaci칩n cruzada con 5 particiones (k-fold = 5)\n",
        "scores = cross_val_score(pipeline, df['Comentario'], df['Valor'], cv=5, scoring='accuracy')\n",
        "\n",
        "# Resultados\n",
        "print(f\"Precisi칩n media con validaci칩n cruzada: {scores.mean():.3f}\")\n",
        "print(f\"Desviaci칩n est치ndar: {scores.std():.3f}\")"
      ],
      "metadata": {
        "id": "I1TpkwSyI8OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizaci칩n de palabras asociadas a rese침as positivas y negativas."
      ],
      "metadata": {
        "id": "voZXIBS9QDz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos las palabras del vocabulario\n",
        "palabras = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "# Coeficientes del modelo (uno por palabra)\n",
        "coeficientes = model_log_reg.coef_[0]\n",
        "\n",
        "# Creamos un DataFrame para visualizarlo\n",
        "df_coef = pd.DataFrame({'palabra': palabras, 'coeficiente': coeficientes})\n",
        "\n",
        "# Ordenamos por importancia\n",
        "df_coef = df_coef.sort_values(by='coeficiente', ascending=False)\n",
        "\n",
        "# En la primera columna veremos el n칰mero \"칤ndice\" de cada palabra seg칰n el 칩rden en que fueron procesadas en el modelo.\n",
        "\n",
        "# Mostramos las 10 palabras m치s asociadas a valoraci칩n positiva y negativa\n",
        "print(\"游댶 Palabras m치s asociadas a rese침as positivas:\")\n",
        "print(df_coef.head(10))\n",
        "\n",
        "print(\"\\n游댷 Palabras m치s asociadas a rese침as negativas:\")\n",
        "print(df_coef.tail(10))"
      ],
      "metadata": {
        "id": "2bJEOJTHQKb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficar las 10 palabras m치s positivas y m치s negativas"
      ],
      "metadata": {
        "id": "6inidvU-R88J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colores pastel suaves (m치s apagados)\n",
        "color_positivas = '#388e3c'  # verde claro apagado\n",
        "color_negativas = '#e65100'  # rojo claro apagado\n",
        "\n",
        "# Top 10 positivas y negativas\n",
        "top_positivas = df_coef.head(10)\n",
        "top_negativas = df_coef.tail(10).sort_values(by='coeficiente')\n",
        "\n",
        "# Crear la figura y los ejes\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Gr치fico de palabras positivas\n",
        "bars1 = ax[0].barh(top_positivas['palabra'], top_positivas['coeficiente'], color=color_positivas)\n",
        "ax[0].set_title('游댶 Palabras asociadas a rese침as positivas', fontsize=14)\n",
        "ax[0].invert_yaxis()\n",
        "ax[0].set_xlabel('Coeficiente', fontsize=12)\n",
        "\n",
        "# Agregar valores al final de las barras (positivas)\n",
        "for bar in bars1:\n",
        "    width = bar.get_width()\n",
        "    ax[0].text(width + 0.01, bar.get_y() + bar.get_height() / 2,\n",
        "               f'{width:.2f}', va='center', fontsize=10)\n",
        "\n",
        "# Gr치fico de palabras negativas\n",
        "bars2 = ax[1].barh(top_negativas['palabra'], top_negativas['coeficiente'], color=color_negativas)\n",
        "ax[1].set_title('游댷 Palabras asociadas a rese침as negativas', fontsize=14)\n",
        "ax[1].invert_yaxis()\n",
        "ax[1].set_xlabel('Coeficiente', fontsize=12)\n",
        "\n",
        "# Agregar valores al final de las barras (negativas)\n",
        "for bar in bars2:\n",
        "    width = bar.get_width()\n",
        "    ax[1].text(width - 0.01, bar.get_y() + bar.get_height() / 2,\n",
        "               f'{width:.2f}', va='center', ha='right', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VnNJCbVbRLMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar bigramas y trigramas\n",
        "# Cambiar 'ngrama' a 'palabra' para acceder a la columna correcta\n",
        "df_bi_tri = df_coef[df_coef['palabra'].str.count(' ') >= 2]\n",
        "\n",
        "top_pos_bi_tri = df_bi_tri.sort_values('coeficiente', ascending=False).head(10)\n",
        "top_neg_bi_tri = df_bi_tri.sort_values('coeficiente', ascending=True).head(10)\n",
        "\n",
        "color_positivas = '#388e3c'  # verde oscuro\n",
        "color_negativas = '#e65100'  # naranja oscuro\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
        "\n",
        "axes[0].barh(top_pos_bi_tri['palabra'], top_pos_bi_tri['coeficiente'], color=color_positivas) # Cambiar 'ngrama' a 'palabra'\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].set_title('游댶 Bigrams y Trigrams positivos')\n",
        "axes[0].set_xlabel('Coeficiente')\n",
        "\n",
        "axes[1].barh(top_neg_bi_tri['palabra'], top_neg_bi_tri['coeficiente'], color=color_negativas) # Cambiar 'ngrama' a 'palabra'\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].set_title('游댷 Bigrams y Trigrams negativos')\n",
        "axes[1].set_xlabel('Coeficiente')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VgoYFK0eTcBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "游 쯈u칠 muestran los gr치ficos?\n",
        "Las palabras con coeficientes positivos son las que m치s contribuyen a que el modelo prediga una rese침a positiva.\n",
        "\n",
        "Las palabras con coeficientes negativos son las que m치s empujan al modelo hacia una predicci칩n negativa."
      ],
      "metadata": {
        "id": "11CjMM0lR28T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba del modelo."
      ],
      "metadata": {
        "id": "bbw6-1oWJYA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nueva_rese침a = \"no lo recominedo\"  # Reemplaza con la rese침a que deseas probar\n",
        "nueva_rese침a_tfidf = tfidfvectorizer.transform([nueva_rese침a])\n",
        "prediccion = model_log_reg.predict(nueva_rese침a_tfidf)\n",
        "# Obtener la probabilidad de la predicci칩n\n",
        "probabilidadpositiva = model_log_reg.predict_proba(nueva_rese침a_tfidf)\n",
        "\n",
        "# Obtener la probabilidad en la clase predicha (0 o 1)\n",
        "probabilidad = probabilidadpositiva[0][1]  # Probabilidad de la clase \"positivo\"\n",
        "\n",
        "print(f\"Se predice que la cr칤tica es de caracter {prediccion[0]}\")\n",
        "print(f\" con una probabilidad de que sea positiva de {probabilidad:.2f}\")"
      ],
      "metadata": {
        "id": "W8rk_rn_JdGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizando Bag of Words."
      ],
      "metadata": {
        "id": "6nA1NdjV-tYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW convierte un conjunto de documentos en una matriz de ocurrencias de palabras. A diferencia de TF-IDF, que pondera las palabras seg칰n su frecuencia e importancia en relaci칩n con todo el corpus, BoW solo cuenta cu치ntas veces aparece una palabra en un documento sin considerar la frecuencia global de la palabra."
      ],
      "metadata": {
        "id": "4N3SqoNl-9SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ajuste de datos de entrenamiento para BoW."
      ],
      "metadata": {
        "id": "9O22cegICxIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciamos el vectorizador BoW\n",
        "vectorizer_bow = CountVectorizer()\n",
        "\n",
        "# Aplicamos el vectorizador a los comentarios lematizados (ahora en formato string)\n",
        "vector_bow = vectorizer_bow.fit_transform(df['Comentarios_sin_StopWords_str'])\n",
        "\n",
        "# Convertimos la matriz de caracter칤sticas en un DataFrame para visualizar\n",
        "bow_df = pd.DataFrame(vector_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
        "\n",
        "# Obtener los nombres de las caracter칤sticas (palabras)\n",
        "features_bow = vectorizer_bow.get_feature_names_out()\n",
        "\n",
        "\n",
        "# Crear un DataFrame con las frecuencias de las palabras\n",
        "df_bow = pd.DataFrame(vector_bow.toarray(), columns=features_bow)"
      ],
      "metadata": {
        "id": "puoPyVHMDFOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar y transformar los datos de entrenamiento\n",
        "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
        "# Transformar los datos de prueba\n",
        "X_test_bow  = X_test_bow  = vectorizer_bow.transform(X_test)"
      ],
      "metadata": {
        "id": "KuBThnFkC0eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generaci칩n y prueba del Modelo con BoW."
      ],
      "metadata": {
        "id": "U-LmgSjwHBdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un modelo de regresi칩n log칤stica\n",
        "# Abajo ten칠s un c칩digo con los par치metros expresados de forma que puedas ir modificandolos\n",
        "model_log_reg_Bow = LogisticRegression() #Instanciamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model_log_reg_Bow.fit(X_train_bow, y_train) # Fiteamos, es decir, el modelo aprende a partir de los datos de entrenamiento\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred_log_reg_Bow = model_log_reg_Bow.predict(X_test_bow) # Predecir"
      ],
      "metadata": {
        "id": "qGBcMOnvHAle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluaci칩n del Modelo.\n"
      ],
      "metadata": {
        "id": "PSo23e2IHX2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriz de Confusi칩n."
      ],
      "metadata": {
        "id": "LxPQYjq3HhL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Matriz de confusi칩n\n",
        "\n",
        "# La Matriz de Confusi칩n es 칰til para Muestra los aciertos y errores del modelo organizados por clase.\n",
        "\n",
        "cm1 = confusion_matrix(y_test, y_pred_log_reg_Bow)\n",
        "labels = ['Negativo', 'Positivo']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicci칩n')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusi칩nc con BoW')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9PBCupjHi8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curva ROC AUC."
      ],
      "metadata": {
        "id": "tlGSMfUAH1D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Curva ROC\n",
        "# ROC AUC SCORE eval칰a qu칠 tan bien el modelo separa las clases.\n",
        "fpr1, tpr1, _ = roc_curve(y_test, model_log_reg_Bow.decision_function(X_test_bow))\n",
        "roc_auc1 = roc_auc_score(y_test, model_log_reg_Bow.decision_function(X_test_bow))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr1, tpr1, color='darkorange', lw=2, label=f'츼rea bajo la curva ROC AUC = {roc_auc1:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HONvbjo4HzuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "M칠tricas de Predicci칩n."
      ],
      "metadata": {
        "id": "vW0kWKhaIHmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. M칠tricas\n",
        "# Accuracy:Para medir qu칠 tan bien predice el modelo en datos nuevos (exactitud).\n",
        "# Accuracy mide el porcentaje total de predicciones correctas sobre el total de casos.\n",
        "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
        "# Precision: Para medir el costo de un falso positivo es alto (por ejemplo, recomendar una pel칤cula mala como buena).\n",
        "# Precision mide qu칠 proporci칩n de las predicciones positivas hechas por el modelo son realmente positivas.\n",
        "precision = precision_score(y_test, y_pred_log_reg)\n",
        "# Recall: Para medir cu치ntos de los casos positivos reales fueron capturados por el modelo.\n",
        "recall = recall_score(y_test, y_pred_log_reg)\n",
        "# f1 Score: Para medir el promedio arm칩nico entre precisi칩n y recall. Un buen balance si ambas cosas son importantes.\n",
        "f1 = f1_score(y_test, y_pred_log_reg)\n",
        "\n",
        "print(\"M칠tricas de desempe침o del modelo:\")\n",
        "print(f\"Accuracy : {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall   : {recall:.2f}\")\n",
        "print(f\"F1 Score : {f1:.2f}\")"
      ],
      "metadata": {
        "id": "I2xeBzIoIL4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validaci칩n Cruzada con BoW."
      ],
      "metadata": {
        "id": "NFXHSA1wO2U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline con Bag of Words y Regresi칩n Log칤stica\n",
        "pipeline1 = make_pipeline(\n",
        "    CountVectorizer(max_features=5000),  # Bag of Words\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "# Validaci칩n cruzada con 5 particiones\n",
        "scores = cross_val_score(pipeline1, df['Comentario'], df['Valor'], cv=5, scoring='accuracy')\n",
        "\n",
        "# Resultados\n",
        "print(f\"Precisi칩n media con validaci칩n cruzada (BoW): {scores.mean():.3f}\")\n",
        "print(f\"Desviaci칩n est치ndar: {scores.std():.3f}\")"
      ],
      "metadata": {
        "id": "vjenYlA_O4eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redes Neuronales."
      ],
      "metadata": {
        "id": "ELvckxoswXK5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qAfjlnIjyIjA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}