{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iynTgO2qzJTa",
        "5ZXBeSKazPVh",
        "DqzsZE2ozTtx",
        "5AH8MLYjfwgp",
        "VMtpSPyriRkl",
        "ubaprtVKf88y",
        "D12fghhagRjE",
        "SDKeqnp-gqyc",
        "jk6_6oiZg5Dl",
        "R79sGteNkpVT",
        "_NKoVBUUkw0D",
        "1KM1qtISk-6T",
        "ZhnBUrQTl_BT",
        "ZBXkyZRzmSdj",
        "GNDNz9V5m_EU",
        "GVqmZ6rRoDTd",
        "njpl5RHiosuU",
        "qD43WO8o5LaU",
        "2189jWXqwNhA",
        "AvQezV_qx8g4",
        "jGzhxUtp6DAD",
        "4zXnyKl47GSz",
        "BKMGMFHX8OKc",
        "6nA1NdjV-tYv",
        "ELvckxoswXK5",
        "8XANfxe7IFA5"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOhXnchlsp8uyAW/kQUylV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leucocitokiller/Proyecto-Fina-NLP/blob/main/Proyecto_final_NLP_Redes_Neuronales_Libenson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Data Science III: NLP & Deep Learning aplicado a Ciencia de Datos*\n",
        "# *Proyecto Final*\n",
        "## *Alumno: Gabriel Libenson*\n",
        "## *Comisi√≥n: 61730*\n"
      ],
      "metadata": {
        "id": "zhg8nb6M37rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducci√≥n.**"
      ],
      "metadata": {
        "id": "iynTgO2qzJTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este trabajo pr√°ctico se aborda el an√°lisis y clasificaci√≥n de opiniones de usuarios mediante t√©cnicas de Procesamiento de Lenguaje Natural (NLP) y Machine Learning. Se emplean dos conjuntos de datos distintos, provenientes de plataformas reconocidas: Yelp, que contiene rese√±as de locales de comida, y Amazon, que incluye comentarios sobre productos."
      ],
      "metadata": {
        "id": "4_kaSM_zJAqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objetivos.**"
      ],
      "metadata": {
        "id": "5ZXBeSKazPVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo principal es desarrollar un modelo capaz de identificar autom√°ticamente si un comentario es positivo o negativo, independientemente de la tem√°tica o sector al que pertenezca. Para ello, se aplican diferentes herramientas y t√©cnicas propias del NLP, tales como el tokenizado, lemmatizaci√≥n, entre otras, que permiten transformar los textos en formatos adecuados para su an√°lisis computacional.\n",
        "\n",
        "Posteriormente, se prueba una variedad de modelos de machine learning para evaluar cu√°l es el m√°s efectivo en la clasificaci√≥n de sentimientos en ambos datasets. Esto incluye desde modelos cl√°sicos hasta t√©cnicas m√°s avanzadas, buscando generalizar el aprendizaje para que el modelo pueda detectar la polaridad del comentario m√°s all√° del contexto espec√≠fico.\n",
        "\n",
        "Este enfoque facilita no solo el entendimiento de las opiniones expresadas por los usuarios, sino que tambi√©n permite desarrollar sistemas automatizados de an√°lisis de sentimientos √∫tiles en distintas aplicaciones comerciales y de investigaci√≥n."
      ],
      "metadata": {
        "id": "ZCSpjSaCJLL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Origen de los datos.**"
      ],
      "metadata": {
        "id": "DqzsZE2ozTtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los datos pertenecen a una adaptaci√≥n de los comentarios de yelp y amazon; fueron obtenidos del siguiente link de Github:\n",
        "\n",
        "https://github.com/luisFernandoCastellanosG/Machine_learning/blob/master/2-Deep_Learning/PLN/Datasets/DataSetOpiniones.zip\n",
        "\n",
        "Datos del autor:\n",
        "\n",
        "https://github.com/luisFernandoCastellanosG/Machine_learning/blob/master/2-Deep_Learning/PLN/Datasets/readme.md\n",
        "\n"
      ],
      "metadata": {
        "id": "tXedftW6L9Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Desarrollo.**"
      ],
      "metadata": {
        "id": "UfJPcx_uzjiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importaci√≥n de Librer√≠as.**"
      ],
      "metadata": {
        "id": "5AH8MLYjfwgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "#-----librerias para trabajar NLP\n",
        "!python -m spacy download es_core_news_md\n",
        "import spacy\n",
        "import es_core_news_md\n",
        "#es_core_news_md Medium (modelo mediano):\n",
        "#Es m√°s pesado y m√°s lento que el sm, pero mucho m√°s preciso. Tiene vectores de palabras, entiende mejor el significado de las palabras.\n",
        "\n",
        "#-----instalaci√≥n d librerias para an√°lisis de sentimientos.\n",
        "!pip install spacy spacy-transformers\n",
        "!pip install pysentimiento\n",
        "from pysentimiento import create_analyzer\n",
        "\n",
        "#----librerias para normalizaci√≥n de textos\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#----librerias para graficar y wordcloud.\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#----librer√≠as para trabajar con TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#----libreria para trabajar con BoW.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#----librerias para Machine learning\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
        "#----librerias de Redes Neuronales.\n",
        "# Importamos el Tokenizer para procesar el texto y convertirlo en secuencias num√©ricas\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# Importamos la funci√≥n para rellenar las secuencias con ceros y asegurarnos que todas tengan la misma longitud\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Importamos el modelo secuencial de Keras, que permite apilar capas de manera lineal\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "!pip install keras-tuner\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.regularizers import l2\n",
        "# Importamos las capas necesarias:\n",
        "# - Embedding: para convertir √≠ndices de palabras en vectores densos.\n",
        "# - SimpleRNN: una capa recurrente que procesa secuencias de datos.\n",
        "# - Dense: una capa totalmente conectada, utilizada para la salida del modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "64jaT-pwfyzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Procesamiento de la Fuente de Datos.**"
      ],
      "metadata": {
        "id": "VMtpSPyriRkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conexi√≥n con la fuente de datos.\n"
      ],
      "metadata": {
        "id": "ubaprtVKf88y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se cargan dos dataset desde Github que contienen comentarios sobre celulares (Amanzon) y servicio de restaurantes (Yelp). Ambos dataset se unifican para tener un mayor volumen de datos para analizar.\n",
        "\n",
        "Los mismos estan compuestos por dos columnas, una con los comentarios de cada usario registrado y otra con el valor asignado a ese comentario.\n",
        "Si el comentario tiene un valor 1 se lo considera positivo y si tiene valor 2 como negativo."
      ],
      "metadata": {
        "id": "KU4pkuDmqq--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diccionario con las fuentes y sus URLs\n",
        "filepath_dict = {\n",
        "    'yelp': 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/main/yelp_comentarios.csv',\n",
        "    'amazon': 'https://raw.githubusercontent.com/Leucocitokiller/Proyecto-Fina-NLP/main/amazon_cells_comentarios.csv'\n",
        "\n",
        "}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['Comentario', 'Valor'], sep=';', encoding='latin-1')\n",
        "    df['Origen'] = source  # se agrega una nueva columna para saber si los comentarios son de Yelp o Amazon.\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "df.head(1100)"
      ],
      "metadata": {
        "id": "QA3En5EYgAtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizaci√≥n de la Fuente de datos.\n"
      ],
      "metadata": {
        "id": "Eagv-jaJgM_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminaci√≥n de signos de puntuaci√≥n."
      ],
      "metadata": {
        "id": "D12fghhagRjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definici√≥n de funci√≥n para eliminar los signos de puntuaci√≥n utilizando re, pero considerando no borrar las vocales con acento.\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Normaliza el texto a NFKD para separar letras y sus tildes\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    # Elimina los caracteres diacr√≠ticos (como las tildes)\n",
        "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
        "    # Elimina todo lo que no sea letras, n√∫meros o espacios\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Aplicar la funci√≥n a la columna 'review_lower'\n",
        "df['Comentarios'] = df['Comentario'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "TlTn8VuygQxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ez4O1zkbhB2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reducir a min√∫sculas el texto."
      ],
      "metadata": {
        "id": "SDKeqnp-gqyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'Comentarios_lower' with lowercase values from 'Comentario'\n",
        "df['Comentarios_lower'] = df['Comentarios'].str.lower()"
      ],
      "metadata": {
        "id": "ExcMpIx7gw-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "_O0hOspchMl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertir a n√∫mero la columna Valor para su postprocesamiento."
      ],
      "metadata": {
        "id": "jk6_6oiZg5Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos la columna rating a valor num√©rico\n",
        "df['Valor'] = pd.to_numeric(df['Valor'], errors='coerce')"
      ],
      "metadata": {
        "id": "a_urHIsIg7Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Valor']"
      ],
      "metadata": {
        "id": "yUwGYj_jhIud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP**"
      ],
      "metadata": {
        "id": "39XV91NgkM9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre Procesamiento."
      ],
      "metadata": {
        "id": "R79sGteNkpVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generaci√≥n del objeto de SPacy para utilizar en el procesamiento del texto en espa√±ol."
      ],
      "metadata": {
        "id": "_NKoVBUUkw0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de aplicar t√©cnicas de an√°lisis de sentimiento, se debe realizar un preprocesamiento del texto que prepare los datos para ser interpretados por modelos de NLP.\n",
        "En este paso, se lleva a cabo la generaci√≥n del objeto de spaCy para trabajar con el idioma espa√±ol, lo cual permite aprovechar herramientas ling√º√≠sticas como la tokenizaci√≥n, lematizaci√≥n"
      ],
      "metadata": {
        "id": "mJtkLU3WP1cG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = es_core_news_md.load()"
      ],
      "metadata": {
        "id": "aiW_XvTOkm2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertir texto a min√∫sculas y Tokenizaci√≥n."
      ],
      "metadata": {
        "id": "1KM1qtISk-6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las primeras transformaciones aplicadas es la conversi√≥n del texto a min√∫sculas, lo que ayuda a normalizar las palabras y evitar que el modelo interprete como diferentes aquellas que solo var√≠an en el uso de may√∫sculas (por ejemplo, \"Bueno\" y \"bueno\"). A continuaci√≥n, se realiza la tokenizaci√≥n, que consiste en dividir el texto en unidades m√≠nimas llamadas tokens (como palabras, signos de puntuaci√≥n o n√∫meros), facilitando el an√°lisis posterior del lenguaje."
      ],
      "metadata": {
        "id": "Qm49X2mKP_L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comentarios_tokenizados'] = df['Comentarios_lower'].apply(lambda text: nlp(text))\n",
        "df[['Comentarios_lower','Comentarios_tokenizados']].head()"
      ],
      "metadata": {
        "id": "uVF3ar5olBDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remoci√≥n de StopWords"
      ],
      "metadata": {
        "id": "ZhnBUrQTl_BT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante la eliminaci√≥n de stopwords, que son palabras vac√≠as o de bajo contenido sem√°ntico (como ‚Äúel‚Äù, ‚Äúla‚Äù, ‚Äúy‚Äù, ‚Äúde‚Äù), se identific√≥ que en algunos casos su remoci√≥n pod√≠a afectar negativamente el sentido original de las frases. Esto es particularmente relevante en rese√±as donde expresiones comunes dependen de ciertas palabras funcionales para conservar su significado completo.\n",
        "\n",
        "Por esta raz√≥n, fue necesario generar un listado personalizado de palabras que deb√≠an conservarse.\n",
        "\n",
        "Esto permiti√≥ preservar la coherencia y contexto de los comentarios, evitando que el modelo perdiera informaci√≥n clave para la detecci√≥n del sentimiento."
      ],
      "metadata": {
        "id": "oMmYeo9lQZbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de palabras que NO queremos eliminar (tienen carga emocional)\n",
        "palabras_sentimiento = {\n",
        "    # Positivas\n",
        "    \"bueno\", \"buena\",\"si\",\"buen√≠simo\", \"excelentes\", \"excelente\", \"genial\", \"maravilloso\", \"maravilla\", \"fant√°stico\", \"fabuloso\", \"incre√≠ble\",\n",
        "    \"perfecto\", \"perfecta\", \"agradable\", \"satisfecho\", \"satisfecha\", \"contento\", \"contenta\", \"encantado\", \"encantada\",\n",
        "    \"amable\", \"simp√°tico\", \"simp√°tica\", \"r√°pido\", \"r√°pida\", \"c√≥modo\", \"c√≥moda\", \"eficaz\", \"eficiente\", \"f√°cil\",\n",
        "    \"recomendable\", \"ideal\", \"espectacular\", \"feliz\", \"brillante\", \"cumpli√≥\", \"cumple\", \"funciona\", \"funciona bien\",\n",
        "    \"inmejorable\", \"confiable\", \"duradero\", \"cumplidor\", \"seguro\", \"preciso\", \"elegante\", \"atento\", \"responsable\",\n",
        "    \"acertado\", \"destacado\", \"excepcional\", \"impecable\", \"sensacional\", \"√∫til\", \"accesible\", \"econ√≥mico\", \"funcional\",\n",
        "    \"intuitivo\", \"conveniente\", \"hermoso\", \"linda\", \"precioso\", \"excelente calidad\", \"vale la pena\",\n",
        "\n",
        "    # Negativas\n",
        "    \"malo\",\"no\", \"mala\", \"mal\", \"p√©simo\", \"p√©sima\",\"nunca\", \"horrible\", \"fatal\", \"insoportable\", \"lento\", \"lenta\", \"inc√≥modo\", \"inc√≥moda\",\n",
        "    \"decepcionante\", \"decepcionado\", \"decepcionada\", \"sucio\", \"sucia\", \"caro\", \"cara\", \"in√∫til\", \"deficiente\", \"desagradable\",\n",
        "    \"complicado\", \"problem√°tico\", \"estafa\", \"enga√±ado\", \"enga√±ada\", \"roto\", \"rota\", \"desastroso\", \"error\", \"errores\",\n",
        "    \"retraso\", \"tardanza\", \"fr√°gil\", \"inestable\", \"poco fiable\", \"nunca m√°s\", \"no volver√©\", \"no recomiendo\", \"no sirve\",\n",
        "    \"no funciona\", \"arruinado\", \"fall√≥\", \"fallando\", \"demora\", \"p√©sima atenci√≥n\", \"servicio malo\", \"mala calidad\", \"molesto\",\n",
        "    \"defecto\", \"problemas\", \"fallas\", \"sin sentido\", \"basura\", \"p√©rdida de dinero\", \"decepci√≥n\"\n",
        "}\n",
        "\n",
        "# Actualizamos spaCy para que NO considere esas palabras como stopwords\n",
        "for palabra in palabras_sentimiento:\n",
        "    lex = nlp.vocab[palabra]\n",
        "    lex.is_stop = False\n",
        "\n",
        "def parse_and_remove_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Remueve las stopwords de un objeto spaCy Doc.\n",
        "    \"\"\"\n",
        "    # Filtrar stopwords y obtener los tokens como texto\n",
        "    tokens_filtrados = [token.text for token in doc if not token.is_stop]\n",
        "    return tokens_filtrados\n",
        "\n",
        "# Aplicar la funci√≥n al DataFrame\n",
        "df['Comentarios_sin_StopWords'] = df['Comentarios_tokenizados'].apply(parse_and_remove_stopwords)\n",
        "\n",
        "df[['Comentarios_tokenizados','Comentarios_sin_StopWords']].head()"
      ],
      "metadata": {
        "id": "2_cr__EqmG4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematizado."
      ],
      "metadata": {
        "id": "ZBXkyZRzmSdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se procede a aplicar la lematizaci√≥n, una t√©cnica que permite reducir cada palabra a su forma base o \"lema\". Por ejemplo, palabras como ‚Äúcomprando‚Äù, ‚Äúcompr√©‚Äù o ‚Äúcomprar√≠an‚Äù se transforman en ‚Äúcomprar‚Äù. Esto es esencial para evitar la dispersi√≥n sem√°ntica y lograr que el modelo reconozca distintas variantes de una palabra como una misma entidad."
      ],
      "metadata": {
        "id": "sYr3W0kMQwwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizar_sin_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Devuelve una lista de lemas excluyendo las stopwords.\n",
        "\n",
        "    Par√°metro:\n",
        "    - doc: objeto spaCy Doc\n",
        "\n",
        "    Retorna:\n",
        "    - Lista de lemas (str) sin stopwords\n",
        "    \"\"\"\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "# Aplicar la funci√≥n y guardar el resultado en una nueva columna\n",
        "df['Comentarios_lema'] = df['Comentarios_tokenizados'].apply(lematizar_sin_stopwords)\n",
        "\n",
        "df[['Comentarios_tokenizados','Comentarios_lema']].head(20)"
      ],
      "metadata": {
        "id": "DsRRS29ImVnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento"
      ],
      "metadata": {
        "id": "9bW_hSCRm02p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conteo de Palabras mas comunes."
      ],
      "metadata": {
        "id": "GNDNz9V5m_EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como parte del an√°lisis exploratorio, se realiz√≥ un conteo de las palabras m√°s frecuentes dentro de los comentarios.\n",
        "Esta etapa permite identificar los t√©rminos que predominan en el lenguaje utilizado por los usuarios y detectar patrones o temas recurrentes en las opiniones.\n",
        "\n",
        "Para un an√°lisis m√°s detallado, el conteo se dividi√≥ entre los comentarios de Yelp y Amazon, con el fin de comparar el vocabulario caracter√≠stico de cada plataforma. Mientras Yelp tiende a centrarse en experiencias relacionadas con servicios (como restaurantes o locales comerciales), Amazon refleja opiniones sobre productos, lo cual se evidencia en las diferencias l√©xicas observadas entre ambos conjuntos de datos."
      ],
      "metadata": {
        "id": "7xH0ak7NRHlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graficar_palabras_comunes(df, origen, top_n=10):\n",
        "    # Filtrar y aplanar los lemas\n",
        "    lemas = [lema for lemas in df[df['Origen'] == origen]['Comentarios_lema'] for lema in lemas]\n",
        "    conteo = Counter(lemas).most_common(top_n)\n",
        "\n",
        "    # Separar palabras y frecuencias\n",
        "    palabras, frecuencias = zip(*conteo)\n",
        "\n",
        "    # Crear gr√°fico\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(palabras, frecuencias, color='skyblue')\n",
        "    plt.xlabel('Frecuencia')\n",
        "    plt.title(f'Top {top_n} Palabras M√°s Comunes - {origen.capitalize()}')\n",
        "    plt.gca().invert_yaxis()  # Poner la palabra m√°s com√∫n arriba\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Graficar para Yelp\n",
        "graficar_palabras_comunes(df, 'yelp')\n",
        "\n",
        "# Graficar para Amazon\n",
        "graficar_palabras_comunes(df, 'amazon')"
      ],
      "metadata": {
        "id": "cHPKUNjjoJQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conteo de bigramas m√°s comunes."
      ],
      "metadata": {
        "id": "GVqmZ6rRoDTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adem√°s del an√°lisis de palabras individuales, se llev√≥ a cabo un conteo de bigramas (pares de palabras consecutivas) con el objetivo de capturar expresiones m√°s completas y contextuales utilizadas por los usuarios en sus comentarios. A diferencia del an√°lisis unigram (una sola palabra), los bigramas permiten identificar frases frecuentes que pueden tener un valor sem√°ntico m√°s claro, como \"muy bueno\", \"no funciona\", \"excelente producto\", entre otros."
      ],
      "metadata": {
        "id": "wDVQ4InbRZ7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import tee\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generar_bigramas(lista):\n",
        "    \"\"\"Devuelve bigramas como tuplas a partir de una lista de palabras\"\"\"\n",
        "    a, b = tee(lista)\n",
        "    next(b, None)\n",
        "    return list(zip(a, b))\n",
        "\n",
        "def graficar_bigramas_comunes(df, origen, top_n=10):\n",
        "    # Filtrar solo los comentarios del origen y generar bigramas\n",
        "    bigramas = [\n",
        "        bigrama\n",
        "        for lemas in df[df['Origen'] == origen]['Comentarios_lema']\n",
        "        for bigrama in generar_bigramas(lemas)\n",
        "    ]\n",
        "\n",
        "    conteo = Counter(bigramas).most_common(top_n)\n",
        "\n",
        "    # Convertir tuplas de bigramas a string para graficar\n",
        "    etiquetas = [' '.join(b) for b, _ in conteo]\n",
        "    frecuencias = [f for _, f in conteo]\n",
        "\n",
        "    # Crear gr√°fico\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(etiquetas, frecuencias, color='mediumseagreen')\n",
        "    plt.xlabel('Frecuencia')\n",
        "    plt.title(f'Top {top_n} Bigramas M√°s Comunes - {origen.capitalize()}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Ejemplo de uso\n",
        "graficar_bigramas_comunes(df, 'yelp')\n",
        "graficar_bigramas_comunes(df, 'amazon')\n"
      ],
      "metadata": {
        "id": "_B3v3nOknL2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordClouds"
      ],
      "metadata": {
        "id": "tn6x4GaEoYRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para complementar el an√°lisis exploratorio, se generaron nubes de palabras que representan gr√°ficamente la frecuencia de aparici√≥n de t√©rminos en los comentarios. Este tipo de visualizaci√≥n permite identificar r√°pidamente las palabras y frases m√°s utilizadas por los usuarios, otorgando una vista intuitiva del contenido predominante en cada dataset.\n",
        "\n",
        "Se realizaron dos tipos de word clouds:\n",
        "\n",
        "Una para monogramas, es decir, palabras individuales.\n",
        "\n",
        "Otra para bigramas, que agrupa las dos palabras consecutivas m√°s frecuentes.\n",
        "\n",
        "Ambos casos compara a Yelp y Amazon."
      ],
      "metadata": {
        "id": "JJ__43PrRsvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WordCloud Yelp."
      ],
      "metadata": {
        "id": "VCXSZ4glokBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar el DataFrame\n",
        "df_yelp = df[df['Origen'] == 'yelp']\n",
        "\n",
        "# Unir todos los lemas en un solo string (comentarios lematizados ya est√°n en listas)\n",
        "texto_yelp = ' '.join([' '.join(lemas) for lemas in df_yelp['Comentarios_lema']])\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_yelp)\n",
        "\n",
        "# Mostrar la nube\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios de Yelp\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UXGDKZCaocLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WordCloud Amazon."
      ],
      "metadata": {
        "id": "njpl5RHiosuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar el DataFrame\n",
        "df_amazon = df[df['Origen'] == 'amazon']\n",
        "\n",
        "# Unir todos los lemas en un solo string (comentarios lematizados ya est√°n en listas)\n",
        "texto_amazon = ' '.join([' '.join(lemas) for lemas in df_amazon['Comentarios_lema']])\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_amazon)\n",
        "\n",
        "# Mostrar la nube\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Nube de Palabras - Comentarios de Yelp\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PHfxMdccou0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WordCloud + Bigramas."
      ],
      "metadata": {
        "id": "_ryxqKXUo6Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_bigramas_spacy(df, origen, top_n=50):\n",
        "    \"\"\"\n",
        "    Genera bigramas usando spaCy a partir de la columna 'Comentarios_Lema', sin stopwords.\n",
        "    Luego genera una nube de palabras.\n",
        "    \"\"\"\n",
        "    # Filtrar los comentarios por 'origen' (por ejemplo, 'yelp' o 'amazon')\n",
        "    comentarios = df[df['Origen'] == origen]['Comentarios_sin_StopWords']\n",
        "\n",
        "    # Generar bigramas\n",
        "    bigramas = []\n",
        "    for comentario in comentarios:\n",
        "        # Crear un Doc de spaCy a partir de la lista de lemas (de la columna 'Comentarios_Lema')\n",
        "        doc = nlp(' '.join(comentario))  # Unimos la lista de lemas y lo procesamos con spaCy\n",
        "        # Extraer bigramas\n",
        "        for i in range(len(doc) - 1):\n",
        "            if not doc[i].is_stop and not doc[i+1].is_stop:  # Asegurarse de que no sean stopwords\n",
        "                bigramas.append((doc[i].lemma_, doc[i+1].lemma_))\n",
        "\n",
        "    # Contar los bigramas m√°s comunes\n",
        "    conteo_bigramas = Counter(bigramas).most_common(top_n)\n",
        "\n",
        "    # Convertir los bigramas a formato texto \"palabra1 palabra2\"\n",
        "    bigramas_texto = {' '.join(bigrama): freq for bigrama, freq in conteo_bigramas}\n",
        "\n",
        "    # Generar la nube de palabras de los bigramas\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigramas_texto)\n",
        "\n",
        "    # Mostrar la nube\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud de Bigramas - {origen.capitalize()}\")\n",
        "    plt.show()\n",
        "\n",
        "# Generar la nube de bigramas para Yelp y Amazon\n",
        "generar_bigramas_spacy(df, 'yelp')\n",
        "generar_bigramas_spacy(df, 'amazon')"
      ],
      "metadata": {
        "id": "qW644mkPo8qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Cloud YELP."
      ],
      "metadata": {
        "id": "6kCTwA9g82po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El word cloud de bigramas extra√≠do de rese√±as en Yelp permite obtener varios insights relevantes sobre la percepci√≥n de los usuarios en relaci√≥n a restaurantes o locales gastron√≥micos.\n",
        "Puntos a destacar:\n",
        "\n",
        "üü¢ Opiniones Positivas (aunque en menor proporci√≥n):\n",
        "\"buen comida\", \"comida deliciosa\", \"comida incre√≠ble\" y \"servicio r√°pido\" reflejan experiencias positivas, donde el sabor y la atenci√≥n fueron bien valorados.\n",
        "\n",
        "Tambi√©n aparecen \"precio razonable\", \"excelente servicio\", y \"personal amable\", lo que sugiere que algunos clientes consideran buena la relaci√≥n calidad-precio y destacan la atenci√≥n al cliente.\n",
        "\n",
        "üî¥ Opiniones Negativas (predominantes):\n",
        "El bigrama m√°s grande y repetido es \"no volver\", seguido por \"no gustar\", \"no buen\", \"no valer\", \"nunca volver\". Esto indica una tendencia fuerte de insatisfacci√≥n.\n",
        "\n",
        "Frases como \"servicio lento\", \"comida mediocre\", \"mal servicio\", y \"30 minuto\" apuntan a problemas concretos en tiempos de espera y calidad del servicio o comida.\n",
        "\n",
        "üü° Insight General:\n",
        "Aunque existen opiniones positivas, el peso visual y frecuencia de las expresiones negativas sugiere una tendencia mayoritaria a la insatisfacci√≥n de los usuarios. Esto podr√≠a deberse a expectativas no cumplidas, problemas de servicio, o mala calidad en algunos platos. Frases como \"simplemente no\", \"definitivamente no\", y \"no recomendar√≠a\" refuerzan esta conclusi√≥n.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qgta7Aca8Nwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Cloud Amazon."
      ],
      "metadata": {
        "id": "6Ro2utcC85U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ Opiniones Negativas (predominan fuertemente):\n",
        "Los bigramas m√°s destacados son \"no funcionar\", \"no compre\", \"no recomendar√≠a\", \"no poder\", \"no comprar\" y \"no cargar\". Esto sugiere una fuerte insatisfacci√≥n con productos que directamente no funcionaron o presentaron fallas.\n",
        "\n",
        "Se menciona \"desperdicio dinero\", \"producto malo\", y \"mant√©ngase alejado\", lo que indica experiencias negativas muy marcadas.\n",
        "\n",
        "La queja sobre \"servicio cliente\" tambi√©n aparece, lo que puede reflejar problemas postventa o con la atenci√≥n al consumidor.\n",
        "\n",
        "üü¢ Opiniones Positivas (en menor medida):\n",
        "Hay menciones como \"buen calidad\", \"excelente auricular\", \"satisfecho compra\", \"producto excelente\", y \"valer pena\", lo cual sugiere que algunos usuarios s√≠ encontraron buenos productos y experiencias satisfactorias.\n",
        "\n",
        "Tambi√©n aparecen \"tono llamado\", \"auricular bluetooth\", \"tel√©fono celular\" sin una connotaci√≥n expl√≠cita negativa, lo que puede referirse simplemente a t√©rminos comunes en las rese√±as.\n",
        "\n",
        "üü° T√©rminos neutros o a interpretar seg√∫n contexto:\n",
        "Bigramas como \"calidad sonido\", \"tono llamado\", \"auricular bluetooth\", o \"bater√≠a funcionar\" pueden estar dentro de contextos tanto positivos como negativos, pero su cercan√≠a con otras frases negativas puede inclinar la interpretaci√≥n hacia quejas sobre calidad de audio o duraci√≥n de bater√≠a.\n",
        "\n",
        "\"sitio web\", \"manos libres\", o \"cargador autom√≥vil\" indican categor√≠as de producto m√°s que sentimientos espec√≠ficos, pero podr√≠an estar en el contexto de reclamos."
      ],
      "metadata": {
        "id": "Z9pc14Am876D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üöÄ An√°lisis de sentimiento en espa√±ol con pysentimiento"
      ],
      "metadata": {
        "id": "3h4HlXQzpWl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez preprocesados los textos, se aplic√≥ un modelo de an√°lisis de sentimiento espec√≠ficamente entrenado para el idioma espa√±ol utilizando la librer√≠a pysentimiento.\n",
        "\n",
        "pysentimiento permite detectar si un comentario es positivo, negativo o neutral, lo cual resulta fundamental para evaluar la percepci√≥n general de los usuarios sobre un producto o servicio. A diferencia de otros enfoques m√°s simples, este modelo considera la estructura gramatical y el significado global de la oraci√≥n, lo que mejora notablemente la precisi√≥n, especialmente en expresiones ambiguas o sarc√°sticas."
      ],
      "metadata": {
        "id": "IxdlTr29SR_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear analizador de sentimientos\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "# Aplicar a una columna de texto\n",
        "df['Sentimiento'] = df['Comentario'].apply(lambda x: analyzer.predict(x).output)\n",
        "# Sentimiento solo guarda lo predicho (POS, NEU o NEG)\n",
        "\n",
        "df['Probabilidad'] = df['Comentario'].apply(lambda x: analyzer.predict(x).probas)\n",
        "#Ese diccionario contiene la probabilidad de cada clase: positivo, neutro, negativo Ejemplo: {'POS': 0.84, 'NEU': 0.10, 'NEG': 0.06}."
      ],
      "metadata": {
        "id": "r5feGF2Gpias"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üìä  Gr√°fico de barras de frecuencia de sentimientos"
      ],
      "metadata": {
        "id": "lZQPvZfZszkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='Sentimiento', order=['POS', 'NEU', 'NEG'], palette='pastel')\n",
        "plt.title('Distribuci√≥n de Sentimientos totales entre Yelp y Amazon')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ddaD7cous3SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa una distribuci√≥n bastante equilibrada de sentimientos entre Positivos, Negativos y Neutros."
      ],
      "metadata": {
        "id": "t5v3p4ojXMOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distribuci√≥n de los sentimientos."
      ],
      "metadata": {
        "id": "NyE94c4JtDWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentimiento'].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    labels=['Positivo', 'Neutro', 'Negativo'],\n",
        "    colors=['lightgreen', 'lightblue', 'salmon']\n",
        ")\n",
        "plt.title('Distribuci√≥n porcentual de Sentimientos')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dI2qGwfVs837"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### An√°lisis de Confianza para filtrar comentarios con baja certeza"
      ],
      "metadata": {
        "id": "N9Ehqc_htP8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de aplicar el modelo de an√°lisis de sentimiento con pysentimiento, se incorpor√≥ una etapa adicional para evaluar la confianza de las predicciones. Este an√°lisis se basa en las probabilidades asignadas a cada clase (positivo, negativo o neutral), lo que permite identificar qu√© tan seguro est√° el modelo respecto a cada clasificaci√≥n realizada.\n",
        "\n",
        "Con esta informaci√≥n se implement√≥ un filtro de confianza, excluyendo o marcando aquellos comentarios cuya predicci√≥n tiene baja certeza (por ejemplo, menor al 60%). Esta estrategia ayuda a reducir errores en el an√°lisis general, ya que evita tomar decisiones basadas en clasificaciones inciertas o ambiguas.\n",
        "\n",
        "Adem√°s, el an√°lisis de confianza permite estudiar qu√© tipos de comentarios generan m√°s dudas en el modelo, lo que puede ser √∫til para mejorar el preprocesamiento, ajustar umbrales, o incluso etiquetar manualmente ciertos casos en futuras iteraciones del modelo."
      ],
      "metadata": {
        "id": "vqG1qPzUSzN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# M√°xima probabilidad (nivel de certeza del modelo)\n",
        "df['Confianza'] = df['Probabilidad'].apply(lambda x: max(x.values()))  # En este caso, de la lista {'POS': 0.84, 'NEU': 0.10, 'NEG': 0.06} s√≥lo guarda 0.84 que es el valor mayor\n",
        "\n",
        "# Filtrar comentarios cuya confianza sea menor a 0.6\n",
        "comentarios_baja_confianza = df[df['Confianza'] < 0.6]\n",
        "comentarios_alta_confianza = df[df['Confianza'] >= 0.6]\n",
        "# Ver los primeros resultados\n",
        "#comentarios_baja_confianza[['Comentarios', 'Sentimiento', 'Confianza']]"
      ],
      "metadata": {
        "id": "cp_TPbgYtUPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distribuci√≥n de los comentarios filtrados con mayor confianza."
      ],
      "metadata": {
        "id": "Np0Wbkh-t9ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Confianza'] >= 0.6]['Sentimiento'].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    labels=['Positivo', 'Neutro', 'Negativo'],\n",
        "    colors=['lightgreen', 'lightblue', 'salmon']\n",
        ")\n",
        "plt.title('Distribuci√≥n porcentual de Sentimientos con alta confianza')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DKGk25nltsFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos deducir que al reducirse los sentinmientos Negativos, al analizador le estaba costando interpretar ese tipo de sentimientos."
      ],
      "metadata": {
        "id": "OACZuNRmXZFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distribuci√≥n de la Confianza de los comentarios."
      ],
      "metadata": {
        "id": "8UwgeOBbuhWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x='Confianza', bins=20, kde=True, color='skyblue')\n",
        "plt.axvline(0.6, color='red', linestyle='--', label='Umbral 0.6')\n",
        "plt.title('Distribuci√≥n de Confianza del Sentimiento')\n",
        "plt.xlabel('Confianza')\n",
        "plt.ylabel('Cantidad de Comentarios')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W1mX2p80uabR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comentarios con baja confianza.\n",
        "\n",
        "Los comentarios con baja probabilidad en la predicci√≥n del sentimiento (cercanos a 0.6) son especialmente valiosos en un an√°lisis de opiniones por lo siguiente:\n",
        "\n",
        "Ambig√ºedad en el lenguaje: Estos comentarios suelen contener expresiones ambiguas, mixtas o neutras, que no son claramente positivas ni negativas. Analizarlos ayuda a entender mejor los matices del lenguaje natural y las limitaciones del modelo.\n",
        "\n",
        "Mejora del modelo: Identificar este tipo de casos permite detectar patrones ling√º√≠sticos que el modelo no logra interpretar bien. Estos ejemplos pueden ser usados para mejorar el entrenamiento futuro mediante t√©cnicas como data augmentation o fine-tuning.\n",
        "\n",
        "Detecci√≥n de casos fronterizos: Comentarios con baja confianza son √∫tiles para encontrar rese√±as \"en el l√≠mite\", que podr√≠an necesitar intervenci√≥n manual o una segunda revisi√≥n, especialmente en aplicaciones cr√≠ticas como moderaci√≥n de contenido o evaluaci√≥n de satisfacci√≥n de clientes.\n",
        "\n",
        "Toma de decisiones m√°s informadas: En un sistema automatizado, se pueden establecer umbrales de confianza. Por ejemplo, rese√±as con probabilidad entre 0.45 y 0.55 pueden marcarse para revisi√≥n humana, lo que mejora la calidad del an√°lisis sin revisar todo el contenido.\n",
        "\n",
        "Calibraci√≥n del modelo: Observar c√≥mo se comporta el modelo en los casos donde no est√° seguro ayuda a evaluar si las probabilidades emitidas est√°n bien calibradas o si el modelo tiende a ser demasiado confiado o conservador.\n"
      ],
      "metadata": {
        "id": "XokZcz4cBBMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_baja_confianza[['Comentario', 'Sentimiento', 'Confianza', 'Probabilidad']].sort_values(by='Confianza').head(20)\n"
      ],
      "metadata": {
        "id": "4tHWg2UiT5Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pruebas de Modelos de Machine Learning.**"
      ],
      "metadata": {
        "id": "vk7QPjtIv4rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este punto, se realizar√° un an√°lisis de texto con el objetivo de predecir una variable num√©rica que nos indica positvo (para 1) y negativo (para 0), a partir de opiniones o rese√±as escritas por usuarios.\n",
        "\n",
        "\n",
        "Para convertir los textos en datos num√©ricos que puedan ser procesados por modelos de machine learning, seutilizaran dos t√©cnicas de representaci√≥n de texto:TF-IDF (Term Frequency-Inverse Document Frequency) y Bag of Words (BoW).\n",
        "\n",
        "\n",
        "TF-IDF pondera la frecuencia de las palabras en cada documento ajust√°ndola seg√∫n su frecuencia inversa en todo el corpus, dando mayor peso a t√©rminos distintivos y reduciendo la influencia de palabras comunes.\n",
        "\n",
        "Bag of Words representa cada documento como un vector que indica la frecuencia de cada palabra, sin considerar el orden ni la relevancia contextual.\n",
        "\n",
        "Estos vectores van a ser utilizados como entrada para un modelo de regresi√≥n log√≠stica, que permitir√° predecir la variable objetivo asociada a cada texto, en este caso si los comentarios son positivos o negativos.\n",
        "\n",
        "Adicionalmente, se incorporar√° un modelo de deep learning utilizando la biblioteca Keras, que aprvechando redes neuronales para capturar patrones m√°s complejos en los textos, incluyendo relaciones contextuales y secuenciales entre palabras que no pueden ser detectadas por las representaciones tradicionales.\n",
        "\n",
        "Se entrenar√°n y evaluar√°n los tres modelos ‚Äîregresi√≥n lineal con Bag of Words, regresi√≥n lineal con TF-IDF y red neuronal profunda con Keras‚Äî para comparar su desempe√±o predictivo.\n",
        "\n",
        "La evaluaci√≥n incluir√° m√©tricas adecuadas para regresi√≥n y an√°lisis de generalizaci√≥n, con el fin de identificar cu√°l enfoque es m√°s efectivo para este problema espec√≠fico.\n",
        "\n",
        "Este an√°lisis permitir√° no solo comparar t√©cnicas cl√°sicas y modernas de procesamiento de texto, sino tambi√©n obtener insights sobre la relevancia y el impacto de las palabras y estructuras en la predicci√≥n, mejorando la comprensi√≥n del comportamiento del modelo y la calidad de las predicciones."
      ],
      "metadata": {
        "id": "OLYy9hwkE4Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Divisi√≥n de datos de entrenamiento y prueba."
      ],
      "metadata": {
        "id": "qD43WO8o5LaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['Comentario'], df['Valor'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "jkK0y7oy5Z_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi√≥n Log√≠stica."
      ],
      "metadata": {
        "id": "2189jWXqwNhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizando TF-IFD."
      ],
      "metadata": {
        "id": "JBBoS9QowA6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency - Inverse Document Frequency) es una t√©cnica de procesamiento de texto utilizada para evaluar la importancia de una palabra dentro de un conjunto de documentos. Se basa en dos conceptos:\n",
        "\n",
        "TF (Frecuencia de T√©rmino): Mide cu√°ntas veces aparece un t√©rmino en un documento espec√≠fico, comparado con el n√∫mero total de t√©rminos en ese documento. Esto ayuda a capturar cu√°n relevante es una palabra dentro de un documento en particular.\n",
        "\n",
        "IDF (Frecuencia Inversa de Documentos): Mide la importancia de una palabra dentro de un conjunto de documentos. Si una palabra aparece en muchos documentos, tiene menos valor. La f√≥rmula es:\n",
        "\n",
        "Esto ayuda a reducir el peso de las palabras que aparecen frecuentemente en todos los documentos (como \"el\", \"y\", \"de\"), ya que no agregan mucha informaci√≥n.\n",
        "\n",
        "As√≠, la importancia de un t√©rmino en un documento depende tanto de su frecuencia en ese documento como de cu√°n com√∫n es en todo el conjunto de documentos."
      ],
      "metadata": {
        "id": "Z5Qu0o3RwyoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C√°lculo de TF-IDF con TfidVetorizer y analisis de n-gramas."
      ],
      "metadata": {
        "id": "AvQezV_qx8g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # TFIDF espera trabajar con strings y  no listas, por lo que se procede a crear una nueva columna con los datos tokenizados en formato str.\n",
        "df['Comentarios_sin_StopWords_str'] = df['Comentarios_sin_StopWords'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Crear el vectorizador\n",
        "tfidfvectorizer = TfidfVectorizer(ngram_range=(1,5))\n",
        "#inlcuyo bigramas y trigramas para que le de contexto a los comentarios. Esto me permite ver un \"No conforme\" y no solamente le \"No\" y el \"Conforme\" por separado.\n",
        "\n",
        "# Ajustar y transformar\n",
        "tfidf_matrix = tfidfvectorizer.fit_transform(df['Comentarios_sin_StopWords_str'])\n",
        "\n",
        "# Obtener los t√©rminos\n",
        "features = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "# Convertir la matriz a DataFrame\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=features)\n",
        "\n",
        "# Sumar TF-IDF por columna\n",
        "tfidf_scores = df_tfidf.sum().sort_values(ascending=False)\n",
        "\n",
        "# Mostrar top 10\n",
        "print(\"üîù Top 10 n-gramas por score TF-IDF:\")\n",
        "print(tfidf_scores.head(10).round(3))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yOA1KulbyJG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame auxiliar con el tipo de n-grama\n",
        "df_scores = pd.DataFrame({\n",
        "    'ngram': tfidf_scores.index,\n",
        "    'score': tfidf_scores.values,\n",
        "    'tipo': tfidf_scores.index.to_series().apply(lambda x: f'{len(x.split())}-grama')\n",
        "})\n",
        "\n",
        "# Ver los 5 m√°s importantes por tipo\n",
        "top_n = 5\n",
        "for tipo in ['1-grama', '2-grama', '3-grama']:\n",
        "    print(f\"\\nüîù Top {top_n} {tipo}s:\")\n",
        "    print(df_scores[df_scores['tipo'] == tipo].head(top_n).to_string(index=False))"
      ],
      "metadata": {
        "id": "8TncVynMzZlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El an√°lisis de los n-gramas (1, 2 y 3 palabras) revela una fuerte tendencia negativa en los comentarios analizados.\n",
        "\n",
        "Esto se evidencia principalmente por:\n",
        "\n",
        "La presencia dominante de la palabra \"no\" como unigram (1-grama) m√°s relevante, indicando una alta frecuencia de negaciones.\n",
        "\n",
        "Los bigramas y trigramas refuerzan esta tendencia negativa, con frases como \"no volveremos\", \"no volvere\", \"no funciona\", \"no recomendaria\", \"no vale pena\" y \"no compre producto\", todas las cuales reflejan insatisfacci√≥n o malas experiencias.\n",
        "\n",
        "Aun as√≠, hay menciones positivas como \"buena comida servicio\", pero estas son menos frecuentes o tienen menor peso que las negativas."
      ],
      "metadata": {
        "id": "b4VhVp3p41Q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ajuste de datos de Entrenamiento."
      ],
      "metadata": {
        "id": "jGzhxUtp6DAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ajustamos el vectorizador TF-IDF con los datos de entrenamiento y test  transformando esos datos en una matriz num√©rica."
      ],
      "metadata": {
        "id": "OREs2HSJ-9jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar y transformar los datos de entrenamiento\n",
        "X_train_tfidf = tfidfvectorizer.fit_transform(X_train)\n",
        "# Transformar los datos de prueba\n",
        "X_test_tfidf = tfidfvectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "C9X3Y3tX6JQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generaci√≥n y prueba de modelo Regresi√≥n Log√≠stica."
      ],
      "metadata": {
        "id": "4zXnyKl47GSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un modelo de regresi√≥n log√≠stica\n",
        "# Abajo ten√©s un c√≥digo con los par√°metros expresados de forma que puedas ir modificandolos\n",
        "model_log_reg = LogisticRegression() #Instanciamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model_log_reg.fit(X_train_tfidf, y_train) # Fiteamos, es decir, el modelo aprende a partir de los datos de entrenamiento\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred_log_reg = model_log_reg.predict(X_test_tfidf) # Predecir"
      ],
      "metadata": {
        "id": "lmGzeGnU7NlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluaci√≥n del Modelo."
      ],
      "metadata": {
        "id": "BKMGMFHX8OKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriz de Confusi√≥n."
      ],
      "metadata": {
        "id": "i8zfSea79_Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Matriz de confusi√≥n\n",
        "\n",
        "# La Matriz de Confusi√≥n es √∫til para Muestra los aciertos y errores del modelo organizados por clase.\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
        "labels = ['Negativo', 'Positivo']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicci√≥n')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SZu-1myh8k3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Interpretaci√≥n de los resultados:\n",
        "TP (Verdaderos Positivos) = 148: Casos positivos correctamente clasificados como positivos.\n",
        "\n",
        "TN (Verdaderos Negativos) = 181: Casos negativos correctamente clasificados como negativos.\n",
        "\n",
        "FP (Falsos Positivos) = 28: Casos negativos mal clasificados como positivos.\n",
        "\n",
        "FN (Falsos Negativos) = 33: Casos positivos mal clasificados como negativos."
      ],
      "metadata": {
        "id": "r9XnpcfN_efX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La matriz de confusi√≥n muestra un buen desempe√±o del modelo:\n",
        "- 181 verdaderos negativos y 148 verdaderos positivos indican una buena capacidad para clasificar correctamente ambas clases.\n",
        "- Sin embargo, hay 28 falsos positivos y 33 falsos negativos, lo que sugiere que el modelo comete algunos errores, especialmente en la identificaci√≥n de la clase positiva.\n",
        "- Estos errores podr√≠an ser relevantes dependiendo del contexto del problema (por ejemplo, si detectar positivos es cr√≠tico)."
      ],
      "metadata": {
        "id": "cbcBvQDf_jVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curva ROC AUC."
      ],
      "metadata": {
        "id": "YAkrMHlM-DEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El ROC AUC (Receiver Operating Characteristic - Area Under Curve) es una m√©trica que mide la capacidad del modelo para distinguir entre clases (positiva y negativa), evaluando todas las combinaciones posibles de umbrales de clasificaci√≥n."
      ],
      "metadata": {
        "id": "K18ta-8I_yhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC: Es una curva que grafica la tasa de verdaderos positivos (TPR) contra la tasa de falsos positivos (FPR) a distintos umbrales.\n",
        "\n",
        "AUC (Area Under Curve): Es el √°rea bajo esa curva, y su valor va de 0 a 1:\n",
        "\n",
        "1.0 = modelo perfecto.\n",
        "\n",
        "0.5 = modelo sin capacidad de clasificaci√≥n (como adivinar).\n",
        "\n",
        "< 0.5 = peor que adivinar (clasifica al rev√©s)."
      ],
      "metadata": {
        "id": "RUs72JXbAGZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, model_log_reg.decision_function(X_test_tfidf))\n",
        "roc_auc = roc_auc_score(y_test, model_log_reg.decision_function(X_test_tfidf))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'√Årea bajo la curva ROC AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o6Iu7ehz9ZCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìà Interpretaci√≥n de tu resultado (AUC = 0.89):\n",
        "El valor 0.89 indica que el modelo tiene una alta capacidad para distinguir entre clases.\n",
        "\n",
        "En promedio, hay un 89% de probabilidad de que el modelo asigne un mayor score a una instancia positiva que a una negativa.\n",
        "\n",
        "Este resultado sugiere que el modelo est√° haciendo un buen trabajo, incluso si a√∫n hay algunos falsos positivos o falsos negativos."
      ],
      "metadata": {
        "id": "ENa983zfAKZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M√©tricas de Predicci√≥n."
      ],
      "metadata": {
        "id": "Tzf7JNqq-FnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy:Para medir qu√© tan bien predice el modelo en datos nuevos (exactitud).\n",
        "Accuracy mide el porcentaje total de predicciones correctas sobre el total de casos.\n",
        "- Precision: Para medir el costo de un falso positivo es alto (por ejemplo, recomendar una pel√≠cula mala como buena).\n",
        "Precision mide qu√© proporci√≥n de las predicciones positivas hechas por el modelo son realmente positivas.\n",
        "- Recall: Para medir cu√°ntos de los casos positivos reales fueron capturados por el modelo.\n",
        "- f1 Score: Para medir el promedio arm√≥nico entre precisi√≥n y recall. Un buen balance si ambas cosas son importantes."
      ],
      "metadata": {
        "id": "EDNMuCVwAUwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. M√©tricas\n",
        "# Accuracy:Para medir qu√© tan bien predice el modelo en datos nuevos (exactitud).\n",
        "# Accuracy mide el porcentaje total de predicciones correctas sobre el total de casos.\n",
        "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
        "# Precision: Para medir el costo de un falso positivo es alto (por ejemplo, recomendar una pel√≠cula mala como buena).\n",
        "# Precision mide qu√© proporci√≥n de las predicciones positivas hechas por el modelo son realmente positivas.\n",
        "precision = precision_score(y_test, y_pred_log_reg)\n",
        "# Recall: Para medir cu√°ntos de los casos positivos reales fueron capturados por el modelo.\n",
        "recall = recall_score(y_test, y_pred_log_reg)\n",
        "#\n",
        "f1 = f1_score(y_test, y_pred_log_reg)\n",
        "\n",
        "print(\"M√©tricas de desempe√±o del modelo:\")\n",
        "print(f\"Accuracy : {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall   : {recall:.2f}\")\n",
        "print(f\"F1 Score : {f1:.2f}\")"
      ],
      "metadata": {
        "id": "yhq9A09B9Y2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados:\n",
        "\n",
        "- Accuracy = 0.82\n",
        "El 82% de todas las predicciones (positivas y negativas) fueron correctas. Es una medida general del rendimiento.\n",
        "Sin embargo, puede ser enga√±osa si las clases est√°n desbalanceadas.\n",
        "\n",
        "- Precision = 0.80\n",
        "De todas las predicciones positivas que hizo el modelo, el 80% fueron realmente positivas.\n",
        "Es importante si queremos minimizar falsos positivos (por ejemplo, evitar alarmas innecesarias).\n",
        "\n",
        "- Recall = 0.82\n",
        "El modelo identific√≥ correctamente el 82% de todos los casos realmente positivos.\n",
        "Es importante si queremos minimizar falsos negativos (por ejemplo, no dejar pasar casos positivos importantes).\n",
        "\n",
        "- F1 Score = 0.81\n",
        "Es el promedio arm√≥nico entre precision y recall. Resume el equilibrio entre ambos.\n",
        " Un F1 de 0.81 indica un buen balance entre identificar positivos y no equivocarse al predecirlos.\n"
      ],
      "metadata": {
        "id": "PpHJs1p1Avpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Validaci√≥n Cruzada.*"
      ],
      "metadata": {
        "id": "YhVd_UpcIthW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La validaci√≥n cruzada es una t√©cnica para evaluar la capacidad de generalizaci√≥n de un modelo. Consiste en:\n",
        "\n",
        "Dividir los datos en k partes (folds).\n",
        "\n",
        "Entrenar el modelo con k-1 partes y validar con la parte restante.\n",
        "\n",
        "Repetir esto k veces, cambiando el fold de validaci√≥n en cada iteraci√≥n.\n",
        "\n",
        "Calcular el promedio de las m√©tricas obtenidas en cada iteraci√≥n.\n",
        "\n",
        "Esto reduce el riesgo de que el modelo est√© sobreajustado (overfitting) a una √∫nica partici√≥n de los datos."
      ],
      "metadata": {
        "id": "sB0HBzUiBfUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline que junta vectorizador y modelo\n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(max_features=5000),\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "# Validaci√≥n cruzada con 5 particiones (k-fold = 5)\n",
        "scores = cross_val_score(pipeline, df['Comentario'], df['Valor'], cv=5, scoring='accuracy')\n",
        "\n",
        "# Resultados\n",
        "print(f\"Precisi√≥n media con validaci√≥n cruzada: {scores.mean():.3f}\")\n",
        "print(f\"Desviaci√≥n est√°ndar: {scores.std():.3f}\")"
      ],
      "metadata": {
        "id": "I1TpkwSyI8OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados de la validaci√≥n cruzada:\n",
        "\n",
        "La precisi√≥n media del modelo es 0.807, lo que indica que, en promedio, el modelo acierta con un 80.7% de efectividad\n",
        "en los distintos subconjuntos del conjunto de datos evaluados.\n",
        "\n",
        "La desviaci√≥n est√°ndar es 0.018, lo que significa que el desempe√±o del modelo es bastante consistente entre los diferentes folds.\n",
        "Es decir, no hay una gran variaci√≥n en la precisi√≥n dependiendo del conjunto de entrenamiento/validaci√≥n utilizado.\n",
        "Estos resultados sugieren que el modelo tiene un buen rendimiento general y una buena capacidad de generalizaci√≥n."
      ],
      "metadata": {
        "id": "P1LeKo0ABsZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizaci√≥n de palabras asociadas a rese√±as positivas y negativas."
      ],
      "metadata": {
        "id": "voZXIBS9QDz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta visualizaci√≥n permite identificar las palabras m√°s frecuentes o relevantes en cada grupo de rese√±as, separando aquellas asociadas con opiniones positivas de las relacionadas con opiniones negativas.\n",
        "\n",
        "Al analizar estas palabras clave, podemos entender mejor qu√© aspectos del producto o servicio generan satisfacci√≥n o insatisfacci√≥n en los usuarios.\n",
        "\n",
        "Este tipo de an√°lisis ayuda a extraer insights cualitativos que complementan las m√©tricas cuantitativas, y es muy √∫til para mejorar la experiencia del cliente y orientar acciones espec√≠ficas de mejora."
      ],
      "metadata": {
        "id": "V8xiZv-rCDoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos las palabras del vocabulario\n",
        "palabras = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "# Coeficientes del modelo (uno por palabra)\n",
        "coeficientes = model_log_reg.coef_[0]\n",
        "\n",
        "# Creamos un DataFrame para visualizarlo\n",
        "df_coef = pd.DataFrame({'palabra': palabras, 'coeficiente': coeficientes})\n",
        "\n",
        "# Ordenamos por importancia\n",
        "df_coef = df_coef.sort_values(by='coeficiente', ascending=False)\n",
        "\n",
        "# En la primera columna veremos el n√∫mero \"√≠ndice\" de cada palabra seg√∫n el √≥rden en que fueron procesadas en el modelo.\n",
        "\n",
        "# Mostramos las 10 palabras m√°s asociadas a valoraci√≥n positiva y negativa\n",
        "print(\"üîº Palabras m√°s asociadas a rese√±as positivas:\")\n",
        "print(df_coef.head(10))\n",
        "\n",
        "print(\"\\nüîΩ Palabras m√°s asociadas a rese√±as negativas:\")\n",
        "print(df_coef.tail(10))"
      ],
      "metadata": {
        "id": "2bJEOJTHQKb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las palabras con coeficientes positivos m√°s altos, como \"gran\", \"buen\", \"excelente\", \"funciona\" y \"incre√≠ble\", est√°n fuertemente asociadas con rese√±as positivas, reflejando satisfacci√≥n, calidad y buen desempe√±o del producto o servicio.\n",
        "\n",
        "En contraste, las palabras con coeficientes negativos m√°s fuertes, como \"no\", \"mala\", \"decepcionado\", \"horrible\" y \"terrible\", se asocian claramente con rese√±as negativas, indicando insatisfacci√≥n, problemas y decepci√≥n por parte de los usuarios.\n",
        "\n",
        "Esto muestra que el modelo ha identificado correctamente los t√©rminos que expresan opiniones positivas y negativas, lo que facilita la interpretaci√≥n y el an√°lisis cualitativo del sentimiento en los textos."
      ],
      "metadata": {
        "id": "NoIqfm9UCbRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficar las 10 palabras m√°s positivas y m√°s negativas"
      ],
      "metadata": {
        "id": "6inidvU-R88J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colores pastel suaves (m√°s apagados)\n",
        "color_positivas = '#388e3c'  # verde claro apagado\n",
        "color_negativas = '#e65100'  # rojo claro apagado\n",
        "\n",
        "# Top 10 positivas y negativas\n",
        "top_positivas = df_coef.head(10)\n",
        "top_negativas = df_coef.tail(10).sort_values(by='coeficiente')\n",
        "\n",
        "# Crear la figura y los ejes\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Gr√°fico de palabras positivas\n",
        "bars1 = ax[0].barh(top_positivas['palabra'], top_positivas['coeficiente'], color=color_positivas)\n",
        "ax[0].set_title('üîº Palabras asociadas a rese√±as positivas', fontsize=14)\n",
        "ax[0].invert_yaxis()\n",
        "ax[0].set_xlabel('Coeficiente', fontsize=12)\n",
        "\n",
        "# Agregar valores al final de las barras (positivas)\n",
        "for bar in bars1:\n",
        "    width = bar.get_width()\n",
        "    ax[0].text(width + 0.01, bar.get_y() + bar.get_height() / 2,\n",
        "               f'{width:.2f}', va='center', fontsize=10)\n",
        "\n",
        "# Gr√°fico de palabras negativas\n",
        "bars2 = ax[1].barh(top_negativas['palabra'], top_negativas['coeficiente'], color=color_negativas)\n",
        "ax[1].set_title('üîΩ Palabras asociadas a rese√±as negativas', fontsize=14)\n",
        "ax[1].invert_yaxis()\n",
        "ax[1].set_xlabel('Coeficiente', fontsize=12)\n",
        "\n",
        "# Agregar valores al final de las barras (negativas)\n",
        "for bar in bars2:\n",
        "    width = bar.get_width()\n",
        "    ax[1].text(width - 0.01, bar.get_y() + bar.get_height() / 2,\n",
        "               f'{width:.2f}', va='center', ha='right', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VnNJCbVbRLMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar bigramas y trigramas\n",
        "# Cambiar 'ngrama' a 'palabra' para acceder a la columna correcta\n",
        "df_bi_tri = df_coef[df_coef['palabra'].str.count(' ') >= 2]\n",
        "\n",
        "top_pos_bi_tri = df_bi_tri.sort_values('coeficiente', ascending=False).head(10)\n",
        "top_neg_bi_tri = df_bi_tri.sort_values('coeficiente', ascending=True).head(10)\n",
        "\n",
        "color_positivas = '#388e3c'  # verde oscuro\n",
        "color_negativas = '#e65100'  # naranja oscuro\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
        "\n",
        "axes[0].barh(top_pos_bi_tri['palabra'], top_pos_bi_tri['coeficiente'], color=color_positivas) # Cambiar 'ngrama' a 'palabra'\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].set_title('üîº Bigrams y Trigrams positivos')\n",
        "axes[0].set_xlabel('Coeficiente')\n",
        "\n",
        "axes[1].barh(top_neg_bi_tri['palabra'], top_neg_bi_tri['coeficiente'], color=color_negativas) # Cambiar 'ngrama' a 'palabra'\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].set_title('üîΩ Bigrams y Trigrams negativos')\n",
        "axes[1].set_xlabel('Coeficiente')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VgoYFK0eTcBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† ¬øQu√© muestran los gr√°ficos?\n",
        "Las palabras con coeficientes positivos son las que m√°s contribuyen a que el modelo prediga una rese√±a positiva.\n",
        "\n",
        "Las palabras con coeficientes negativos son las que m√°s empujan al modelo hacia una predicci√≥n negativa."
      ],
      "metadata": {
        "id": "11CjMM0lR28T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba del modelo."
      ],
      "metadata": {
        "id": "bbw6-1oWJYA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se genera un c√≥digo para probar distintas frases con comentarios genericos con el motivo de evaluar la funcionalidad del modelo de predicci√≥n."
      ],
      "metadata": {
        "id": "E7ssFQe3D2jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nueva_rese√±a = \"no lo recominedo\"  # Reemplaza con la rese√±a que deseas probar\n",
        "nueva_rese√±a_tfidf = tfidfvectorizer.transform([nueva_rese√±a])\n",
        "prediccion = model_log_reg.predict(nueva_rese√±a_tfidf)\n",
        "# Obtener la probabilidad de la predicci√≥n\n",
        "probabilidadpositiva = model_log_reg.predict_proba(nueva_rese√±a_tfidf)\n",
        "\n",
        "# Obtener la probabilidad en la clase predicha (0 o 1)\n",
        "probabilidad = probabilidadpositiva[0][1]  # Probabilidad de la clase \"positivo\"\n",
        "\n",
        "print(f\"Se predice que la cr√≠tica es de caracter {prediccion[0]}\")\n",
        "print(f\" con una probabilidad de que sea positiva de {probabilidad:.2f}\")"
      ],
      "metadata": {
        "id": "W8rk_rn_JdGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizando Bag of Words."
      ],
      "metadata": {
        "id": "6nA1NdjV-tYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW convierte un conjunto de documentos en una matriz de ocurrencias de palabras. A diferencia de TF-IDF, que pondera las palabras seg√∫n su frecuencia e importancia en relaci√≥n con todo el corpus, BoW solo cuenta cu√°ntas veces aparece una palabra en un documento sin considerar la frecuencia global de la palabra."
      ],
      "metadata": {
        "id": "4N3SqoNl-9SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ajuste de datos de entrenamiento para BoW."
      ],
      "metadata": {
        "id": "9O22cegICxIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciamos el vectorizador BoW\n",
        "vectorizer_bow = CountVectorizer()\n",
        "\n",
        "# Aplicamos el vectorizador a los comentarios lematizados (ahora en formato string)\n",
        "vector_bow = vectorizer_bow.fit_transform(df['Comentarios_sin_StopWords_str'])\n",
        "\n",
        "# Convertimos la matriz de caracter√≠sticas en un DataFrame para visualizar\n",
        "bow_df = pd.DataFrame(vector_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
        "\n",
        "# Obtener los nombres de las caracter√≠sticas (palabras)\n",
        "features_bow = vectorizer_bow.get_feature_names_out()\n",
        "\n",
        "\n",
        "# Crear un DataFrame con las frecuencias de las palabras\n",
        "df_bow = pd.DataFrame(vector_bow.toarray(), columns=features_bow)"
      ],
      "metadata": {
        "id": "puoPyVHMDFOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar y transformar los datos de entrenamiento\n",
        "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
        "# Transformar los datos de prueba\n",
        "X_test_bow  = X_test_bow  = vectorizer_bow.transform(X_test)"
      ],
      "metadata": {
        "id": "KuBThnFkC0eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generaci√≥n y prueba del Modelo con BoW."
      ],
      "metadata": {
        "id": "U-LmgSjwHBdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un modelo de regresi√≥n log√≠stica\n",
        "# Abajo ten√©s un c√≥digo con los par√°metros expresados de forma que puedas ir modificandolos\n",
        "model_log_reg_Bow = LogisticRegression() #Instanciamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model_log_reg_Bow.fit(X_train_bow, y_train) # Fiteamos, es decir, el modelo aprende a partir de los datos de entrenamiento\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred_log_reg_Bow = model_log_reg_Bow.predict(X_test_bow) # Predecir"
      ],
      "metadata": {
        "id": "qGBcMOnvHAle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluaci√≥n del Modelo.\n"
      ],
      "metadata": {
        "id": "PSo23e2IHX2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriz de Confusi√≥n."
      ],
      "metadata": {
        "id": "LxPQYjq3HhL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Matriz de confusi√≥n\n",
        "\n",
        "# La Matriz de Confusi√≥n es √∫til para Muestra los aciertos y errores del modelo organizados por clase.\n",
        "\n",
        "cm1 = confusion_matrix(y_test, y_pred_log_reg_Bow)\n",
        "labels = ['Negativo', 'Positivo']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicci√≥n')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusi√≥nc con BoW')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9PBCupjHi8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No hubo cambios respecto a la evaluaci√≥n realiza en el modelo usando TF-IDF"
      ],
      "metadata": {
        "id": "SXYbiJd-mjbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curva ROC AUC."
      ],
      "metadata": {
        "id": "tlGSMfUAH1D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Curva ROC\n",
        "# ROC AUC SCORE eval√∫a qu√© tan bien el modelo separa las clases.\n",
        "fpr1, tpr1, _ = roc_curve(y_test, model_log_reg_Bow.decision_function(X_test_bow))\n",
        "roc_auc1 = roc_auc_score(y_test, model_log_reg_Bow.decision_function(X_test_bow))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr1, tpr1, color='darkorange', lw=2, label=f'√Årea bajo la curva ROC AUC = {roc_auc1:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HONvbjo4HzuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No hubo cambios respecto a la evaluaci√≥n realiza en el modelo usando TF-IDF"
      ],
      "metadata": {
        "id": "TkowryuSmydP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M√©tricas de Predicci√≥n."
      ],
      "metadata": {
        "id": "vW0kWKhaIHmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. M√©tricas\n",
        "# Accuracy:Para medir qu√© tan bien predice el modelo en datos nuevos (exactitud).\n",
        "# Accuracy mide el porcentaje total de predicciones correctas sobre el total de casos.\n",
        "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
        "# Precision: Para medir el costo de un falso positivo es alto (por ejemplo, recomendar una pel√≠cula mala como buena).\n",
        "# Precision mide qu√© proporci√≥n de las predicciones positivas hechas por el modelo son realmente positivas.\n",
        "precision = precision_score(y_test, y_pred_log_reg)\n",
        "# Recall: Para medir cu√°ntos de los casos positivos reales fueron capturados por el modelo.\n",
        "recall = recall_score(y_test, y_pred_log_reg)\n",
        "# f1 Score: Para medir el promedio arm√≥nico entre precisi√≥n y recall. Un buen balance si ambas cosas son importantes.\n",
        "f1 = f1_score(y_test, y_pred_log_reg)\n",
        "\n",
        "print(\"M√©tricas de desempe√±o del modelo:\")\n",
        "print(f\"Accuracy : {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall   : {recall:.2f}\")\n",
        "print(f\"F1 Score : {f1:.2f}\")"
      ],
      "metadata": {
        "id": "I2xeBzIoIL4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validaci√≥n Cruzada con BoW."
      ],
      "metadata": {
        "id": "NFXHSA1wO2U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline con Bag of Words y Regresi√≥n Log√≠stica\n",
        "pipeline1 = make_pipeline(\n",
        "    CountVectorizer(max_features=5000),  # Bag of Words\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "# Validaci√≥n cruzada con 5 particiones\n",
        "scores = cross_val_score(pipeline1, df['Comentario'], df['Valor'], cv=5, scoring='accuracy')\n",
        "\n",
        "# Resultados\n",
        "print(f\"Precisi√≥n media con validaci√≥n cruzada (BoW): {scores.mean():.3f}\")\n",
        "print(f\"Desviaci√≥n est√°ndar: {scores.std():.3f}\")"
      ],
      "metadata": {
        "id": "vjenYlA_O4eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa una mejora m√≠nima de la precisi√≥n y la desviaci√≥n est√°ndar.\n",
        "\n",
        "Los valores obtenidos usando TF-IDF fueron:\n",
        "\n",
        "Precisi√≥n = 0,807\n",
        "\n",
        "Desviaci√≥n est√°ndar = 0,018"
      ],
      "metadata": {
        "id": "ic7FONznnKIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redes Neuronales."
      ],
      "metadata": {
        "id": "ELvckxoswXK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba de Modelo con Keras."
      ],
      "metadata": {
        "id": "8XANfxe7IFA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizar los textos\n",
        "\n",
        "Se convierten los comentarios en una matriz num√©rica de hasta 10.000 caracter√≠sticas, considerando unigramas y bigramas.\n",
        "\n",
        "Esto captura no solo palabras individuales, sino tambi√©n combinaciones frecuentes de dos palabras, lo cual mejora la capacidad del modelo para captar expresiones √∫tiles como ‚Äúmuy bueno‚Äù o ‚Äúno sirve‚Äù."
      ],
      "metadata": {
        "id": "AERUMkhfoM_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizar los textos\n",
        "vectorizer_Neuro = TfidfVectorizer(ngram_range=(1, 2), max_features=10000)\n",
        "X = vectorizer_Neuro.fit_transform(df['Comentarios_sin_StopWords_str']).toarray()"
      ],
      "metadata": {
        "id": "wLwCMh8r2OFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variable objetivo\n",
        "\n",
        "Se extrae la variable objetivo desde la columna 'Valor' del DataFrame df, y se convierte en un array de NumPy para usarlo en el modelo de Machine Learning."
      ],
      "metadata": {
        "id": "mESFTwlvoZOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable objetivo\n",
        "y = df['Valor'].values"
      ],
      "metadata": {
        "id": "L5IRUcaa2oV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separaci√≥n del dataset en 80% entrenamiento y 20% validaci√≥n, estratificando por clase para mantener la proporci√≥n de etiquetas (positivo/negativo)."
      ],
      "metadata": {
        "id": "d9Qww0-johak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir en train y test (estratificado)\n",
        "X_train1, X_val, y_train1, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "Bw9CGsoF2tIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de red neuronal secuencial:\n",
        "\n",
        "Capa 1: 128 neuronas con ReLU + Dropout (50%)\n",
        "\n",
        "Capa 2: 64 neuronas con ReLU + Dropout (30%)\n",
        "\n",
        "Capa de salida: 1 neurona con sigmoide, ideal para clasificaci√≥n binaria.\n",
        "\n",
        "Compilaci√≥n:\n",
        "\n",
        "Se usa binary_crossentropy como funci√≥n de p√©rdida.\n",
        "\n",
        "M√©trica: precisi√≥n (accuracy).\n",
        "\n",
        "Optimizador: Adam con learning_rate=0.001"
      ],
      "metadata": {
        "id": "bW8b8vFSoqLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train1.shape[1],)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))  # 1 neurona y activaci√≥n sigmoid para binario\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',   # funci√≥n para binaria\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xa26Z1Wq0vAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen del modelo:\n",
        "\n",
        "Resumen estructurado del modelo de red neuronal que acab√°s de construir con Keras. Esta salida es muy √∫til para entender la arquitectura y verificar que el modelo est√© correctamente configurado."
      ],
      "metadata": {
        "id": "yZNooFhmpXLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostramos el resumen del modelo: esto nos dar√° detalles sobre cada capa y el n√∫mero de par√°metros entrenables en el modelo.\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IHjJPGEN3Z0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopping para evitar sobreentrenamiento\n",
        "\n",
        "El bloque implementa una t√©cnica de regularizaci√≥n llamada Early Stopping, que detiene autom√°ticamente el entrenamiento del modelo cuando la p√©rdida en el conjunto de validaci√≥n (val_loss) deja de mejorar durante un n√∫mero determinado de √©pocas.\n",
        "\n",
        "Esto ayuda a prevenir el sobreajuste, es decir, que el modelo aprenda demasiado los datos de entrenamiento y pierda capacidad de generalizaci√≥n."
      ],
      "metadata": {
        "id": "PBWrEPrfpvgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Use X_train1 and y_train1 which were specifically prepared for the neural network\n",
        "history = model.fit(X_train1, y_train1,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=100,\n",
        "                    batch_size=10,\n",
        "                    callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "ldJslQYk02Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriz de Confusi√≥n."
      ],
      "metadata": {
        "id": "cZvDhez8qHuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(confusion_matrix(y_val, y_pred))\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "29EyX-w_3KhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisi√≥n para clase 1 (0.84): cuando el modelo predice \"positivo\", acierta el 84% de las veces.\n",
        "\n",
        "Recall para clase 1 (0.69): de todos los comentarios realmente positivos, el modelo detecta correctamente el 69%. Esto indica que se le escapan varios positivos.\n",
        "\n",
        "Recall para clase 0 (0.86): el modelo detecta muy bien los negativos.\n",
        "\n",
        "F1-score m√°s bajo en clase 1 (0.75): el modelo tiene m√°s dificultades en predecir correctamente los positivos.\n",
        "\n"
      ],
      "metadata": {
        "id": "016-fgL-qRRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Gr√°fico de Accuracy y Loss por √âpoca\n",
        "\n",
        "Si las curvas de entrenamiento y validaci√≥n est√°n muy separadas, puede haber overfitting.\n",
        "\n",
        "Si ambas curvas bajan y se estabilizan juntas, el modelo generaliza bien.\n",
        "\n",
        "EarlyStopping puede cortar las √©pocas antes de 100, por eso es clave ver hasta d√≥nde lleg√≥ realmente.\n"
      ],
      "metadata": {
        "id": "Hc8eZPsB3fLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento', color='#4caf50')\n",
        "plt.plot(history.history['val_accuracy'], label='Validaci√≥n', color='#2196f3')\n",
        "plt.title('Precisi√≥n (Accuracy) por √âpoca')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('Precisi√≥n')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Entrenamiento', color='#f57c00')\n",
        "plt.plot(history.history['val_loss'], label='Validaci√≥n', color='#e53935')\n",
        "plt.title('P√©rdida (Loss) por √âpoca')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "reR7qkjk3fpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìà An√°lisis del gr√°fico de Precisi√≥n (Accuracy)\n",
        "La precisi√≥n de entrenamiento sube r√°pidamente y alcanza casi el 100% en pocas √©pocas.\n",
        "\n",
        "En cambio, la precisi√≥n de validaci√≥n se estabiliza alrededor del 77%‚Äì79%, sin seguir la mejora del entrenamiento.\n",
        "\n",
        "üìå Conclusi√≥n: Esto indica que el modelo est√° sobreajustando (overfitting): aprende muy bien los datos de entrenamiento, pero pierde capacidad de generalizaci√≥n.\n",
        "\n",
        "üìâ An√°lisis del gr√°fico de P√©rdida (Loss)\n",
        "La p√©rdida en entrenamiento cae bruscamente y casi llega a cero.\n",
        "\n",
        "La p√©rdida de validaci√≥n baja al principio pero luego empieza a subir desde la √©poca 2‚Äì3.\n",
        "\n",
        "üìå Conclusi√≥n: Otro signo claro de overfitting. El modelo memoriza los datos de entrenamiento y comienza a equivocarse m√°s en datos nuevos (validaci√≥n)."
      ],
      "metadata": {
        "id": "XIwKnXeTrGfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ ¬øQu√© se prodr√≠a hacer para mejorar?\n",
        "Aumentar regularizaci√≥n:\n",
        "\n",
        "Aumentar el Dropout (actualmente se tiene 0.5 y 0.3).\n",
        "\n",
        "Agregar L2 regularization (penalizaci√≥n en los pesos).\n",
        "\n",
        "Reducir complejidad del modelo:\n",
        "\n",
        "Menos neuronas o capas.\n",
        "\n",
        "El modelo podr√≠a ser demasiado complejo para el tama√±o de tu dataset.\n",
        "\n",
        "M√°s datos o data augmentation: ayuda a reducir el overfitting.\n",
        "\n",
        "EarlyStopping est√° funcionando bien: cort√≥ el entrenamiento antes de que empeore m√°s.\n",
        "\n",
        "Probar un modelo cl√°sico (Logistic Regression, SVM) como comparaci√≥n. A veces rinden igual o mejor con TF-IDF.\n",
        "\n"
      ],
      "metadata": {
        "id": "CP_cRt_ZrWfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_train1, y_train1, verbose=False)\n",
        "print(\"Precisi√≥n Entrenamiento: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n",
        "print(\"Precisi√≥n Prueba:  {:.4f}\".format(accuracy))\n",
        ""
      ],
      "metadata": {
        "id": "zLicGea133U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluaci√≥n del modelo."
      ],
      "metadata": {
        "id": "x7duPNWG31EN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluaci√≥n del modelo con una rese√±a real del conjunto de datos.\n",
        "\n",
        "Se muestra c√≥mo el modelo clasifica una rese√±a individual y compara su predicci√≥n con la realidad"
      ],
      "metadata": {
        "id": "9ZFBHijz3JF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Seleccionar una rese√±a real del DataFrame\n",
        "indice = 1000  # Cambiar este n√∫mero si se quiere ver otra rese√±a\n",
        "\n",
        "oracion_real = df['Comentario'].iloc[indice]\n",
        "valoracion_real = df['Valor'].iloc[indice]\n",
        "\n",
        "nueva_rese√±a_vectorizada = vectorizer_Neuro.transform([oracion_real])\n",
        "\n",
        "# Paso 2: Predecir con el modelo\n",
        "\n",
        "nueva_rese√±a_vectorizada_dense = nueva_rese√±a_vectorizada.toarray()\n",
        "\n",
        "prediccion = model.predict(nueva_rese√±a_vectorizada_dense)\n",
        "\n",
        "# Paso 3: Convertir la probabilidad a clase 0 o 1\n",
        "valoracion_predicha = 1 if prediccion[0][0] >= 0.5 else 0\n",
        "\n",
        "# Paso 6: Mostrar resultados\n",
        "print(f\"Rese√±a: {oracion_real}\")\n",
        "print(f\"Valoraci√≥n real: {valoracion_real}\")\n",
        "print(f\"Valoraci√≥n predicha: {valoracion_predicha}\")\n",
        "print(f\"Probabilidad predicha: {prediccion[0][0]:.4f}\")"
      ],
      "metadata": {
        "id": "DVJGqXxwgZtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test del modelo con frases nuevas."
      ],
      "metadata": {
        "id": "5Bd8pxxx3pY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testeamos con nuevas oraciones\n",
        "\n",
        "# Definir una nueva oraci√≥n para predecir.\n",
        "nueva_oracion = [\"si es fatal\"]\n",
        "\n",
        "nueva_secuencia_vectorizada = vectorizer_Neuro.transform(nueva_oracion)\n",
        "\n",
        "# Convertir a array denso si el modelo lo espera\n",
        "nueva_secuencia_vectorizada_dense = nueva_secuencia_vectorizada.toarray()\n",
        "\n",
        "\n",
        "# Usar el modelo para predecir la valoraci√≥n (0 o 1)\n",
        "\n",
        "prediccion = model.predict(nueva_secuencia_vectorizada_dense)\n",
        "\n",
        "print(f\"Predicci√≥n: {prediccion[0][0]}\")\n",
        "valoracion = 1 if prediccion[0][0] >= 0.5 else 0\n",
        "print(f\"Valoraci√≥n predicha: {valoracion}\")"
      ],
      "metadata": {
        "id": "vGXWFPv9_Rnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X ahora contiene un array de secuencias num√©ricas (en formato tensor o matriz), en las que cada n√∫mero representa un √≠ndice de palabra del vocabulario.\n",
        "# Estas secuencias est√°n ajustadas para tener la misma longitud (max_len=100), con las m√°s largas recortadas y las m√°s cortas rellenadas con ceros.\n",
        "# Visualizamos el tipo de dato que es X\n",
        "print(type(X))\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "id": "n5IpwpRgdI9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones.**"
      ],
      "metadata": {
        "id": "2Tuqz1acz2js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lo largo del trabajo se probaron distintos m√©todos de preprocesamiento para textos, aplicados a un an√°lisis de sentimientos.\n",
        "Dentro del an√°lizsis de sentimiento se evaluaron los comentarios con baja probabilidad de predicci√≥n. Esto nos va a permitir visualizar la sintaxis de estos y tomar decisiones a la hora de mejorar el modelo.\n",
        "Como futuras lineas, se puede re entrenar el modelo evaluando estas oraciones para que interprete mejor los sentiminetos.\n",
        "\n",
        "Se trabaj√≥ con t√©cnicas como Bag of Words y TF-IDF, y tambi√©n se explor√≥ un modelo m√°s avanzado basado en Deep Learning usando Keras.\n",
        "\n",
        "Se entrenaron tres modelos de machine learning con estas representaciones, y el que mejor funcion√≥ fue el que utiliz√≥ TF-IDF, mostrando buenos resultados en la clasificaci√≥n de sentimientos. En cambio, el modelo de Deep Learning no tuvo el rendimiento esperado, probablemente porque falt√≥ ajustar mejor los par√°metros y tambi√©n porque el volumen de datos no era muy grande.\n",
        "\n",
        "Como posible l√≠nea futura, se puede seguir probando con el modelo de Deep Learning, ajustando hiperpar√°metros (como incluir regularizaci√≥n L2, capas LSTM, etc.) y entrenando con una mayor cantidad de datos, lo cual podr√≠a mejorar bastante los resultados."
      ],
      "metadata": {
        "id": "ZVK7c8gd6wNR"
      }
    }
  ]
}